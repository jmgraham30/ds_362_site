---
title: "Lesson 10"
subtitle: "Reinforcement Learning"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson10.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(kableExtra)
library(ggthemes)
library(ReinforcementLearning)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))


penguins <- penguins %>%
  drop_na()

penguins_X <- penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm)

```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning) paradigm in machine learning and how it compares and contrasts with supervised and unsupervised learning.  

- Use [`ReinforcementLearning`](https://cran.r-project.org/web/packages/ReinforcementLearning/vignettes/ReinforcementLearning.html) R package. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 1 from of *Reinforcement Learning: An Introduction* by Sutton and Barto [@sutton2018reinforcement]. This book is available online [here](http://incompleteideas.net/book/the-book-2nd.html).

- [View reinforcement learning video by Crash Coruse AI on YouTube](https://youtu.be/nIgIv4IfJ6s?si=oCuyZD28P-KlzBPD).

```{r}
#| echo: false

vembedr::embed_youtube(id="nIgIv4IfJ6s?si=oCuyZD28P-KlzBPD",height=450) %>%
  vembedr::use_align("center")
```

- [View lecture 1 of intro to reinforcement learning course by Emma Brunskill on YouTube](https://youtu.be/FgzM3zpZ55o?si=JaWnWnU-zPA6LPoc).



## Overview

Reinforcement learning is a paradigm in machine learning that is concerned with how an agent should take actions in an environment so as to maximize some notion of cumulative reward. Basically, the agent learns to interact with the environment by trial and error. The agent is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.

To learn more about reinforcement learning, you can work through the guided tutorial [here](https://ds362assignments.netlify.app/homework_09/).

We also recommend reading chapter 1 from of *Reinforcement Learning: An Introduction* by Sutton and Barto [@sutton2018reinforcement]. This book is available online [here](http://incompleteideas.net/book/the-book-2nd.html).


## An Example Using `ReinforcementLearning` R Package


This section demonstrates the capabilities of the `ReinforcementLearning` package with the help of a practical example. Another example is developed in the guided tutorial [here](https://ds362assignments.netlify.app/homework_09/).

Our practical example aims at teaching optimal movements to a robot in a grid-shaped maze.

Here the agent must navigate from a random starting position to a final position on a simulated $2 \times 2$ grid. The grid looks as follows:

|       |        |       |
|:-----:|:-------|:-----:|
| s1 | Wall | s4 |
| s2 | Open |  s3 |


Each cell on the grid reflects one state, yielding a total of 4 different states. In each state, the agent can perform one out of four possible actions: to move up, down, left, or right, with the only restriction being that it must remain on the grid. In other words, the grid is surrounded by a wall, which makes it impossible for the agent to move off the grid. A wall between s1 and s4 hinders direct movements between these states. Finally, the reward structures is as follows: each movement leads to a negative reward of -1 in order to penalize routes that are not the shortest path. If the agent reaches the goal position, it earns a reward of 10.

We first define the sets of available states (`states`) and actions (`actions`).

```{r}
# Define state and action sets
states <- c("s1", "s2", "s3", "s4")
actions <- c("up", "down", "left", "right")
```

We then rewrite the above problem formulation into the following environment function. As previously mentioned, this function must take a state and an action as input. The if-conditions determine the current combination of state and action. In our example, the state refers to the agent’s position on the grid and the action denotes the intended movement. Based on these, the function decides upon the next state and a numeric reward. These together are returned as a list.

```{r}
# Load built-in environment function for 2x2 gridworld 
env <- gridworldEnvironment
print(env)
```

After having specified the environment function, we can use the built-in `sampleExperience()` function to sample observation sequences from the environment. The following code snippet generates a data frame data containing 1000 random state-transition tuples $(s_{i},a_{i},r_{i+1},s_{i+1})$.

```{r}
# Sample N = 1000 random sequences from the environment
data <- sampleExperience(N = 1000, 
                         env = env, 
                         states = states, 
                         actions = actions)
head(data)
```


We can now use the observation sequence in `data` in order to learn the optimal behavior of the agent. For this purpose, we first customize the learning behavior of the agent by defining a control object. We follow the default parameter choices and set the learning rate alpha to 0.1, the discount factor gamma to 0.5 and the exploration greediness epsilon to 0.1. Subsequently, we use the `ReinforcementLearning()` function to learn the best possible policy for the the input data.

```{r}
# Define reinforcement learning parameters
control <- list(alpha = 0.1, gamma = 0.5, epsilon = 0.1)

# Perform reinforcement learning
model <- ReinforcementLearning(data, 
                               s = "State", 
                               a = "Action", 
                               r = "Reward", 
                               s_new = "NextState", 
                               control = control)
```


The `ReinforcementLearning()` function returns an `rl` object. We can evoke `computePolicy(model)` in order to display the policy that defines the best possible action in each state. Alternatively, we can use `print(model)` in order to write the entire state-action table to the screen, i.e. the Q-value of each state-action pair. Evidently, the agent has learned the optimal policy that allows it to take the shortest path from an arbitrary starting position to the goal position s4.

```{r}
# Print policy
computePolicy(model)
```


```{r}
# Print state-action function
print(model)
```

Ultimately, we can use `summary(model)` to inspect the model further. This command outputs additional diagnostics regarding the model such as the number of states and actions. Moreover, it allows us to analyze the distribution of rewards. For instance, we see that the total reward in our sample (*i.e.* the sum of the rewards column `r`) is highly negative. This indicates that the random policy used to generate the state transition samples deviates from the optimal case. Hence, the next section explains how to apply and update a learned policy with new data samples.

```{r}
# Print summary statistics
summary(model)
```

We now apply an existing policy to unseen data in order to evaluate the out-of-sample performance of the agent. The following example demonstrates how to sample new data points from an existing policy. The result yields a column with the best possible action for each given state.

```{r}
# Example data
data_unseen <- data.frame(State = c("s1", "s2", "s1"), 
                          stringsAsFactors = FALSE)

# Pick optimal action
data_unseen$OptimalAction <- predict(model, data_unseen$State)

data_unseen
```


Finally, one can update an existing policy with new observational data. This is beneficial when, for instance, additional data points become available or when one wants to plot the reward as a function of the number of training samples. For this purpose, the `ReinforcementLearning()` function can take an existing `rl` model as an additional input parameter. Moreover, it comes with an additional pre-defined action selection mode, namely $\epsilon$-greedy, thereby following the best action with probability $1−\epsilon$ and a random one with $\epsilon$.

```{r}
# Sample N = 1000 sequences from the environment
# using epsilon-greedy action selection
data_new <- sampleExperience(N = 1000, 
                             env = env, 
                             states = states, 
                             actions = actions, 
                             actionSelection = "epsilon-greedy",
                             model = model, 
                             control = control)

# Update the existing policy using new training data
model_new <- ReinforcementLearning(data_new, 
                                   s = "State", 
                                   a = "Action", 
                                   r = "Reward", 
                                   s_new = "NextState", 
                                   control = control,
                                   model = model)
```


The following code snippet shows that the updated policy yields significantly higher rewards as compared to the previous policy. These changes can also be visualized in a learning curve via plot(model_new).

```{r}
# Print result
print(model_new)
```


```{r}
# Plot reinforcement learning curve
plot(model_new)
```


## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)