---
title: "Lesson 7"
subtitle: "Neural Networks and Deep Learning"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson07.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(latex2exp)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))
```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the structure of a neural network and explain the role of activation functions, loss functions, optimization algorithms, and regularization  in neural networks and deep learning.

- Implement the gradient descent algorithm for optimization of simple functions.

- Implement a neural network in R using packages such as `brulee`, `keras` or `torch`.



## Readings, etc.

For this lesson, refer to the following readings, etc.:


- Read chapter 10 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. 

- Read section 7.1 from *Mathematics for Machine Learning* [@deisenroth2020mathematics]. This book is freely available online. [View the book](https://mml-book.github.io/).

Watch the following video lectures on neural networks: 

* [View Introduction to Neural Networks video on YouTube](https://youtu.be/jJb2qytbcNg?si=hwdtetIKGC18RbXT).

```{r}
#| echo: false

vembedr::embed_youtube(id="jJb2qytbcNg?si=hwdtetIKGC18RbXT",height=450) %>%
  vembedr::use_align("center")
```


    
## Overview   

[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is an active area of research in machine learning and artificial intelligence and [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are the foundation of deep learning. In this lesson, we will introduce neural networks and discuss how they are used in deep learning.

Neural networks are a class of machine learning models that are inspired by the structure of the brain. They are composed of a series of layers of neurons that are connected to each other. Each artificial neuron is a simple computational unit that takes in a set of inputs, performs a computation, and produces an output. The output of one neuron is then used as the input to the next neuron. The first layer of artificial neurons is called the input layer and the last layer of neurons is called the output layer. The layers in between the input and output layers are called hidden layers. The number of hidden layers in a neural network is called the depth of the network. The number of neurons in each layer is called the width of the network. @fig-slnn shows a neural network with one hidden layer consisting of 4 neurons or nodes. Later we will develop notation to describe neural networks mathematically. From here on out we will ignore the biological analogy that is the historical origin of neural networks and focus on the mathematical model.

![A neural network with a single hidden layer consisting of four neurons or nodes.](https://www.dropbox.com/scl/fi/t16o0um8wenwr0kq0t6i4/10_1.png?rlkey=epw6xbd5x8qpu9xqtsdmryqaq&raw=1){#fig-slnn}


Neural networks are conceptually simple but the mathematical details can be confusing. The general idea is that a neural network takes an input of $p$ predictor variables $X = (X_{1},X_{2},\ldots , X_{p})$ and builds a *nonlinear* function $f(X)$ to predict the response $Y$. What distinguishes neural networks for other nonlinear methods is the particular structure of the model function $f$.
 
 
### Describing a Neural Network
 
 In order to develop some intuition, we will start by exploring an interactive visualization of a neural network via the [Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.92779&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) website. [Visit the Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.92779&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). 

The visualization allows you to create a neural network and then train it on a dataset. The dataset can be a classification problem or a regression problem. The visualization allows you to change the activation function, the number of hidden layers, the number of neurons in each layer, and the learning rate. These are components related to the training of a network that we will define in detail later. 

### Describing a Neural Networks in R

Let's also take a look at a simple neural network in R. The [`brulee`](https://brulee.tidymodels.org/index.html) package provides a simple interface via a function `brulee_mlp()` (it's a good idea to skim the [`brulee_mlp` documentation](https://brulee.tidymodels.org/reference/brulee_mlp.html)) for creating neural networks in R and uses the `tidymodels` framework for modeling. The code, available via [this GitHub repo](https://github.com/jmgraham30/nn_class_example/tree/main),  creates a neural network with one hidden layer and trains it on the `penguins` dataset. The repository also contains a script with an example of tuning a neural network with a single hidden layer. Let's examine this together in an RStudio project. 

### Suggestions for Further Reading
    
Here are some additional resources on neural networks and deep learning that cover various aspects of the field that we do not have time to go over in this course:

1. For historical context, see *Thinking Machines: The Quest for Artificial Intelligence--and Where It's Taking Us Next* by Dormehl or *Deep Learning* by Kelleher.

2. For an excellent overview of the types of problems that neural networks and deep learning are well-suited for, see [@krohn2019deep].

3. For a more detailed introduction to neural networks, see [@Goodfellow-et-al-2016]. This book is accessible online, [view the book](https://www.deeplearningbook.org/).

### Recent Applications of Neural Networks and Deep Learning

1. **Natural Language Processing (NLP):**
   - *Example*: BERT, GPT-3, and other large transformer models for tasks like language translation, text generation, and sentiment analysis.
   - *Learn More*: [OpenAI's GPT-3](https://openai.com/research/gpt-3), [Google's BERT](https://ai.google/research/pubs/pub48095)

2. **Computer Vision:**
   - *Example*: Convolutional Neural Networks (CNNs) for image classification, object detection, and facial recognition.
   - *Learn More*: [ImageNet Large Scale Visual Recognition Challenge](http://www.image-net.org/challenges/LSVRC/)

3. **Healthcare:**
   - *Example*: Using deep learning to analyze medical images, detect diseases, and predict patient outcomes.
   - *Learn More*: [Stanford's CheXNet](https://stanfordmlgroup.github.io/projects/chexnet/)

4. **Autonomous Vehicles:**
   - *Example*: Self-driving cars rely on neural networks to perceive their surroundings and make decisions.
   - *Learn More*: [Waymo's Self-Driving Technology](https://waymo.com/technology/)

5. **Recommender Systems:**
   - *Example*: Recommending products, movies, or content to users based on their preferences and behavior.
   - *Learn More*: [Netflix's Recommendation Algorithm](https://help.netflix.com/en/node/100639)

6. **Finance:**
   - *Example*: Predictive modeling for stock price forecasting, fraud detection, and algorithmic trading.
   - *Learn More*: [Stock Price Prediction with LSTM](https://github.com/mwitiderrick/stockprice)

7. **Voice Assistants:**
   - *Example*: Voice recognition and natural language understanding in smart speakers like Amazon Echo and Google Home.
   - *Learn More*: [Amazon Alexa](https://developer.amazon.com/en-US/alexa)

8. **Generative Adversarial Networks (GANs):**
   - *Example*: Creating art, generating synthetic images, and deepfakes.
   - *Learn More*: [NVIDIA's GAN Research](https://www.nvidia.com/en-us/research/ai-playground/)

9. **Robotics:**
   - *Example*: Deep reinforcement learning for robot control and autonomous navigation.
   - *Learn More*: [OpenAI's Robotics Research](https://openai.com/research/robotics)

10. **Climate Science:**
    - *Example*: Using neural networks to analyze climate data, model climate change, and predict extreme weather events.
    - *Learn More*: [DeepMind's Climate Science](https://deepmind.com/research/case-studies/climate-change)

### Sizes of Neural Networks in Various Applications

1. **Natural Language Processing (NLP):**
   - *GPT-3*: GPT-3, developed by OpenAI, is one of the largest language models with 175 billion parameters.
   - *BERT*: Google's BERT has 340 million parameters and is highly influential in NLP.

2. **Computer Vision:**
   - *ImageNet Models*: Models like VGG-16, VGG-19, and ResNet used for image classification have tens of millions of parameters.
   - *Large CNNs*: In complex computer vision tasks, models can have hundreds of millions of parameters. Examples include Inception models and DenseNet.

3. **Healthcare:**
   - The size of neural networks in healthcare applications varies, ranging from a few million to tens of millions of parameters for tasks like medical image analysis.

4. **Autonomous Vehicles:**
   - Neural networks used in autonomous vehicles vary in size. Perception networks processing sensor data may have tens of millions of parameters, while decision-making networks might be smaller.

5. **Generative Adversarial Networks (GANs):**
   - Large GANs, such as BigGAN, can have hundreds of millions of parameters and are used for image generation.

6. **Climate Science:**
   - Climate models using deep learning can have varying numbers of parameters, typically in the millions to tens of millions, depending on the model's complexity.




## Neural Networks

A neural network is a nonlinear function that is described by parameters called weights and bias. Each node or neuron in a layer of the network inputs a linear combination of the outputs from the nodes of the previous layer. The weights and bias parameters specify the linear combination which is passed through an [activation function](https://en.wikipedia.org/wiki/Activation_function) to produce the output of the node. The activation function is a nonlinear function, its output defines the output of the node. Later, we will define some typical activation functions. 

The output of each node is then used as the input to the nodes of the next layer. The output of the last layer is the output of the neural network. The weights and bias parameters are learned during the training process. The training process involves finding the weights and bias parameters that **minimize a loss function**. The loss function is a measure of how well the neural network is performing on the training data. The goal of training a neural network is to find the weights and bias parameters that minimize the loss function.

As with all of the other machine learning algorithms we have covered so far, deep learning requires us to solve some kind of optimization problem. In the case of neural networks, we seek to find the weights and bias parameters that minimize the loss function. For regression problems, the loss function is typically the mean squared error (MSE). For classification problems, the loss function is typically the cross-entropy loss. 

The weights and bias parameters are learned using an algorithm called [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). Gradient descent is an optimization algorithm that is used to find the minimum of a function. In the context of neural networks, we use gradient descent to find the minimum of the loss function. The loss function is a function of the weights and biases of the neural network. The weights and biases are the parameters of the neural network. The loss function is a measure of how well the neural network is performing on the training data. The goal of training a neural network is to find the weights and bias parameters that minimize the loss function.

[Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) is a variant of gradient descent that is used to train neural networks. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.

Gradient descent and SGD require us to compute the gradient (multi-variable derivative) of the loss function with respect to the weights and bias parameters. The activation function at each node of the network results in a nonlinear function of the parameters we want to optimize. Thus, computing the implementation of gradient descent for neural networks forces us to use the chain rule for derivatives and this becomes a very messy calculation. 

A major development in the field of neural networks was the introduction of the [backpropagation algorithm](https://en.wikipedia.org/wiki/Backpropagation) by Rumelhart, Hinton, and Williams in 1986. The backpropagation algorithm is an algorithm for training neural networks. This algorithm is used to calculate the gradient of the loss function with respect to the weights and bias parameters. The gradient is then used to update the weights and bias parameters via gradient descent or something similar. The backpropagation algorithm is clever use of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) for derivatives. 

We will proceed by getting a feel for gradient descent in the context of functions that are a lot simpler than neural networks. 

### Gradient Descent for Optimization

Gradient descent is an iterative optimization algorithm for finding the minimum of a function $f:\mathbb{R}^{d} \rightarrow \mathbb{R}$. The algorithm starts with an initial guess ${\bf x}_{0} \in \mathbb{R}^{d}$ for the minimizing value for the function. The algorithm then iteratively updates the guess by an iteration of the form

$$
{\bf x}_{k+1} = {\bf x}_{k} - \alpha_{k} \nabla f({\bf x}_{k})
$$

where $\alpha_{k} > 0$ is the step size and $\nabla f({\bf x}_{k})$ is the gradient of the function $f$ at the point ${\bf x}_{k}$. Recall that the gradient of a function is a vector that points in the direction of the steepest ascent of the function. In practical implementations one often takes the step size $\alpha_{k} = \text{constant}$ for all $k$ and this constant is called the **learning rate**. In the context of neural networks, each step in an iteration of gradient descent is called an **epoch**.

Let's start with a simple one-dimensional problem. Consider the function

$$
f(x) = x^4 + 7x^3 + 5x^2 - 17x + 3
$$
which is plotted in @fig-quart-fun. This function has a global minimum at $x = -4.5$. We can use gradient descent to find at least approximately the value of $x$ that minimizes $f(x)$.

```{r}
#| label: fig-quart-fun
#| fig-cap: A fourth-degree polynomial function with a global minimum at $x = -4.5$.
#| code-fold: true

f <- function(x) {
  x^4 + 7*x^3 + 5*x^2 - 17*x + 3
}

x <- seq(-6, 2, length.out = 100)

tibble(x = x) %>% 
  mutate(y = f(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  geom_vline(xintercept = -4.5, linetype = "dashed") +
  geom_point(aes(x = -4.5, y = f(-4.5)), color = "purple", size = 3) +
  labs(
    x = TeX("$x$"),
    y = TeX("$f(x)$"),
    title = "A polynomial function with a global minimum"
  )

```

The gradient (derivative) of $f(x)$ is given by

$$
f'(x) = 4x^3 + 21x^2 + 10x - 17
$$

Let's iterate gradient descent for 25 epochs with a learning rate of $\alpha = 0.01$ and an initial guess of $x_{0} = -3$. We will plot the value of $x$ at each epoch and the value of the function $f(x)$ at each epoch. We will also plot the value of the gradient at each epoch. 



```{r}
#| label: fig-quart-grad-descent
#| fig-cap: Gradient descent for the function $f(x)$. 
#| code-fold: true
#| fig-width: 8
#| fig-height: 6
#| fig-align: center

f_prime <- function(x) {
  4*x^3 + 21*x^2 + 10*x - 17
}

x <- seq(-6, 2, length.out = 100)

gd_min <- function(x0, 
                  alpha=0.01, 
                  fun_to_min=f, 
                  fun_deriv=f_prime, 
                  n_epochs = 25){
  x <- x0
  x_vals <- c(x)
  y_vals <- c(fun_to_min(x))
  grad_vals <- c(fun_deriv(x))
  
  for (i in 1:n_epochs){
    x <- x - alpha*fun_deriv(x)
    x_vals <- c(x_vals, x)
    y_vals <- c(y_vals, fun_to_min(x))
    grad_vals <- c(grad_vals, fun_deriv(x))
  }
  
  return(tibble(x = x_vals, y = y_vals, grad = grad_vals))
}

tst <- gd_min(-3)

tibble(x = x) %>% 
  mutate(y = f(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  geom_vline(xintercept = -4.5, linetype = "dashed") +
  geom_point(aes(x = -4.5, y = f(-4.5)), color = "purple", size = 3) +
  geom_point(data = tst, aes(x = x, y = y, color=y),size=2) +
  labs(
    x = TeX("$x$"),
    y = TeX("$f(x)$"),
    title = TeX(r'(Gradient descent for the function $f(x)$)')
  )

```




### Single Layer Networks


### Multilayer networks


### Convolutional and Recurrent Networks


## `r icons::icon_style(icons::fontawesome("r-project"),scale=2,fill="steelblue")` Neural Networks and Deep Learning in R



## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)