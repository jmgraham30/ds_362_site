---
title: "Lesson 7"
subtitle: "Neural Networks and Deep Learning"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson07.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(latex2exp)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))
```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the structure of a neural network and explain the role of activation functions, loss functions, optimization algorithms, and regularization  in neural networks and deep learning.

- Implement the gradient descent algorithm for optimization of simple functions.

- Implement a neural network in R using packages such as `brulee`, `keras` or `torch`.



## Readings, etc.

For this lesson, refer to the following readings, etc.:


- Read chapter 10 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. 

- Read section 7.1 from *Mathematics for Machine Learning* [@deisenroth2020mathematics]. This book is freely available online. [View the book](https://mml-book.github.io/).

Watch the following video lectures on neural networks: 

* [View Introduction to Neural Networks video on YouTube](https://youtu.be/jJb2qytbcNg?si=hwdtetIKGC18RbXT).

```{r}
#| echo: false

vembedr::embed_youtube(id="jJb2qytbcNg?si=hwdtetIKGC18RbXT",height=450) %>%
  vembedr::use_align("center")
```


    
## Overview   

[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is an active area of research in machine learning and artificial intelligence and [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are the foundation of deep learning. In this lesson, we will introduce neural networks and discuss how they are used in deep learning.

Neural networks are a class of machine learning models that are inspired by the structure of the brain. They are composed of a series of layers of neurons that are connected to each other. Each artificial neuron is a simple computational unit that takes in a set of inputs, performs a computation, and produces an output. The output of one neuron is then used as the input to the next neuron. The first layer of artificial neurons is called the input layer and the last layer of neurons is called the output layer. The layers in between the input and output layers are called hidden layers. The number of hidden layers in a neural network is called the depth of the network. The number of neurons in each layer is called the width of the network. @fig-slnn shows a neural network with one hidden layer consisting of 4 neurons or nodes. Later we will develop notation to describe neural networks mathematically. From here on out we will ignore the biological analogy that is the historical origin of neural networks and focus on the mathematical model.

![A neural network with a single hidden layer consisting of four neurons or nodes.](https://www.dropbox.com/scl/fi/t16o0um8wenwr0kq0t6i4/10_1.png?rlkey=epw6xbd5x8qpu9xqtsdmryqaq&raw=1){#fig-slnn}


Neural networks are conceptually simple but the mathematical details can be confusing. The general idea is that a neural network takes an input of $p$ predictor variables $X = (X_{1},X_{2},\ldots , X_{p})$ and builds a *nonlinear* function $f(X)$ to predict the response $Y$. What distinguishes neural networks for other nonlinear methods is the particular structure of the model function $f$.
 
 
### Describing a Neural Network
 
 In order to develop some intuition, we will start by exploring an interactive visualization of a neural network via the [Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.92779&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) website. [Visit the Neural Network Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.92779&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). 

The visualization allows you to create a neural network and then train it on a dataset. The dataset can be a classification problem or a regression problem. The visualization allows you to change the activation function, the number of hidden layers, the number of neurons in each layer, and the learning rate. These are components related to the training of a network that we will define in detail later. 

### Describing a Neural Networks in R

Let's also take a look at a simple neural network in R. The [`brulee`](https://brulee.tidymodels.org/index.html) package provides a simple interface via a function `brulee_mlp()` (it's a good idea to skim the [`brulee_mlp` documentation](https://brulee.tidymodels.org/reference/brulee_mlp.html)) for creating neural networks in R and uses the `tidymodels` framework for modeling. The code, available via [this GitHub repo](https://github.com/jmgraham30/nn_class_example/tree/main),  creates a neural network with one hidden layer and trains it on the `penguins` dataset. The repository also contains a script with an example of tuning a neural network with a single hidden layer. Let's examine this together in an RStudio project. 

### Suggestions for Further Reading
    
Here are some additional resources on neural networks and deep learning that cover various aspects of the field that we do not have time to go over in this course:

1. For historical context, see *Thinking Machines: The Quest for Artificial Intelligence--and Where It's Taking Us Next* by Dormehl or *Deep Learning* by Kelleher.

2. For an excellent overview of the types of problems that neural networks and deep learning are well-suited for, see [@krohn2019deep].

3. For a more detailed introduction to neural networks, see [@Goodfellow-et-al-2016]. This book is accessible online, [view the book](https://www.deeplearningbook.org/).


## Neural Networks

A neural network is a nonlinear function that is described by parameters called weights and bias. Each node or neuron in a layer of the network inputs a linear combination of the outputs from the nodes of the previous layer. The weights and bias parameters specify the linear combination which is passed through an [activation function](https://en.wikipedia.org/wiki/Activation_function) to produce the output of the node. The activation function is a nonlinear function, its output defines the output of the node. Later, we will define some typical activation functions. 

The output of each node is then used as the input to the nodes of the next layer. The output of the last layer is the output of the neural network. The weights and bias parameters are learned during the training process. The training process involves finding the weights and bias parameters that **minimize a loss function**. The loss function is a measure of how well the neural network is performing on the training data. The goal of training a neural network is to find the weights and bias parameters that minimize the loss function.

As with all of the other machine learning algorithms we have covered so far, deep learning requires us to solve some kind of optimization problem. In the case of neural networks, we seek to find the weights and bias parameters that minimize the loss function. For regression problems, the loss function is typically the mean squared error (MSE). For classification problems, the loss function is typically the cross-entropy loss. 

The weights and bias parameters are learned using an algorithm called [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). Gradient descent is an optimization algorithm that is used to find the minimum of a function. In the context of neural networks, we use gradient descent to find the minimum of the loss function. The loss function is a function of the weights and biases of the neural network. The weights and biases are the parameters of the neural network. The loss function is a measure of how well the neural network is performing on the training data. The goal of training a neural network is to find the weights and bias parameters that minimize the loss function.

[Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) is a variant of gradient descent that is used to train neural networks. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.

Gradient descent and SGD require us to compute the gradient (multi-variable derivative) of the loss function with respect to the weights and bias parameters. The activation function at each node of the network results in a nonlinear function of the parameters we want to optimize. Thus, computing the implementation of gradient descent for neural networks forces us to use the chain rule for derivatives and this becomes a very messy calculation. 

A major development in the field of neural networks was the introduction of the [backpropagation algorithm](https://en.wikipedia.org/wiki/Backpropagation) by Rumelhart, Hinton, and Williams in 1986. The backpropagation algorithm is an algorithm for training neural networks. This algorithm is used to calculate the gradient of the loss function with respect to the weights and bias parameters. The gradient is then used to update the weights and bias parameters via gradient descent or something similar. The backpropagation algorithm is clever use of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) for derivatives. 

We will proceed by getting a feel for gradient descent in the context of functions that are a lot simpler than neural networks. 

### Gradient Descent for Optimization

Gradient descent is an iterative optimization algorithm for finding the minimum of a function $f:\mathbb{R}^{d} \rightarrow \mathbb{R}$. The algorithm starts with an initial guess ${\bf x}_{0} \in \mathbb{R}^{d}$ for the minimizing value for the function. The algorithm then iteratively updates the guess by an iteration of the form

$$
{\bf x}_{k+1} = {\bf x}_{k} - \alpha_{k} \nabla f({\bf x}_{k})
$$

where $\alpha_{k} > 0$ is the step size and $\nabla f({\bf x}_{k})$ is the gradient of the function $f$ at the point ${\bf x}_{k}$. Recall that the gradient of a function is a vector that points in the direction of the steepest ascent of the function. In practical implementations one often takes the step size $\alpha_{k} = \text{constant}$ for all $k$ and this constant is called the **learning rate**. In the context of neural networks, each step in an iteration of gradient descent is called an **epoch**.

Let's start with a simple one-dimensional problem. Consider the function

$$
f(x) = x^4 + 7x^3 + 5x^2 - 17x + 3
$$
which is plotted in @fig-quart-fun. This function has a global minimum at $x = -4.5$. We can use gradient descent to find at least approximately the value of $x$ that minimizes $f(x)$.

```{r}
#| label: fig-quart-fun
#| fig-cap: A fourth-degree polynomial function with a global minimum at $x = -4.5$.
#| code-fold: true

f <- function(x) {
  x^4 + 7*x^3 + 5*x^2 - 17*x + 3
}

x <- seq(-6, 2, length.out = 100)

tibble(x = x) %>% 
  mutate(y = f(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  geom_vline(xintercept = -4.5, linetype = "dashed") +
  geom_point(aes(x = -4.5, y = f(-4.5)), color = "purple", size = 3) +
  labs(
    x = TeX("$x$"),
    y = TeX("$f(x)$"),
    title = "A polynomial function with a global minimum"
  )

```

The gradient (derivative) of $f(x)$ is given by

$$
f'(x) = 4x^3 + 21x^2 + 10x - 17
$$

Let's iterate gradient descent for 25 epochs with a learning rate of $\alpha = 0.01$ and an initial guess of $x_{0} = -3$. We will plot the value of $x$ at each epoch and the value of the function $f(x)$ at each epoch. We will also plot the value of the gradient at each epoch. 



```{r}
#| label: fig-quart-grad-descent
#| fig-cap: Gradient descent for the function $f(x)$. 
#| code-fold: true
#| fig-width: 8
#| fig-height: 6
#| fig-align: center

f_prime <- function(x) {
  4*x^3 + 21*x^2 + 10*x - 17
}

x <- seq(-6, 2, length.out = 100)

gd_min <- function(x0, 
                  alpha=0.01, 
                  fun_to_min=f, 
                  fun_deriv=f_prime, 
                  n_epochs = 25){
  x <- x0
  x_vals <- c(x)
  y_vals <- c(fun_to_min(x))
  grad_vals <- c(fun_deriv(x))
  
  for (i in 1:n_epochs){
    x <- x - alpha*fun_deriv(x)
    x_vals <- c(x_vals, x)
    y_vals <- c(y_vals, fun_to_min(x))
    grad_vals <- c(grad_vals, fun_deriv(x))
  }
  
  return(tibble(x = x_vals, y = y_vals, grad = grad_vals))
}

tst <- gd_min(-3)

tibble(x = x) %>% 
  mutate(y = f(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  geom_vline(xintercept = -4.5, linetype = "dashed") +
  geom_point(aes(x = -4.5, y = f(-4.5)), color = "purple", size = 3) +
  geom_point(data = tst, aes(x = x, y = y, color=y),size=2) +
  labs(
    x = TeX("$x$"),
    y = TeX("$f(x)$"),
    title = TeX(r'(Gradient descent for the function $f(x)$)')
  )

```

### Activation Functions


1. **Sigmoid Activation Function:**
   
   - *Description*: The sigmoid activation function is commonly used in neural networks for binary classification tasks. It maps input values to the range \((0, 1)\), making it suitable for output layers of binary classifiers.
   - *Mathematical Expression*: 
   
   $$
   \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}}
   $$

2. **ReLU (Rectified Linear Unit) Activation Function:**
   
   - *Description*: ReLU is a widely used activation function that introduces non-linearity by returning the input for positive values and zero for negative values. It helps mitigate the vanishing gradient problem.
   - *Mathematical Expression*: 
   
   $$
   \text{ReLU}(x) = \max(0, x) 
   $$

3. **Tanh (Hyperbolic Tangent) Activation Function:**
   
   - *Description*: Tanh is another common activation function that maps input values to the range \((-1, 1)\). It provides zero-centered output, which can help training converge faster.
   - *Mathematical Expression*: 
   
   $$
   \text{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} 
   $$

4. **Leaky ReLU Activation Function:**
   
   - *Description*: Leaky ReLU is a variation of ReLU that allows a small gradient when the input is negative. It addresses the "dying ReLU" problem by preventing neurons from becoming inactive.
   - *Mathematical Expression*: 
   
   $$
   \text{Leaky ReLU}(x) = \begin{cases} x, & \text{if } x \geq 0 \\ \alpha x, & \text{if } x < 0 \end{cases}
   $$
    
where $\alpha$ is a small constant.   
    

@fig-activation-functions shows the plot for each of the activation functions we defined. 

```{r}
#| label: fig-activation-functions
#| fig-cap: Activation functions used in neural networks.
#| code-fold: true

  

# Create a data frame with x values
x <- seq(-3, 3, length.out = 1000)
df <- data.frame(x = x)

# Sigmoid Activation Function
df$sigmoid <- 1 / (1 + exp(-x))

# ReLU (Rectified Linear Unit) Activation Function
df$relu <- pmax(0, x)

# Tanh (Hyperbolic Tangent) Activation Function
df$tanh <- (exp(x) - exp(-x)) / (exp(x) + exp(-x))

# Leaky ReLU Activation Function
leaky_relu <- function(x, alpha = 0.03) {
  ifelse(x >= 0, x, alpha * x)
}
df$leaky_relu <- leaky_relu(x)

# Plot the activation functions
ggplot(df, aes(x)) +
  geom_line(aes(y = sigmoid, color = "Sigmoid"), linewidth = 1) +
  geom_line(aes(y = relu, color = "ReLU"), linewidth = 1) +
  geom_line(aes(y = tanh, color = "Tanh"), linewidth = 1) +
  geom_line(aes(y = leaky_relu, color = "Leaky ReLU"), linewidth = 1) +
  labs(
    title = "Common Activation Functions for Neural Networks",
    x = "Input (x)",
    y = "Output",
    color = "Activation Function"
  ) +
  scale_color_manual(values = c("Sigmoid" = "#E69F00", "ReLU" = "#56B4E9", "Tanh" = "#009E73", "Leaky ReLU" = "#CC79A7")) +
  theme_minimal()

```


### Single Layer Networks

A single layer neural network is the simplest type of neural network. It consists of a single layer of neurons that take in a set of inputs and produce a set of outputs. The output is computed by applying an activation function to a weighted sum of the inputs. We can describe a single layer neural network with $K$ hidden units mathematically via

$$
\begin{align*}
f(X) &= \beta_{0} + \sum_{k=1}^{K}\beta_{k}h_{k}(X) \\
     &= \beta_{0} + \sum_{k=1}^{K}\beta_{k}g\left(w_{k0} + \sum_{j=1}^{p}w_{kj}X_{j}\right)
\end{align*}
$$
where $g$ is some activation function. Notice that a single layer neural network with $p$ inputs and $K$ hidden units has $1 + K + K + pK = 1 + (p+2)K$ parameters. We can view a single layer neural network as a generalized linear model with $K$ basis functions. That is, a linear regression in $K$ activation functions. 

To gain some perspective on what the nonlinearity in an single layer neural network allows us to capture, let's look at a simple example. Suppose that we have $p=2$ input variables $X_{1}$ and $X_{2}$ and $K=2$ hidden units. Further, suppose that our activation function is $g(z) = z^2$. Set the following parameters:

$$
\begin{array}{ccc} \beta_{0} = 0, & \beta_{1} = \frac{1}{4}, & \beta_{2} = -\frac{1}{4} \\ w_{10} = 0, & w_{11} = 1, & w_{12} = 1, \\ w_{20} = 0, & w_{21} = 1, & w_{22} = -1.   \end{array}
$$

Then, 

$$
\begin{align*}
h_{1} &= (0 + X_{1} + X_{2})^2, \\
h_{2} &= (0 + X_{1} - X_{2})^2. 
\end{align*}
$$

Thus, 

$$
\begin{align*}
f(X) &= \beta_{0} + \beta_{1}h_{1} + \beta_{2}h_{2} \\
     &= 0 + \frac{1}{4}(X_{1} + X_{2})^2 - \frac{1}{4}(X_{1} - X_{2})^2 \\
     &= \frac{1}{4}(X_{1}^2 + 2X_{1}X_{2} + X_{2}^2) - \frac{1}{4}(X_{1}^2 - 2X_{1}X_{2} + X_{2}^2) \\
     &= X_{1}X_{2}.
\end{align*}
$$

So, we see that the sum of two nonlinear transformations of linear functions can produce an interaction term. This is a somewhat artificial example but the point is that the nonlinearity in a single layer neural network via an activation function can "detect" a variety of features in our data.

## Multilayer networks

In principle, a single layer neural network can approximate most functions of interest in machine learning. This is a consequence of a family of results known as [universal approximation theorems](https://en.wikipedia.org/wiki/Universal_approximation_theorem). However, in practice, deep learning is improved by using multiple layers. A neural network with multiple layers is known as a multilayer neural network. Let's examine the training of a multilayer neural network in more detail.

### Backpropagation

The implementation of gradient descent or its variant in deep learning typically require use to compute the gradient of chains of functions like

$$
{\bf y} = (f_{K} \circ f_{K-1} \circ \cdots \circ f_{1})({\bf x})
$$
where ${\bf x}$ are the inputs, ${\bf y}$ are the observations, and every $f_{i}$ has its own parameters. In neural networks, 

$$
f_{i}({\bf x}_{i-1}) = \sigma({\bf W}_{i-1}{\bf x}_{i-1} + {\bf b}_{i-1})
$$
is the activation function for the $i$-th layer, where ${\bf W}_{i-1}$ is the weight matrix and ${\bf b}_{i-1}$ is the bias vector. Here, ${\bf x}_{i-1}$ is the output from layer $i-1$. To train such a model, we need to compute the gradient of the loss function with respect to the parameters ${\bf W}_{i}$ and ${\bf b}_{i}$ for each layer $i$. This is done via the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm. We will now explain how this works. For a reference, see section 5.6.1 from [@deisenroth2020mathematics]. See also [these online notes](https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf).

## Convolutional and Recurrent Networks

### Overview of Neural Network Types

Thus far, we have been discussing what are known as feedforward neural networks. However, there are variations on this that are worth describing as different kinds of neural networks are used for different kinds of tasks, including image recognition, natural language processing, and time series analysis. There are three main types of neural networks: feedforward, convolutional, and recurrent networks.

#### Feedforward Neural Networks (FNNs)

- *Description*: Feedforward neural networks are the simplest type of neural network. They consist of an input layer, one or more hidden layers, and an output layer. Information flows in one direction, from input to output, without cycles or loops.
- *Use Cases*: FNNs are commonly used for tasks like classification and regression.
- *Advantages*: Easy to implement, work well for structured data.
- *Disadvantages*: Limited for tasks involving sequences or spatial data.

#### Convolutional Neural Networks (CNNs)

- *Description*: Convolutional neural networks are designed for tasks involving grid-like data, such as images. They use convolutional layers to automatically learn and detect features at different spatial hierarchies.
- *Use Cases*: CNNs excel at image classification, object detection, and segmentation.
- *Advantages*: Hierarchical feature learning, translational invariance.
- *Disadvantages*: May require substantial data, computationally intensive.

#### Recurrent Neural Networks (RNNs)

- *Description*: Recurrent neural networks are specialized for sequence data. They use recurrent connections to maintain memory of previous inputs. This enables tasks that depend on sequence context.
- *Use Cases*: RNNs are used for tasks like time series prediction, speech recognition, and natural language processing.
- *Advantages*: Sequence modeling, dynamic context understanding.
- *Disadvantages*: Vulnerable to vanishing gradients, limited memory.

#### Comparison and Contrast

- *Data Type*: FNNs work well with structured data, while CNNs are designed for grid-like data (e.g., images), and RNNs are for sequential data.
- *Architecture*: FNNs have no internal memory or feedback loops, while RNNs maintain memory through recurrent connections. CNNs have convolutional layers for feature extraction.
- *Applications*: FNNs are suitable for tabular data, CNNs for image-related tasks, and RNNs for sequences (text, time series).
- *Training*: FNNs are typically trained via backpropagation, CNNs leverage convolutional filters, and RNNs use backpropagation through time (BPTT).

Each type of neural network is tailored to specific data types and tasks. The choice of network architecture depends on the problem at hand, and in some cases, hybrid models may be used to leverage the strengths of multiple network types.


### Convolutional Neural Networks (CNNs)

Convolutional Neural Networks (CNNs) are a type of neural network designed for processing grid-like data, with a primary focus on tasks related to images and other spatial data. CNNs have been instrumental in revolutionizing computer vision and are widely used for tasks like image classification, object detection, and segmentation.

#### Architecture of CNNs

CNNs are characterized by a unique architecture that is well-suited for extracting features from grid-like data:

1. **Convolutional Layers**: These layers are responsible for feature extraction. They consist of learnable filters (kernels) that scan the input data, capturing local patterns. Convolutional layers create feature maps that highlight relevant spatial features.

2. **Pooling Layers**: Pooling layers reduce the spatial dimensions of the feature maps, decreasing the computational load. Max pooling is a common approach where the maximum value in a local region is retained.

3. **Fully Connected Layers**: After feature extraction, CNNs often have one or more fully connected layers similar to those in feedforward networks. These layers combine the features to make predictions.

#### Training Process for CNNs

The training process for CNNs involves the following key steps:

1. **Initialization**: Weights and biases in the network are initialized, typically with small random values.

2. **Forward Propagation**: During forward propagation, input data is passed through the network. Convolutional and pooling layers extract features, while fully connected layers produce predictions.

3. **Loss Calculation**: A loss function is used to measure the difference between the predicted values and the ground truth. Common loss functions include cross-entropy for classification tasks.

4. **Backpropagation**: During backpropagation, gradients are computed with respect to the loss. Gradients are used to adjust the network's weights and biases in a direction that minimizes the loss.

5. **Optimization**: Various optimization algorithms, like stochastic gradient descent (SGD) or its variants (e.g., Adam), are used to update the network's parameters. The learning rate determines the size of weight updates.

6. **Training Loop**: Steps 2 to 5 are repeated iteratively for a fixed number of epochs or until convergence. Mini-batch training is common, where the data is divided into small batches for more efficient training.

#### CNNs in Practice

In practice, CNNs are often pre-trained on large datasets (e.g., ImageNet) to learn useful feature representations. Transfer learning allows fine-tuning these pre-trained models for specific tasks, reducing the need for large datasets.

The success of CNNs can be attributed to their ability to automatically learn hierarchical features from raw data. They excel at capturing patterns in different spatial hierarchies, making them suitable for a wide range of computer vision tasks.

### Recurrent Neural Networks (RNNs)

Recurrent Neural Networks (RNNs) are a type of neural network specially designed for sequential data and tasks that depend on maintaining memory of previous inputs. RNNs have been widely used in natural language processing, time series analysis, speech recognition, and more.

#### Architecture of RNNs

RNNs are characterized by their recurrent connections, which enable the network to maintain memory across time steps. The basic architecture of an RNN consists of the following components:

1. **Hidden State**: At each time step, an RNN maintains a hidden state that serves as a memory of past inputs and computations.

2. **Recurrent Connection**: The hidden state at the current time step is computed based on the input at the current time step and the hidden state at the previous time step. This recurrent connection allows RNNs to capture dependencies across sequential data.

3. **Output Layer**: RNNs can have an output layer, which produces predictions or features based on the current hidden state. The output can be used for various tasks, such as sequence classification or prediction.

#### Training Process for RNNs

The training process for RNNs involves the following key steps:

1. **Initialization**: Weights and biases in the network are initialized, typically with small random values.

2. **Forward Propagation**: During forward propagation, input data is passed through the network one time step at a time. The hidden state is updated at each time step based on the input and the previous hidden state.

3. **Loss Calculation**: A loss function is used to measure the difference between the predicted values and the ground truth. Common loss functions include mean squared error for regression tasks and cross-entropy for classification tasks.

4. **Backpropagation Through Time (BPTT)**: BPTT is a variant of backpropagation used to compute gradients through time. It involves calculating gradients of the loss with respect to the network's parameters, considering all time steps.

5. **Optimization**: Gradients are used to adjust the network's weights and biases using optimization algorithms like stochastic gradient descent (SGD) or its variants.

6. **Training Loop**: Steps 2 to 5 are repeated iteratively for a fixed number of time steps or until convergence. Mini-batch training is common, where the data is divided into small batches for more efficient training.

#### RNNs in Practice

In practice, RNNs may face challenges like vanishing gradients, where gradients become extremely small and hinder the training process. To address this, variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been developed with more sophisticated gating mechanisms.

RNNs are effective for tasks that involve sequential data, as they can capture dependencies and temporal patterns. They have been widely used for natural language processing, including tasks like language modeling, machine translation, and sentiment analysis.

Despite their effectiveness, RNNs may be computationally expensive and may not scale well to very long sequences. In such cases, alternatives like attention mechanisms or Transformers have gained popularity.


## `r icons::icon_style(icons::fontawesome("r-project"),scale=2,fill="steelblue")` Neural Networks in R



## Further Topics on Deep Learning

This is some further information on neural networks and deep learning based on questions raised by students. 

### Recent Applications of Neural Networks and Deep Learning

1. **Natural Language Processing (NLP):**
   - *Example*: BERT, GPT-3, and other large transformer models for tasks like language translation, text generation, and sentiment analysis.
   - *Learn More*: [OpenAI's GPT-3](https://openai.com/research/gpt-3), [Google's BERT](https://ai.google/research/pubs/pub48095)

2. **Computer Vision:**
   - *Example*: Convolutional Neural Networks (CNNs) for image classification, object detection, and facial recognition.
   - *Learn More*: [ImageNet Large Scale Visual Recognition Challenge](http://www.image-net.org/challenges/LSVRC/)

3. **Healthcare:**
   - *Example*: Using deep learning to analyze medical images, detect diseases, and predict patient outcomes.
   - *Learn More*: [Stanford's CheXNet](https://stanfordmlgroup.github.io/projects/chexnet/)

4. **Autonomous Vehicles:**
   - *Example*: Self-driving cars rely on neural networks to perceive their surroundings and make decisions.
   - *Learn More*: [Waymo's Self-Driving Technology](https://waymo.com/technology/)

5. **Recommender Systems:**
   - *Example*: Recommending products, movies, or content to users based on their preferences and behavior.
   - *Learn More*: [Netflix's Recommendation Algorithm](https://help.netflix.com/en/node/100639)

6. **Finance:**
   - *Example*: Predictive modeling for stock price forecasting, fraud detection, and algorithmic trading.
   - *Learn More*: [Stock Price Prediction with LSTM](https://github.com/mwitiderrick/stockprice)

7. **Voice Assistants:**
   - *Example*: Voice recognition and natural language understanding in smart speakers like Amazon Echo and Google Home.
   - *Learn More*: [Amazon Alexa](https://developer.amazon.com/en-US/alexa)

8. **Generative Adversarial Networks (GANs):**
   - *Example*: Creating art, generating synthetic images, and deepfakes.
   - *Learn More*: [NVIDIA's GAN Research](https://www.nvidia.com/en-us/research/ai-playground/)

9. **Robotics:**
   - *Example*: Deep reinforcement learning for robot control and autonomous navigation.
   - *Learn More*: [OpenAI's Robotics Research](https://openai.com/research/robotics)

10. **Climate Science:**
    - *Example*: Using neural networks to analyze climate data, model climate change, and predict extreme weather events.
    - *Learn More*: [DeepMind's Climate Science](https://deepmind.com/research/case-studies/climate-change)

### Sizes of Neural Networks in Various Applications

1. **Natural Language Processing (NLP):**
   - *GPT-3*: GPT-3, developed by OpenAI, is one of the largest language models with 175 billion parameters.
   - *BERT*: Google's BERT has 340 million parameters and is highly influential in NLP.

2. **Computer Vision:**
   - *ImageNet Models*: Models like VGG-16, VGG-19, and ResNet used for image classification have tens of millions of parameters.
   - *Large CNNs*: In complex computer vision tasks, models can have hundreds of millions of parameters. Examples include Inception models and DenseNet.

3. **Healthcare:**
   - The size of neural networks in healthcare applications varies, ranging from a few million to tens of millions of parameters for tasks like medical image analysis.

4. **Autonomous Vehicles:**
   - Neural networks used in autonomous vehicles vary in size. Perception networks processing sensor data may have tens of millions of parameters, while decision-making networks might be smaller.

5. **Generative Adversarial Networks (GANs):**
   - Large GANs, such as BigGAN, can have hundreds of millions of parameters and are used for image generation.

6. **Climate Science:**
   - Climate models using deep learning can have varying numbers of parameters, typically in the millions to tens of millions, depending on the model's complexity.


### Choosing Depth and Width of Neural Networks

Selecting the appropriate depth and width of neural networks is a crucial decision in deep learning. Here are common techniques and considerations for making this choice:

1. **Model Complexity vs. Data Size:**
   - Consider the balance between the model's complexity and the size of the available data. Smaller datasets may benefit from simpler, shallower networks, while larger datasets can support deeper and wider architectures.

2. **Empirical Exploration:**
   - Start with a basic architecture and experiment with deeper or wider networks to find the optimal balance. This involves training and evaluating different architectures to identify the best-performing one for the task.

3. **Regularization Techniques:**
   - Implement techniques like dropout, weight decay (L2 regularization), and batch normalization to prevent overfitting, enabling deeper networks without sacrificing generalization performance.

4. **Transfer Learning:**
   - For certain applications, consider transfer learning from pretrained models. Fine-tuning a pretrained model may require fewer layers and parameters, saving training time.

5. **Architectural Variations:**
   - Explore different architectural variations, such as the number of layers, hidden units, and filter sizes. Techniques like grid search or random search can be employed to discover promising architectures.

6. **Architectural Search Algorithms:**
   - Utilize automated neural architecture search (NAS) algorithms, including reinforcement learning-based methods and evolutionary algorithms, to discover optimal neural network architectures automatically.

7. **Model Size and Computational Resources:**
   - Be mindful of computational resources when choosing the depth and width. Deeper and wider models demand more training time and memory.

8. **Task Complexity:**
   - The complexity of the task can guide architectural decisions. Simple tasks may require shallower networks, while complex tasks may benefit from deeper and wider architectures.

9. **Pruning and Quantization:**
   - After training, apply pruning or quantization techniques to reduce the size and complexity of trained models without sacrificing performance.

10. **Ensemble Methods:**
    - Consider using ensemble methods that combine predictions from multiple neural networks with different architectures to enhance overall performance.

11. **Domain Expertise:**
    - Knowledge of the domain and task can guide architectural choices. For example, specific data types, like sequential data in NLP, may benefit from recurrent or attention-based architectures.

12. **Validation and Cross-Validation:**
    - Leverage cross-validation and validation performance metrics to identify the appropriate trade-off between model capacity and generalization.

The choice of architecture often involves a trade-off between model capacity and the risk of overfitting. Experimentation may be required to determine the ideal architecture for a particular task. The deep learning field continues to evolve, offering new techniques and tools to assist with architecture selection.


## Freely Available Libraries and Packages for Neural Networks

### R Packages for Neural Networks and Deep Learning

1. **keras:**
   - *Description*: The `keras` package in R provides an interface to the Keras deep learning framework. Keras is known for its ease of use and flexibility and can run on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK).
   - *Documentation*: [keras Documentation](https://keras.rstudio.com/)
   - *Tutorials*: [keras Tutorials](https://keras.rstudio.com/articles/tutorial_basic_classification.html)

2. **tensorflow:**
   - *Description*: The `tensorflow` package for R provides a low-level interface to the TensorFlow deep learning framework. It allows for fine-grained control over model architecture and training.
   - *Documentation*: [tensorflow Documentation](https://tensorflow.rstudio.com/)
   - *Tutorials*: [tensorflow Tutorials](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_classification/)
   
3. **torch:**
   - *Description*: The `torch` package offers an interface to PyTorch, a deep learning framework known for its flexibility and dynamic computation graph. It is suitable for both research and production.
   - *Documentation*: [torch Documentation](https://torch.mlverse.org/)
   - *Textbook*: [Deep Learning and Scientific Computing with R](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/)

4. **caret:**
   - *Description*: While not a deep learning library, the `caret` package is a versatile tool for training and evaluating machine learning models, including neural networks. It provides a unified interface for various R packages and algorithms.
   - *Documentation*: [caret Documentation](https://topepo.github.io/caret/)
  

5. **h2o:**
   - *Description*: The `h2o` package is designed for scalable machine learning and deep learning. It provides an easy-to-use interface for building deep learning models, autoML, and more.
   - *Documentation*: [h2o Documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepLearningBooklet.pdf)


These R packages cover a range of deep learning needs, from high-level and user-friendly interfaces to low-level control and flexibility. Explore the provided documentation and tutorials to get started with deep learning using these tools.


### Other Languages

1. **TensorFlow:**
   - *Description*: TensorFlow is an open-source deep learning framework by Google, widely used for building and training neural networks.
   - *Documentation*: [TensorFlow Official Documentation](https://www.tensorflow.org/)
   - *Tutorials*: [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)

2. **Keras:**
   - *Description*: Keras is a high-level neural networks API that runs on top of TensorFlow and other frameworks, simplifying model building.
   - *Documentation*: [Keras Official Documentation](https://keras.io/)
   - *Tutorials*: [Keras Tutorials](https://keras.io/getting_started/intro_to_keras_for_engineers/)

3. **PyTorch:**
   - *Description*: PyTorch is a deep learning framework known for its flexibility and dynamic computation graph, ideal for research and development.
   - *Documentation*: [PyTorch Official Documentation](https://pytorch.org/docs/stable/index.html)
   - *Tutorials*: [PyTorch Tutorials](https://pytorch.org/tutorials/)

4. **scikit-learn:**
   - *Description*: While primarily focused on traditional machine learning, scikit-learn offers neural network capabilities for integration with other machine learning tasks.
   - *Documentation*: [scikit-learn Official Documentation](https://scikit-learn.org/stable/documentation.html)
   - *Tutorials*: [scikit-learn Neural Networks Guide](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)

5. **Fastai:**
   - *Description*: Fastai is a high-level deep learning library built on PyTorch, designed to make deep learning more accessible for practitioners.
   - *Documentation*: [Fastai Documentation](https://docs.fast.ai/)
   - *Tutorials*: [Fastai Tutorials](https://docs.fast.ai/tutorial.html)

6. **Flux.jl:**
   - *Description*: Flux is a widely used deep learning library in Julia known for its simplicity and flexibility. It provides a user-friendly API for defining and training neural networks.
   - *Documentation*: [Flux.jl Documentation](https://fluxml.ai/Flux.jl/stable/)

7. **Knet.jl:**
   - *Description*: Knet (pronounced "kay-net") is a deep learning framework that aims to be as fast and efficient as possible. It's suitable for both research and production use.
   - *Documentation*: [Knet.jl Documentation](https://denizyuret.github.io/Knet.jl/stable/)


These libraries and packages offer diverse capabilities and features for implementing and training neural networks. Explore the provided documentation and tutorials to get started with deep learning using these tools.




## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)