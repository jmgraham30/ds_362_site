---
title: "Lesson 1"
subtitle: "Overview of Data Mining and Machine Learning"
author: "JMG"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson01.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| message: false
#| warning: false
#| echo: false

# load packages used in document
library(tidyverse)
library(tidytuesdayR)
library(ISLR2)

theme_set(theme_minimal(base_size = 13))

egg_prod_df  <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')
cage_free_df <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/cage-free-percentages.csv')
```

## Learning Objectives

After this lesson, students will be able to:

1) Recall how to load, manipulate, and plot data in R. 


2) Recall the basic data science workflow. 

3) State in general terms what we mean by **data mining** and **machine learning**, also known as **statistical learning**.

4) Recognize where data mining and machine learning fit in the basic data science workflow.


## Readings, etc.

1) Read Chapter 1 of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. 

2) The following two video lectures are also recommended:

* Motivating problems for machine (statistical) learning. [Watch video on YouTube](https://youtu.be/LvySJGj-88U).
  
```{r}
#| echo: false

vembedr::embed_youtube(id="LvySJGj-88U",height=450) %>%
  vembedr::use_align("center")
```


* Supervised and unsupervised learning. [Watch video on YouTube](https://youtu.be/B9s8rpdNxU0).
  
```{r}
#| echo: false

vembedr::embed_youtube(id="B9s8rpdNxU0",height=450) %>%
  vembedr::use_align("center")
```

3) Skim the README for the [Tidy Tuesday data repository](https://github.com/rfordatascience/tidytuesday) [@TT]. [View the repository.](https://github.com/rfordatascience/tidytuesday) Throughout the semester, we will use example data from the Tidy Tuesday data repository.

## Course Overview

This course provides coverage of essential topics in data science at the intermediate level with an emphasis on machine learning. Broadly, we will cover algorithms that are commonly used for gaining insight from data. The things you learn in the class will be applicable in a variety of different areas, professions, and even other classes.

There is a website for the course, [view the website](https://knowledge-discovery.netlify.app/). For course logistics, see the official course syllabus, [view the syllabus](https://knowledge-discovery.netlify.app/syllabus.html). Assignments and other information specific to the course in a given semester will be posted on the course learning management system (LMS). The course website provides links to many additional resources, [view the links](https://knowledge-discovery.netlify.app/links.html).

While we will refer often to several texts (most of which have been published online as open access materials) throughout the course, most of the content will be delivered via "notebooks" like the one you're reading now[^1] that intermix text, mathematical notation, figures, programming language code, and web links. In some cases, you will be asked to go through the notebooks on your own and sometimes we will go through the notebooks together. Either way, any time you encounter code in a notebook, it is expected that you will take the time to run any code (mostly by copying and pasting) for yourself. The only way to master the material is through active participation.  

[^1]: The notebooks are created using [Quarto](https://quarto.org/) and [R markdown](https://rmarkdown.rstudio.com/), topics that we will cover in more detail later.  

For this course, we assume that students have some prior experience in using a programming language like [R](https://www.r-project.org/) or [Python](https://www.python.org/) to analyze data. In particular, it is assumed that students in the course can load, clean and plot data in R or Python. A facility in working with data frames via an R package like [`dplyr`](https://dplyr.tidyverse.org/) or the [`pandas`](https://pandas.pydata.org/) Python library is assumed. We also assume students can use [`ggplot2`](https://ggplot2.tidyverse.org/) in R or [`matplotlib`](https://matplotlib.org/) in Python for making appropriate plots of data. A great resource for the assumed programming background is [*R for Data Science*](https://r4ds.had.co.nz/) [@wickham2016r]. [View the online version of *R for Data Science*](https://r4ds.had.co.nz/). You can also review material from the prerequisite course DS 201 *Introduction to Data Science*, [view the DS 201 course website](https://intro-ds.netlify.app/). 

During the course, our preference tends toward using R for examples and application. Coding will be essential for assignments and it is recommended that students use R on coding assignments. However, students may request to use another language such as Python, requests will be granted or denied by the instructor on a case-by-case basis with a rationale for the decision provided.       

### Why R?

The [R language](https://www.r-project.org/) [@r2023] for statistical computing is one of the most popular computing tools for data science, among the other [most popular](https://www.datacamp.com/blog/top-programming-languages-for-data-scientists-in-2022) are [Python](https://www.python.org/) and [Julia](https://julialang.org/). Some of the strengths of R include

* free and open source which facilitates reproducibility and auditability, 

* ecosystem of packages that greatly extend the functionality of R,

* [rmarkdown](https://rmarkdown.rstudio.com/) [@rmarkdown2020] and [Quarto](https://quarto.org/) frameworks for [literate programming](https://en.wikipedia.org/wiki/Literate_programming) enable presentation and communication of data analyses and facilitate reproducibility and auditability, 

* [RStudio](https://posit.co/download/rstudio-desktop/) [integrated development environment](https://en.wikipedia.org/wiki/Integrated_development_environment) (IDE) by [Posit](https://posit.co/) enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages, 

* a strong and collaborative user community, see R Community Explorer website, [view the website.](https://r-community.org/usergroups/). 

### An `r icons::icon_style(icons::fontawesome("r-project"),scale=2,fill="steelblue")` refresher

This section reviews some coding in R with which the student is assumed to have some familiarity such as from the prerequisite course DS 201 *Introduction to Data Science*. 

[![Artwork by Allison Horst](https://github.com/allisonhorst/stats-illustrations/blob/main/rstats-artwork/exploder.gif?raw=1){fig-alt="A gif showing the logo for the R language for statistical computing."}](https://allisonhorst.com/)

Throughout the course, we will make use of many R packages. For example, we will use the `tidyverse` package (which is a meta package that contains packages such as `dplyr` and `ggplot2`), the `tidytuesdayR` package for importing data from the [Tidy Tuesday data repository](https://github.com/rfordatascience/tidytuesday) [@TT], and the `ISLR2` package corresponding to the textbook *An Introduction to Statistical Learning* [@tibshirani2017introduction]. Packages contain data, functions, etc. An R package must be installed before it can be loaded and used. You only need to install a package once, but you have to load packages at each new R session. 

There are two common ways to install packages:

1) If using the RStudio IDE (highly recommended), use the Install button on the Packages tab. All you have to do then is to search for the package you want to install and click the Install option. Note that this is most useful for R packages available through the [Comprehensive R Archive Network](https://cran.r-project.org/web/packages/available_packages_by_name.html) (CRAN). 

2) With the `install.packages` function. For example, 

```{r}
#| eval: false
#| code-fold: false

install.packages(c("tidyverse","tidytuesdayR","ISLR2"))
```

If you want to install a package from some other repository, refer to the documentation, do a web search, or ask the instructor. 

To load packages, use

```{r}
#| eval: false
#| code-fold: false

# load packages used in document
library(tidyverse)
library(tidytuesdayR)
library(ISLR2)
```

Now, functions, data, etc. from these packages will be available for reference by name. For example, we can access the documentation for the `tt_load` function from `tidytuesdayR` package by running the command

```{r}
#| code-fold: false

?tt_load
```

This informs us that to load data from the TidyTuesday Github, we need to input a character corresponding to the date for that particular data set. For example, 

```{r}
#| code-fold: false
#| eval: false

tt_data_examp <- tt_load("2023-04-11")
```

 downloads two data files, `egg-production.csv` and `cage-free-percentages.csv` from the April 11, 2023 Tidy Tuesday post. We can access the egg production data as a data frame as follows:
 
```{r}
#| code-fold: false
#| eval: false

egg_prod_df <- tt_data_examp$`egg-production`
```

The first few rows of this data set look as follows:

```{r}
#| echo: false

egg_prod_df %>%
  head() %>%
  kableExtra::kable()
```

Let's use the `glimpse` function to get a quick sense of what's in this data:

```{r}
#| code-fold: false

glimpse(egg_prod_df)
```


**Question:** What are some initial questions we might want to address using the egg production data? Read through the repository for the egg production data [view repository](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-04-11/readme.md). Further, [see the report summary](https://thehumaneleague.org/article/E008R01-us-egg-production-data) by [The Human League](https://thehumaneleague.org/) project. What do the researchers report based on an analysis of the data? 

**Exercise:** Examine the data downloaded as `cage-free-percentages.csv`. What information does it contain? 

The following R code essentially reproduces Figure 2 from the egg production report by [The Human League](https://thehumaneleague.org/), [see the report](https://assets.ctfassets.net/ww1ie0z745y7/5x4LpTMoZLQbGpYSaZXpY3/24e96497c51f7398f03776790e9a1b9d/E008R01-us-egg-production-data.pdf):

```{r}
#| label: fig-egg_report
#| fig-cap: Reproducing Figure 2 from The Human League report on egg production. 
#| fig-alt: Plot of percent of US hens in cage-free housing per year . Plots shows that the percent of US hens in cage-free housing has increasing significantly over time. 

cage_free_df %>%
  mutate(year=lubridate::year(observed_month)) %>%
  group_by(year) %>%
  summarise(percent_hens=mean(percent_hens)) %>%
  ggplot(aes(x=year,y=percent_hens)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks=2007:2021) + 
  scale_y_continuous(breaks=seq(0,30,by=5),labels=paste0(as.character(seq(0,30,by=5)),"%")) + 
  labs(x="Date (year)",y="Percentage of US hens in cage-free housing")
```

**Question:** What does the plot in @fig-egg_report show? 

**Exercise:** Carefully examine the code used to make @fig-egg_report. What does each line of the code do? Note that these are the types of basic data manipulations and plots you should be comfortable making entering into DS 362. 

**Exercise:** Look at the list of data sets contained in the `ISLR2` package by running the command `data(package="ISLR2")`. Pick two of the data sets and determine what information is contained in your chosen data sets. Make a couple of basic plots using one of the data sets you chose.   


### Review of the Data Science Workflow

Before starting our study of data mining and machine learning, let's review the basic data science workflow as covered in an introductory level data science course. Generally, the steps in the workflow are:

1) Gather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application. If you're going to use machine learning as part of your data analysis, this is a good place to make sure that you have enough data and also to split your data into a training set and test set.  

2) Document the data and data collection process. 

    * It is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See *The Alignment Problem* by Brian Christian for a thoughtful discussion on the issues related to these types of considerations [@christian2020alignment].  

3) Import the data for analysis. If you have not already, you may want to split your data into a training set and test set.

4) Explore, clean, and transform the data. Data visualization is essential at this step. Make sure to apply the same manipulations used for cleaning and transformation to both the training and test sets.

5) Generate initial insight or more detailed questions. 

    * Steps 4-5 constitute an exploratory data analysis (EDA). It is typical to conduct EDA before doing any more sophisticated analyses such as machine learning, and this is a habit you should get into. Students are assumed to be familiar with the EDA process from DS 201. If you need a refresher, the following video is recommended: 
    
    * Exploratory data analysis worked example video by Hadley Wickham, [watch the video on YouTube](https://youtu.be/go5Au01Jrvs).

```{r}
#| echo: false

vembedr::embed_youtube(id="go5Au01Jrvs",height=450) %>%
  vembedr::use_align("center")
```


6) Decide what type(s) of analysis or analyses are to be performed. This is the stage at which machine learning enters into the process.  

    * Make sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See [Table 1.1](https://datasciencebook.ca/intro.html#tab:questions-table) from [@timbers2022data] for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table [here](https://datasciencebook.ca/intro.html#tab:questions-table).

7) Assess the analysis. In particular, use an appropriate metric to estimate model error. 

8) At this stage, it may be necessary to repeat steps 1 - 7.

9) Report your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis. 


It is essential the our data science workflow be **reproducible** and **auditable**. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility).  @fig-reproducible illustrates the concept of reproducibility. 

[![Artwork by Allison Horst](https://github.com/allisonhorst/stats-illustrations/blob/main/rstats-artwork/reproducibility_court.png?raw=1){#fig-reproducible fig-alt="An illustration of the concept of reproducibility."}](https://allisonhorst.com/)


## Overview of data mining and machine learning





## Preparation for the next lesson

For the next lesson:

* Read section 2.1 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read sections 2.1 and 2.2 of *Statistical Learning with Math and R* [@suzuki2020statistical].

* Watch the corresponding video lecture on regression. [View on YouTube](https://youtu.be/ox0cKk7h4o0).

```{r}
#| echo: false

vembedr::embed_youtube(id="ox0cKk7h4o0",height=450) %>%
  vembedr::use_align("center")
```


## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::



[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)
