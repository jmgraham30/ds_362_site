[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS 362: Data-Driven Knowledge Discovery",
    "section": "",
    "text": "Welcome to the course website for DS 362: Data-Driven Knowledge Discovery for Fall 2023.\nThis course is offered at the University of Scranton and is being taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "DS 362: Data-Driven Knowledge Discovery",
    "section": "License",
    "text": "License\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html#required-readings",
    "href": "syllabus.html#required-readings",
    "title": "Syllabus",
    "section": "Required Readings",
    "text": "Required Readings\n\nAn Introduction to Statistical Learning 2nd Ed. by James, Witten, Hastie, and Tibshirani, view the free online version of the text. There is a set of lecture videos associated with this text, view these videos on YouTube. There is also a version of the text using Python instead of R, view the Python version.\nStatistical Learning with Math and R by Joe Suzuki, available online through the Weinberg Library, view the text. There is also a version of the text using Python instead of R, view the Python version"
  },
  {
    "objectID": "syllabus.html#r-references",
    "href": "syllabus.html#r-references",
    "title": "Syllabus",
    "section": "R References",
    "text": "R References\n\nHands-On Programming with R by Garrett Grolemund, view the free online version of the text.\nR for Data Science by Hadley Wickham & Garrett Grolemund, view the free online version of the text.\nData Visualization A Practical Introduction by Healy, view the free online version of the text."
  },
  {
    "objectID": "syllabus.html#additional-readings",
    "href": "syllabus.html#additional-readings",
    "title": "Syllabus",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nElements of Statistical Learning 2nd Ed. by Hastie, Tibshirani, and Friedman, freely available online here.\nStatistical Learning from a Regression Perspective by Richard A. Berk, available online through the Weinberg Library here."
  },
  {
    "objectID": "syllabus.html#recommended-readings",
    "href": "syllabus.html#recommended-readings",
    "title": "Syllabus",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nHands-On Machine Learning with R, view the free online version of the text.\nTidy Modeling with R by Max Kuhn & Julia Silge, view the free online version of the text.\nFeature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson, view the free online version of the text.\nTelling Stories with Data by Rohan Alexander, view the free online version of the text.\nData Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee, view the free online version of the text.\nTree-Based Methods for Statistical Learning by Brandon M. Greenwell, view the free online version of the text."
  },
  {
    "objectID": "syllabus.html#further-reading",
    "href": "syllabus.html#further-reading",
    "title": "Syllabus",
    "section": "Further Reading",
    "text": "Further Reading\n\nElements of Statistical Learning 2nd Ed. by Hastie, Tibshirani, and Friedman, view the free online version of the text.\nLinks to additional resources related to the course material will be posted on the course website. View the resources link."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "links.html#texts",
    "href": "links.html#texts",
    "title": "Links",
    "section": "Texts",
    "text": "Texts\n\nR Related\n\nDeep Learning and Scientific Computing with R torch, view the free online version of the text.\nggplot2: Elegant Graphics for Data Analysis (3e), view the free online version of the text\n\n\n\nPython Related\n\nPython for Data Analysis, 3E, view the free online version of the text"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "DS 362: Data-Driven Knowledge Discovery",
    "section": "",
    "text": "Welcome to the course website for DS 362: Data-Driven Knowledge Discovery for Fall 2023.\nThis course is offered at the University of Scranton and is being taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "links.html#websites",
    "href": "links.html#websites",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "lesson01/index.html",
    "href": "lesson01/index.html",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nRecall how to load, manipulate, and plot data in R.\nRecall the basic data science workflow.\nState in general terms what we mean by data mining and machine learning, also known as statistical learning.\nRecognize where data mining and machine learning fit into the basic data science workflow."
  },
  {
    "objectID": "lesson01/index.html#learning-objectives",
    "href": "lesson01/index.html#learning-objectives",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nRecall how to load, manipulate, and plot data in R.\nRecall the basic data science workflow.\nState in general terms what we mean by data mining and machine learning, also known as statistical learning.\nRecognize where data mining and machine learning fit into the basic data science workflow."
  },
  {
    "objectID": "lesson01/index.html#readings-etc.",
    "href": "lesson01/index.html#readings-etc.",
    "title": "Lesson 1",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapter 1 of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nThe following two video lectures are also recommended:\n\n\nMotivating problems for machine (statistical) learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSupervised and unsupervised learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSkim the README for the Tidy Tuesday data repository (Mock 2018). View the repository. Throughout the semester, we will use example data from the Tidy Tuesday data repository."
  },
  {
    "objectID": "lesson01/index.html#references",
    "href": "lesson01/index.html#references",
    "title": "Lesson 1",
    "section": "References",
    "text": "References\n\n\nChristian, Brian. 2020. The Alignment Problem: Machine Learning and Human Values. WW Norton & Company.\n\n\nMock, J. Thomas. 2018. “Tidy Tuesday.” GitHub Repository. https://github.com/rfordatascience/tidytuesday; GitHub.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-08-28\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2        * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n ISLR2          * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n palmerpenguins * 0.1.1   2022-08-15 [1] CRAN (R 4.3.0)\n purrr          * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo    * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR   * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "links.html#videos",
    "href": "links.html#videos",
    "title": "Links",
    "section": "Videos",
    "text": "Videos\n\nGetting started with Quarto YouTube video, watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nDatasheets for Datasets video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nExploratory data analysis worked example video by Hadley Wickham, watch the video on YouTube."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nThis course covers the process of knowledge discovery including data selection, pre-processing, transformation, data mining, evaluation, and validation, with an emphasis on data mining concepts, algorithms, and techniques for common tasks such as association rule learning, classification, regression, clustering, and outlier detection.\n\n\nPrerequisites\n\nCMPS 240 and DS 201 and DS 210"
  },
  {
    "objectID": "syllabus.html#student-learning-objectives-and-assessment",
    "href": "syllabus.html#student-learning-objectives-and-assessment",
    "title": "Syllabus",
    "section": "Student Learning Objectives and Assessment:",
    "text": "Student Learning Objectives and Assessment:\n\n\n\n\nTable 1: Course objectives and assessment.\n\n\nCourse SLO\nAssessment\n\n\n\n\nAfter completing this course, students will be able to import, pre-process, and transform common data sets as appropriate for use in machine learning applications.\nHomework, Case Studies, and Project\n\n\nAfter completing this course, students will be able to implement and apply foundational machine learning methods.\nHomework, Case Studies, and Project\n\n\nAfter completing this course, students will be able to present and communicate results obtained via data analysis in an effective manner.\nCase Studies and Project"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nGrade Policy\nThe overall course grade will be based on (roughly twelve) weekly homework assignment totaling 30% of the overall course grade, two data modeling case study assignments totaling 40% of the overall course grade, and a semester project totaling 30% of the overall course grade.\n\n\nGrade Scale\nLetter grades will be assigned based on the following scale:\n\n\n\n\nTable 2: Letter grade scale.\n\n\nGrade Range\nLetter Grade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n83-86\nB\n\n\n80-82\nB-\n\n\n76-79\nC+\n\n\n72-75\nC\n\n\n69-71\nC-\n\n\n65-68\nD+\n\n\n60-64\nD\n\n\n&lt;60\nF"
  },
  {
    "objectID": "syllabus.html#weekly-schedule",
    "href": "syllabus.html#weekly-schedule",
    "title": "Syllabus",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nWeek 1: Review of background concepts\nWeek 2: Data mining\nWeek 3: Machine learning\nWeek 4: Regression\nWeek 5: Classification; Project component 1 due\nWeek 6: Resampling methods\nWeek 7: Model selection and regularization; Case Study 1 due\nWeek 8: CARTs\nWeek 9: Support vector machines\nWeek 10: Neural Networks\nWeek 11: Deep learning; Project component 2 due\nWeek 12: Association rule learning\nWeek 13: Unsupervised methods; Case Study 2 due\nWeek 14: Reinforcement\nWeek 15: Ethical considerations; Project final version due"
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important Dates",
    "text": "Important Dates\n\n\n\n\nTable 3: Important dates.\n\n\nEvent\nDate\n\n\n\n\nClasses begin\n08-28\n\n\nLast day to add classes\n90-01\n\n\nHoliday, no classes\n09-04\n\n\n100% tuition refund\n09-06\n\n\nDrop (no grade)\n09-27\n\n\nFall break\n10-07 to 10-10\n\n\nMid-semester\n10-18\n\n\nWithdraw with W\n11-10\n\n\nThanksgiving break\n11-22 to 11-26\n\n\nLast week\n12-05 to 12-11\n\n\nFinals\n12-12 to 12-16"
  },
  {
    "objectID": "syllabus.html#students-with-disabilities",
    "href": "syllabus.html#students-with-disabilities",
    "title": "Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nReasonable academic accommodations may be provided to students who submit relevant and current documentation of their disability. Students are encouraged to contact the Center for Teaching and Learning Excellence (CTLE) at disabilityservices@scranton.edu or (570) 941-4038 if they have or think they may have a disability and wish to determine eligibility for any accommodations. For more information, please visit http://www.scranton.edu/disabilities."
  },
  {
    "objectID": "syllabus.html#writing-center-services",
    "href": "syllabus.html#writing-center-services",
    "title": "Syllabus",
    "section": "Writing Center Services",
    "text": "Writing Center Services\nThe Writing Center focuses on helping students become better writers. Consultants will work one-on-one with students to discuss students’ work and provide feedback at any stage of the writing process. Scheduling appointments early in the writing progress is encouraged.\nTo meet with a writing consultant, call (570) 941-6147 to schedule an appointment, or send an email with your available meeting times, the course for which you need assistance, and your phone number to: writing-center@scranton.edu. The Writing Center does offer online appointments for our distance learning students."
  },
  {
    "objectID": "syllabus.html#academic-honesty-and-integrity",
    "href": "syllabus.html#academic-honesty-and-integrity",
    "title": "Syllabus",
    "section": "Academic Honesty and Integrity",
    "text": "Academic Honesty and Integrity\nEach student is expected to do their own work. It is also expected that each student respect and abide by the Academic Code of Honesty as set forth in the University of Scranton student handbook. Conduct that violates the Academic Code of Honesty includes plagiarism, duplicate submission of the same work, collusion, providing false information, unauthorized use of computers, theft and destruction of property, and unauthorized possession of tests and other materials. Steps taken in response to suspected violations may include a discussion with the instructor, an informal meeting with the dean of the college, and a hearing before the Academic Dishonesty Hearing Board. Students who are found to have violated the Code will ordinarily be assigned the grade F by the instructor and may face other sanctions. The complete Academic Code of Honesty is located on the University website at https://www.scranton.edu/academics/wml/acad-integ/acad-code-honesty.shtml."
  },
  {
    "objectID": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "href": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "title": "Syllabus",
    "section": "My Reporting Obligation as a Responsible Employee",
    "text": "My Reporting Obligation as a Responsible Employee\nAs a faculty member, I am deeply invested in the well-being of each student I teach. I am here to assist you with your work in this course. Additionally, if you come to me with other non-course-related concerns, I will do my best to help. It is important for you to know that all faculty members are required to report incidents of sexual harassment or sexual misconduct involving students. This means that I cannot keep information about sexual harassment, sexual assault, sexual exploitation, intimate partner violence or stalking confidential if you share that information with me. I will keep the information as private as I can but am required to bring it to the attention of the University’s Title IX Coordinator, Elizabeth M. Garcia, or Deputy Title IX Coordinator, Diana M. Collins, who, in conversation with you, will explain available support, resources, and options. I will not report anything to anybody without first letting you know and discussing choices as to how to proceed. The University’s Counseling Center (570-941-7620) is available to you as a confidential resource; counselors (in the counseling center) do not have an obligation to report to the Title IX Coordinator."
  },
  {
    "objectID": "syllabus.html#non-discrimination-statement",
    "href": "syllabus.html#non-discrimination-statement",
    "title": "Syllabus",
    "section": "Non-discrimination Statement",
    "text": "Non-discrimination Statement\nThe University is committed to providing an educational, residential, and working environment that is free from harassment and discrimination. Members of the University community, applicants for employment or admissions, guests, and visitors have the right to be free from harassment or discrimination based on race, color, religion, ancestry, gender, sex, pregnancy, sexual orientation, gender identity or expression, age, disability, genetic information, national origin, veteran status, or any other status protected by applicable law.\nStudents who believe they have been subject to harassment or discrimination based on any of the above class of characteristics, or experience sexual harassment, sexual misconduct or gender discrimination should contact Elizabeth M. Garcia, Title IX Coordinator, (570) 941-6645 elizabeth.garcia2@scranton.edu, Deputy Title IX Coordinators Diana M. Collins (570) 941-6645 diana.collins@scranton.edu, or Ms. Lauren Rivera, AVP for Student Life and Dean of Students, at (570)941-7680 lauren.rivera@scranton.edu. The United States Department of Education’s Office for Civil Rights (OCR) enforces Title IX. Information regarding OCR may be found at &lt;www.ed.gov/about/offices/list/ocr/index.html&gt;\nThe University of Scranton Sexual Harassment and Sexual Misconduct Policy can be found online at https://www.scranton.edu/diversity. All reporting options and resources are available at https://www.scranton.edu/CARE.\n\nAbout Pronouns\nIt is easy to make assumptions about an individual’s pronouns, but we try not to! Please tell us in class or via a private email if you would like to let us know what your pronouns are, if/when you would like us (and others) to use them, and certainly feel free to correct us or others if we make a mistake. Using the pronouns that a person has indicated they prefer is considered both professional and polite, and as such we ask that all members of our class use the appropriate pronouns.\nIf you have questions about this, please feel free to look up more information here (https://www.mypronouns.org/) or email jason.graham@scranton.edu with any questions."
  },
  {
    "objectID": "syllabus.html#student-mental-health-suggestions-and-resources",
    "href": "syllabus.html#student-mental-health-suggestions-and-resources",
    "title": "Syllabus",
    "section": "Student Mental Health: Suggestions and Resources",
    "text": "Student Mental Health: Suggestions and Resources\nMany students experience mental health challenges at some point in college. Struggles vary and might be related to academics, anxiety, depression, relationships, grief/loss, substance abuse, and other challenges. There are resources to help you and getting help is the smart and courageous thing to do.\n\nCounseling Center (6th Floor O’Hara Hall; 570-941-7620) – Free, confidential individual and group counseling is available on campus.\nTeletherapy – For students who wish to access therapy via video, phone, and/or chat, the University offers a teletherapy resource. Please contact the Counseling Center (570-941-7620) to inquire about teletherapy.\nMental Health Screenings – Confidential, online “check up from your neck up” to help you determine if you should connect with a mental health professional.\nDean of Students Office (201 DeNaples Center; 570-941-7680) – Private support and guidance for students navigating personal challenges that may impact success at the University"
  },
  {
    "objectID": "syllabus.html#final-note",
    "href": "syllabus.html#final-note",
    "title": "Syllabus",
    "section": "Final Note",
    "text": "Final Note\nThe instructor reserve the right to modify this syllabus; students will immediately be notified of any such changes and an updated syllabus will be made available to the class via the course learning management system."
  },
  {
    "objectID": "lesson01/index.html#preparation-for-the-next-lesson",
    "href": "lesson01/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 1",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead section 2.1 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read sections 2.1 and 2.2 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on regression. View on YouTube."
  },
  {
    "objectID": "syllabus.html#use-of-ai",
    "href": "syllabus.html#use-of-ai",
    "title": "Syllabus",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial intelligence (AI) can be an effective tool in data science. For example, AI-based programming assistants like GitHub Copilot or generative model platforms like ChatGPT now help programmers and developers to write better code in less time. Learning to use AI is essentially becoming a basic skill for the modern data scientist. Because of this, I do not want to completely discourage the use of AI assistance.\nHowever, I ask that you avoid using AI platforms or tools in a manner that is inappropriate in the context of this course. This course teaches a variety of concepts, skills, and critical thinking. Using AI in such a way as to avoid learning, developing skills, or critical thinking is not appropriate. If you find yourself using AI to look up answers, search for complete solutions to problems, or things like this, then your use of AI is not acceptable. It might be helpful to think of AI as an analog to a calculator. If the goal of an assignment is for you to demonstrate that you can do a certain calculation, then using a calculator is not appropriate. On the other hand, if the goal of an assignment is for you to demonstrate that you can solve a problem for which a minor step involves doing a calculation, then using a calculator is okay. AI should be treated analogously.\nIn particular, it is expected that students will be able to explain independently and in detail what any line of code submitted as part of an assignment this semester does. Also, it is expected that students can explain independently and in detail the solution to any problem submitted as part of an assignment this semester.\nIf you have any doubts about your use of AI, then either ask the instructor if your use of AI is acceptable or just don’t use AI."
  },
  {
    "objectID": "lesson01/index.html#course-overview",
    "href": "lesson01/index.html#course-overview",
    "title": "Lesson 1",
    "section": "Course Overview",
    "text": "Course Overview\nThis course provides coverage of essential topics in data science at the intermediate level with an emphasis on machine learning. Broadly, we will cover algorithms that are commonly used for gaining insight from data. The things you learn in the class will be applicable in a variety of different areas, professions, and even other classes.\nThere is a website for the course, view the website. For course logistics, see the official course syllabus, view the syllabus. Assignments and other information specific to the course in a given semester will be posted on the course learning management system (LMS). The course website provides links to many additional resources, view the links.\nWhile we will refer often to several texts (most of which have been published online as open access materials) throughout the course, much of the content will be delivered via “notebooks” like the one you’re reading now1 that intermix text, mathematical notation, figures, programming language code, and web links. In some cases, you will be asked to go through the notebooks on your own and sometimes we will go through the notebooks together. Either way, any time you encounter code in a notebook, it is expected that you will take the time to run any code (mostly by copying and pasting) for yourself. The only way to master the material is through active participation.\nFor this course, we assume that students have some prior experience in using a programming language like R or Python to analyze data. In particular, it is assumed that students in the course can load, clean and plot data in R or Python. A facility in working with data frames via an R package like dplyr or the pandas Python library is assumed. We also assume students can use ggplot2 in R or matplotlib in Python for making appropriate plots of data. A great resource for the assumed programming background is R for Data Science (Wickham and Grolemund 2016). View the online version of R for Data Science. You can also review material from the prerequisite course DS 201 Introduction to Data Science, view the DS 201 course website.\nDuring the course, our preference tends toward using R for examples and application. Coding will be essential for assignments and it is recommended that students use R on coding assignments. However, students may request to use another language such as Python, requests will be granted or denied by the instructor on a case-by-case basis with a rationale for the decision provided.\n\nWhy R?\nThe R language (R Core Team 2023) for statistical computing is one of the most popular computing tools for data science, among the other most popular are Python and Julia. Some of the strengths of R include\n\nfree and open source which facilitates reproducibility and auditability,\necosystem of packages that greatly extend the functionality of R,\nrmarkdown (Xie, Dervieux, and Riederer 2020) and Quarto frameworks for literate programming enable presentation and communication of data analyses and facilitate reproducibility and auditability,\nRStudio integrated development environment (IDE) by Posit enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages,\na strong and collaborative user community, see R Community Explorer website, view the website..\n\n\n\nAn   refresher\nThis section reviews some coding in R with which the student is assumed to have some familiarity such as from the prerequisite course DS 201 Introduction to Data Science.\n\n\n\nArtwork by Allison Horst\n\n\nThroughout the course, we will make use of many R packages. For example, we will use the tidyverse package (which is a meta package that contains packages such as dplyr and ggplot2), the tidytuesdayR package for importing data from the Tidy Tuesday data repository (Mock 2018), and the ISLR2 package corresponding to the textbook An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). Packages contain data, functions, etc. An R package must be installed before it can be loaded and used. You only need to install a package once, but you have to load packages at each new R session.\nThere are two common ways to install packages:\n\nIf using the RStudio IDE (highly recommended), use the Install button on the Packages tab. All you have to do then is to search for the package you want to install and click the Install option. Note that this is most useful for R packages available through the Comprehensive R Archive Network (CRAN).\nWith the install.packages function. For example,\n\n\n# install packages with install.packages function\ninstall.packages(c(\"tidyverse\",\"tidytuesdayR\",\"ISLR2\"))\n\nIf you want to install a package from a repository other than CRAN, refer to the documentation, do a web search, or ask the instructor.\nTo load packages, use\n\n# load packages \nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(ISLR2)\n\nNow, functions, data, etc. from these packages will be available for reference by name 2. For example, we can access the documentation for the tt_load function from tidytuesdayR package by running the command\n\n?tt_load\n\nThis informs us that to load data from the TidyTuesday Github, we need to input a character corresponding to the date for that particular data set. For example,\n\ntt_data_examp &lt;- tt_load(\"2023-04-11\")\n\ndownloads two data files, egg-production.csv and cage-free-percentages.csv from the April 11, 2023 Tidy Tuesday post. We can access the egg production data as a data frame as follows:\n\negg_prod_df &lt;- tt_data_examp$`egg-production`\n\nThe following line of code is not run but demonstrates how to load a data file such as a comma-separated values file (CSV)\n\negg_prod_df &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\n\nThe previous code uses the read_csv function from the readr package (part of tidyverse) to directly read in the egg-production.csv from the specified location. An excellent review on reading data into R is provided in Chapter 2 of (Timbers, Campbell, and Lee 2022). View the chapter online.\nThe first few rows of this data set look as follows:\n\n\n\n\n\nobserved_month\nprod_type\nprod_process\nn_hens\nn_eggs\nsource\n\n\n\n\n2016-07-31\nhatching eggs\nall\n57975000\n1147000000\nChicEggs-09-23-2016.pdf\n\n\n2016-08-31\nhatching eggs\nall\n57595000\n1142700000\nChicEggs-10-21-2016.pdf\n\n\n2016-09-30\nhatching eggs\nall\n57161000\n1093300000\nChicEggs-11-22-2016.pdf\n\n\n2016-10-31\nhatching eggs\nall\n56857000\n1126700000\nChicEggs-12-23-2016.pdf\n\n\n2016-11-30\nhatching eggs\nall\n57116000\n1096600000\nChicEggs-01-24-2017.pdf\n\n\n2016-12-31\nhatching eggs\nall\n57750000\n1132900000\nChicEggs-02-28-2017.pdf\n\n\n\n\n\n\n\nLet’s use the glimpse function to get a quick sense of what’s in this data:\n\nglimpse(egg_prod_df)\n\nRows: 220\nColumns: 6\n$ observed_month &lt;date&gt; 2016-07-31, 2016-08-31, 2016-09-30, 2016-10-31, 2016-1…\n$ prod_type      &lt;chr&gt; \"hatching eggs\", \"hatching eggs\", \"hatching eggs\", \"hat…\n$ prod_process   &lt;chr&gt; \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\",…\n$ n_hens         &lt;dbl&gt; 57975000, 57595000, 57161000, 56857000, 57116000, 57750…\n$ n_eggs         &lt;dbl&gt; 1147000000, 1142700000, 1093300000, 1126700000, 1096600…\n$ source         &lt;chr&gt; \"ChicEggs-09-23-2016.pdf\", \"ChicEggs-10-21-2016.pdf\", \"…\n\n\nQuestion: What are some initial questions we might want to address using the egg production data? Read through the repository for the egg production data view repository. Further, see the report summary by The Human League project. What do the researchers report based on an analysis of the data?\nExercise: Examine the data downloaded as cage-free-percentages.csv. What information does it contain?\nThe following R code essentially reproduces Figure 2 from the egg production report by The Human League, see the report:\n\n\nCode\ncage_free_df %&gt;%\n  mutate(year=lubridate::year(observed_month)) %&gt;%\n  group_by(year) %&gt;%\n  summarise(percent_hens=mean(percent_hens)) %&gt;%\n  ggplot(aes(x=year,y=percent_hens)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(breaks=2007:2021) + \n  scale_y_continuous(breaks=seq(0,30,by=5),labels=paste0(as.character(seq(0,30,by=5)),\"%\")) + \n  labs(x=\"Date (year)\",y=\"Percentage of US hens in cage-free housing\")\n\n\n\n\n\nFigure 1: Reproducing Figure 2 from The Human League report on egg production.\n\n\n\n\nQuestion: What does the plot in Figure 1 show?\nExercise: Carefully examine the code used to make Figure 1. What does each line of the code do? Note that these are the types of basic data manipulations and plots you should be comfortable making entering into DS 362.\nExercise: Look at the list of data sets contained in the ISLR2 package by running the command data(package=\"ISLR2\"). Pick two of the data sets and determine what information is contained in your chosen data sets. Make a couple of basic plots using one of the data sets you chose.\n\n\nReview of the Data Science Workflow\nBefore starting our study of data mining and machine learning, let’s review the basic data science workflow as covered in an introductory level data science course. Generally, the steps in the workflow are:\n\nGather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application. If you’re going to use machine learning as part of your data analysis, this is a good place to make sure that you have enough data and also to split your data into a training set and test set.\nDocument the data and data collection process.\n\nIt is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See The Alignment Problem by Brian Christian for a thoughtful discussion on the issues related to these types of considerations (Christian 2020).\n\nImport the data for analysis. If you have not already, you may want to split your data into a training set and test set.\nExplore, clean, and transform the data. Data visualization is essential at this step. Make sure to apply the same manipulations used for cleaning and transformation to both the training and test sets.\nGenerate initial insight or more detailed questions.\n\nSteps 4-5 constitute an exploratory data analysis (EDA). It is typical to conduct EDA before doing any more sophisticated analyses such as machine learning, and this is a habit you should get into. Students are assumed to be familiar with the EDA process from DS 201. If you need a refresher, the following video is recommended:\nExploratory data analysis worked example video by Hadley Wickham, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\n\nDecide what type(s) of analysis or analyses are to be performed. This is the stage at which machine learning enters into the process.\n\nMake sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See Table 1.1 from (Timbers, Campbell, and Lee 2022) for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table here.\n\nAssess the analysis. In particular, use an appropriate metric to estimate model error.\nAt this stage, it may be necessary to repeat steps 1 - 7.\nReport your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis.\n\nIt is essential the our data science workflow be reproducible and auditable. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility). Figure 2 illustrates the concept of reproducibility.\n\n\n\nFigure 2: Artwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#footnotes",
    "href": "lesson01/index.html#footnotes",
    "title": "Lesson 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe notebooks are created using Quarto and R markdown, topics that we will cover in more detail later.↩︎\nOccasionally, there will be a conflict between functions from two different packages. R will issue a warning when such conflicts arise. You can also use the “double colon” notation to reference an object in a package without the need to load the package. For example, dplyr::select references the select function from the dplyr package.↩︎\nIf one or more of our predictor variables is categorical, then we will use a technique known as dummy variables to convert each categorical predictor variable into a sequence of numerical variables. This will add to the number of columns in our matrix representation of our predictors \\(X\\). We will cover the concept of dummy variables later in the course.↩︎"
  },
  {
    "objectID": "lesson01/index.html#review-of-the-data-science-workflow",
    "href": "lesson01/index.html#review-of-the-data-science-workflow",
    "title": "Lesson 1",
    "section": "Review of the Data Science Workflow",
    "text": "Review of the Data Science Workflow\nBefore starting our exploration of machine learning, let’s review the basic data science workflow as covered in an introductory level data science course. Generally, the steps in the workflow are:\n\nGather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application. If you’re going to use machine learning as part of your data analysis, this is a good place to make sure that you have enough data and also to split your data into a training set and test set.\nDocument the data and data collection process.\n\nIt is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See The Alignment Problem by Brian Christian for a thoughtful discussion on the issues related to these types of considerations (Christian 2020).\n\nImport the data for analysis. If you have not already, you may want to split your data into a training set and test set.\nExplore and clean the data. Data visualization is essential at this step.\nGenerate initial insight or more detailed questions.\nDecide what type(s) of analysis or analyses are to be performed. This is the stage at which machine learning enters into the process.\n\nMake sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See Table 1.1 from (timbers2022data?) for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table here.\n\nAssess the analysis. In particular, use an appropriate metric to estimate model error.\nAt this stage, it may be necessary to repeat steps 1 - 7.\nReport your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis.\n\nIt is essential the our data science workflow be reproducible and auditable. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility). Figure 1 illustrates the concept of reproducibility.\n\n\n\nFigure 1: Artwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#an-refresher",
    "href": "lesson01/index.html#an-refresher",
    "title": "Lesson 1",
    "section": "An   refresher",
    "text": "An   refresher\n\n\n\nArtwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#overview-of-data-mining-and-machine-learning",
    "href": "lesson01/index.html#overview-of-data-mining-and-machine-learning",
    "title": "Lesson 1",
    "section": "Overview of data mining and machine learning",
    "text": "Overview of data mining and machine learning\nIn this course, we view data mining as processes that can be implemented as algorithms that can be used to gain useful insights from data. Machine learning, also known as statistical learning uses mathematical models to learn from data. The methods of machine learning provide one approach to data mining, an approach that is currently very popular. Machine learning is also a currently prominent approach to developing artificial intelligence technologies. Of course, what one means by a “useful insight” is highly dependent on the domain of specialization or area of application. Thus, data mining is an inherently interdisciplinary field that intersects with many disciplines such as computer science, data science, mathematics and statistics and a variety of other fields.\nOne can break the processes of data mining into a number of components including:\n\nData collection - This may involve automating the collection, storage, and maintenance of data. The data collection process should be well-documented.\nFeature extraction and data cleaning - Real-world data is messy and will rarely be immediately suitable for any meaningful analysis.\nModeling and analysis - Where an attempt is made to gain useful insight from data. This step should include a critical assessment, often via a quantitative assessment of results from modeling and analysis.\n\nAs we proceed through the course, we will touch on each of components 1 - 3 in more detail. For the remainder of this lesson, let’s look at some questions about specific data to motivate our study of machine learning.\n\nA Motivating Example\nThe Tidy Tuesday entry for January 25, 2022 contains data related to ratings of board games. This data consists of two csv files that contain various information or features on some specific games. View the data repository. An obvious question that board game creators or players might be interested in is, what factors contribute to the rating of a board game?\nThe first thing we need to do is load the data. In the following code, we download the two csv files and combine them into one common data set:\n\n# read in data csv files\nratings &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv\")\ndetails &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv\")\n\n# combine into one data frame\nratings_joined &lt;-\n  ratings %&gt;%\n  left_join(details, by = \"id\")\n\n# glimpse the data\nglimpse(ratings_joined)\n\nRows: 21,831\nColumns: 32\n$ num.x                   &lt;dbl&gt; 105, 189, 428, 72, 103, 191, 100, 3, 15, 35, 3…\n$ id                      &lt;dbl&gt; 30549, 822, 13, 68448, 36218, 9209, 178900, 16…\n$ name                    &lt;chr&gt; \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders…\n$ year                    &lt;dbl&gt; 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016…\n$ rank                    &lt;dbl&gt; 106, 190, 429, 73, 104, 192, 101, 4, 16, 36, 3…\n$ average                 &lt;dbl&gt; 7.59, 7.42, 7.14, 7.74, 7.61, 7.41, 7.60, 8.42…\n$ bayes_average           &lt;dbl&gt; 7.487, 7.309, 6.970, 7.634, 7.499, 7.305, 7.50…\n$ users_rated             &lt;dbl&gt; 108975, 108738, 108024, 89982, 81561, 76171, 7…\n$ url                     &lt;chr&gt; \"/boardgame/30549/pandemic\", \"/boardgame/822/c…\n$ thumbnail               &lt;chr&gt; \"https://cf.geekdo-images.com/S3ybV1LAp-8SnHIX…\n$ num.y                   &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n$ primary                 &lt;chr&gt; \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders…\n$ description             &lt;chr&gt; \"In Pandemic, several virulent diseases have b…\n$ yearpublished           &lt;dbl&gt; 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016…\n$ minplayers              &lt;dbl&gt; 2, 2, 3, 2, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2, 2, 2…\n$ maxplayers              &lt;dbl&gt; 4, 5, 4, 7, 4, 5, 8, 5, 2, 5, 5, 4, 5, 5, 5, 4…\n$ playingtime             &lt;dbl&gt; 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150…\n$ minplaytime             &lt;dbl&gt; 45, 30, 60, 30, 30, 30, 15, 120, 30, 30, 90, 3…\n$ maxplaytime             &lt;dbl&gt; 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150…\n$ minage                  &lt;dbl&gt; 8, 7, 10, 10, 13, 8, 14, 12, 10, 12, 12, 10, 1…\n$ boardgamecategory       &lt;chr&gt; \"['Medical']\", \"['City Building', 'Medieval', …\n$ boardgamemechanic       &lt;chr&gt; \"['Action Points', 'Cooperative Game', 'Hand M…\n$ boardgamefamily         &lt;chr&gt; \"['Components: Map (Global Scale)', 'Component…\n$ boardgameexpansion      &lt;chr&gt; \"['Pandemic: Gen Con 2016 Promos – Z-Force Tea…\n$ boardgameimplementation &lt;chr&gt; \"['Pandemic Legacy: Season 0', 'Pandemic Legac…\n$ boardgamedesigner       &lt;chr&gt; \"['Matt Leacock']\", \"['Klaus-Jürgen Wrede']\", …\n$ boardgameartist         &lt;chr&gt; \"['Josh Cappel', 'Christian Hanisch', 'Régis M…\n$ boardgamepublisher      &lt;chr&gt; \"['Z-Man Games', 'Albi', 'Asmodee', 'Asmodee I…\n$ owned                   &lt;dbl&gt; 168364, 161299, 167733, 120466, 106956, 105748…\n$ trading                 &lt;dbl&gt; 2508, 1716, 2018, 1567, 2009, 930, 1110, 538, …\n$ wanting                 &lt;dbl&gt; 625, 582, 485, 1010, 655, 692, 340, 2011, 924,…\n$ wishing                 &lt;dbl&gt; 9344, 7383, 5890, 12105, 8621, 6620, 5764, 192…\n\n\nThere is a lot of information in this data. For a more specific question, suppose we want to know if/how the number of players, game length, and age recommendations for a game impact the average rating of a game. Let’s select those features of primary interest:\n\n# select relevant features\nratings_relevant &lt;- ratings_joined %&gt;%\n  select(average,minplayers,maxplayers,playingtime,minage)\n\nLet’s see the first few rows of this data:\n\n\n\n\n\naverage\nminplayers\nmaxplayers\nplayingtime\nminage\n\n\n\n\n7.59\n2\n4\n45\n8\n\n\n7.42\n2\n5\n45\n7\n\n\n7.14\n3\n4\n120\n10\n\n\n7.74\n2\n7\n30\n10\n\n\n7.61\n2\n4\n30\n13\n\n\n7.41\n2\n5\n60\n8\n\n\n\n\n\n\n\nThis problem involves predicting a numerical response variable, that is, average based on the value of a number of predictor variables. This is a type of supervised learning problem known as regression. Since the response variable is numerical, even if we include some categorical predictor variables, we still have a regression problem.\nThe data scientist Julia Silge analyzes the board game data in a blog from post from January 28, 2022. See the blog post. Throughout the course, we will refer to a number of Silge’s posts and data analyses as they provide excellent worked examples of data mining and machine learning.\n\n\nAnother Example\nNow, consider the Palmer penguins data set available through the palmerpenguins package. This data contains information on penguins from the Palmer Station, Antarctica.\nWe glimpse the data\n\n# load the library (don't forget to install if necessary)\nlibrary(palmerpenguins)\n# glimpse data\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nand examine the first few rows of the data\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\nOne question we may be interested in is, can the physical measurements of the penguins be used to distinguish the animals in terms of their species? That is, can the physical measurements be used to predict the species of a penguin without having to, say conduct a genetic analysis? Here, our response variable is categorical. This is another type of supervised learning problem know as classification  Whenever we have a categorical response, we have a classification problem.\nFor both the board game data and the penguins data, there is a response variable that we are interested to predict based on the values of certain predictor variables. Further, the data can be viewed as paired where each set of predictor values (which we denote generically by \\(X\\)) has a corresponding known response value (denoted \\(y\\)). This data provides a set of examples that can be used to learn the relationship between the predictors \\(X\\) and the response \\(y\\). This is why regression and classification problems are called supervised learning problems.\n\nSome Mathematical Notation\nIt is convenient to organize our data by using the mathematical notation of vectors and matrices, especially for machine learning problems. Then, many machine learning algorithms can be derived or implemented via the powerful methods of numerical linear algebra. Suppose that we have paired data consisting of \\(n\\) observations (think rows) and \\(p\\) numerical predictor variables 3. Then our predictor values \\(X\\) can be organized as an \\(n \\times p\\) matrix such as\n\\[\nX = \\left[\\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\cdots &  \\ddots & \\vdots  \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{array}\\right]\n\\]\nWe then view the response variable \\(y\\) as a vector of length \\(p\\), that is,\n\\[\ny = \\left[\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{p}\n\\end{array}\\right]\n\\]\nEach row of the matrix representation for \\(X\\) corresponds to an observation and, in case of supervised learning, is associated with a single entry of \\(y\\). For example, row \\(j\\) of \\(X\\), that is \\([\\begin{array}{cccc} x_{j1} & x_{j2} & \\cdots & x_{jp} \\end{array}]\\), is labelled by \\(y_{j}\\).\nOur next step in the course will be to study some common supervised learning algorithms. There are also situations in which a data set does not consist of a natural pairing between predictors and a response. Unpaired or unlabeled data can be analysed using unsupervised learning methods. We will address unsupervised learning problems and algorithms later in the course."
  },
  {
    "objectID": "syllabus.html#homework-assignments",
    "href": "syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\nThere will be roughly 12 weekly homework assignments throughout the semester totaling 30% of the overall course grade. These assignments with due dates will be posted to the course learning management system. Homework problems will be a mix of hand-written and computer assignments and the problems will relate to the material covered in lectures and readings."
  },
  {
    "objectID": "syllabus.html#data-package-assignment",
    "href": "syllabus.html#data-package-assignment",
    "title": "Syllabus",
    "section": "Data Package Assignment",
    "text": "Data Package Assignment"
  },
  {
    "objectID": "syllabus.html#semester-project",
    "href": "syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\nThe semester project will incorporate all the components of a data analysis covered in the course throughout the semester applied to a particular data set. Various components of the project will be due at different times but you will have the opportunity to revise some components prior to the submission of the final product.\nA complete project, counting for 30% of the overall course grade will consist of the following:\n\nAn initial exploratory data analysis for your data set.\nAn appropriate analysis of your data set with the goal to address a specific research question.\nA project report developed using Quarto.\nSlides for a presentation summarizing your project. You will not actually present the slides.\nA GitHub repository containing all code (appropriately documented) written and used in your project.\n\nYour project report and presentation should be written as if it is addressed to a stake holder with some subject matter knowledge in the domain of application but not necessarily with a quantitative or programming background. Further details on the semester project such as the parameters of the assignment and a grade rubric will be posted on the course learning management system."
  },
  {
    "objectID": "syllabus.html#data-modeling-case-studies",
    "href": "syllabus.html#data-modeling-case-studies",
    "title": "Syllabus",
    "section": "Data Modeling Case Studies",
    "text": "Data Modeling Case Studies\nOver the course of the semester, you will be asked to complete two data modeling case studies that will form a total of 40% of the overall course grade. For each case study, you will be provided with a data set and then asked to apply and compare multiple methods of analysis on the provided data set. A complete data modeling case study should consist of\n\nAppropriately documented code implementing appropriate models.\nA GitHub repository with all code.\nA report developed in Quarto that summarizes your conclusions.\n\nFurther details on the case study assignments such as the parameters of the assignments and grade rubrics will be posted on the course learning management system."
  }
]