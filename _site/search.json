[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS 362: Data-Driven Knowledge Discovery",
    "section": "",
    "text": "Welcome to the course website for DS 362: Data-Driven Knowledge Discovery for Fall 2023.\nThis course is offered at the University of Scranton as taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "DS 362: Data-Driven Knowledge Discovery",
    "section": "License",
    "text": "License\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html#required-readings",
    "href": "syllabus.html#required-readings",
    "title": "Syllabus",
    "section": "Required Readings",
    "text": "Required Readings\n\nAn Introduction to Statistical Learning 2nd Ed. by James, Witten, Hastie, and Tibshirani, view the free online version of the text. There is a set of lecture videos associated with this text, view these videos on YouTube. There is also a version of the text using Python instead of R, view the Python version.\nStatistical Learning with Math and R by Joe Suzuki, available online through the Weinberg Library, view the text. There is also a version of the text using Python instead of R, view the Python version"
  },
  {
    "objectID": "syllabus.html#r-references",
    "href": "syllabus.html#r-references",
    "title": "Syllabus",
    "section": "R References",
    "text": "R References\n\nHands-On Programming with R by Garrett Grolemund, view the free online version of the text.\nR for Data Science by Hadley Wickham & Garrett Grolemund, view the free online version of the text.\nData Visualization A Practical Introduction by Healy, view the free online version of the text."
  },
  {
    "objectID": "syllabus.html#additional-readings",
    "href": "syllabus.html#additional-readings",
    "title": "Syllabus",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nElements of Statistical Learning 2nd Ed. by Hastie, Tibshirani, and Friedman, freely available online here.\nStatistical Learning from a Regression Perspective by Richard A. Berk, available online through the Weinberg Library here."
  },
  {
    "objectID": "syllabus.html#recommended-readings",
    "href": "syllabus.html#recommended-readings",
    "title": "Syllabus",
    "section": "Recommended Readings",
    "text": "Recommended Readings\n\nHands-On Machine Learning with R, view the free online version of the text.\nTidy Modeling with R by Max Kuhn & Julia Silge, view the free online version of the text.\nFeature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson, view the free online version of the text.\nTelling Stories with Data by Rohan Alexander, view the free online version of the text.\nData Science: A First Introduction by Tiffany Timbers, Trevor Campbell, and Melissa Lee, view the free online version of the text.\nTree-Based Methods for Statistical Learning by Brandon M. Greenwell, view the free online version of the text."
  },
  {
    "objectID": "syllabus.html#further-reading",
    "href": "syllabus.html#further-reading",
    "title": "Syllabus",
    "section": "Further Reading",
    "text": "Further Reading\n\nElements of Statistical Learning 2nd Ed. by Hastie, Tibshirani, and Friedman, view the free online version of the text.\nLinks to additional resources related to the course material will be posted on the course website. View the resources link."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "links.html#texts",
    "href": "links.html#texts",
    "title": "Links",
    "section": "Texts",
    "text": "Texts\n\nR Related\n\nDeep Learning and Scientific Computing with R torch, view the free online version of the text.\nggplot2: Elegant Graphics for Data Analysis (3e), view the free online version of the text\n\n\n\nPython Related\n\nPython for Data Analysis, 3E, view the free online version of the text"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "DS 362: Data-Driven Knowledge Discovery",
    "section": "",
    "text": "Welcome to the course website for DS 362: Data-Driven Knowledge Discovery for Fall 2023.\nThis course is offered at the University of Scranton as taught by Mathematics Department faculty member Jason M. Graham during the Fall 2023 semester."
  },
  {
    "objectID": "links.html#websites",
    "href": "links.html#websites",
    "title": "Links",
    "section": "",
    "text": "Getting Started with R and RStudio, view the website.\nHow to Use Git/GitHub with R, view the website.\nTidy Tuesday data repository, view the website.\nFrom Data to Viz website on visualizing data, view the website.\nJulia Silge’s blog posts (mostly) on TidyTuesday data analyses, view the website."
  },
  {
    "objectID": "lesson01/index.html",
    "href": "lesson01/index.html",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nRecall how to load, manipulate, and plot data in R.\nRecall the basic data science workflow.\nState in general terms what we mean by data mining and machine learning, also known as statistical learning.\nRecognize where data mining and machine learning fit into the basic data science workflow."
  },
  {
    "objectID": "lesson01/index.html#learning-objectives",
    "href": "lesson01/index.html#learning-objectives",
    "title": "Lesson 1",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nRecall how to load, manipulate, and plot data in R.\nRecall the basic data science workflow.\nState in general terms what we mean by data mining and machine learning, also known as statistical learning.\nRecognize where data mining and machine learning fit into the basic data science workflow."
  },
  {
    "objectID": "lesson01/index.html#readings-etc.",
    "href": "lesson01/index.html#readings-etc.",
    "title": "Lesson 1",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapter 1 of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nSkim the README for the Tidy Tuesday data repository (Mock 2018). View the repository. Throughout the semester, we will use example data from the Tidy Tuesday data repository."
  },
  {
    "objectID": "lesson01/index.html#references",
    "href": "lesson01/index.html#references",
    "title": "Lesson 1",
    "section": "References",
    "text": "References\n\n\nChristian, Brian. 2020. The Alignment Problem: Machine Learning and Human Values. WW Norton & Company.\n\n\nMock, J. Thomas. 2018. “Tidy Tuesday.” GitHub Repository. https://github.com/rfordatascience/tidytuesday; GitHub.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. CRC Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-09\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n dplyr          * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2        * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n ISLR2          * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n palmerpenguins * 0.1.1   2022-08-15 [1] CRAN (R 4.3.0)\n purrr          * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo    * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR   * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "links.html#videos",
    "href": "links.html#videos",
    "title": "Links",
    "section": "Videos",
    "text": "Videos\n\nGetting started with Quarto YouTube video, watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nDatasheets for Datasets video, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nExploratory data analysis worked example video by Hadley Wickham, watch the video on YouTube."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nThis course covers the process of knowledge discovery including data selection, pre-processing, transformation, data mining, evaluation, and validation, with an emphasis on data mining concepts, algorithms, and techniques for common tasks such as association rule learning, classification, regression, clustering, and outlier detection.\n\n\nPrerequisites\n\nCMPS 240 and DS 201 and DS 210"
  },
  {
    "objectID": "syllabus.html#student-learning-objectives-and-assessment",
    "href": "syllabus.html#student-learning-objectives-and-assessment",
    "title": "Syllabus",
    "section": "Student Learning Objectives and Assessment:",
    "text": "Student Learning Objectives and Assessment:\n\n\n\n\nTable 1: Course objectives and assessment.\n\n\nCourse SLO\nAssessment\n\n\n\n\nAfter completing this course, students will be able to import, pre-process, and transform common data sets as appropriate for use in machine learning applications.\nHomework, Case Studies, and Project\n\n\nAfter completing this course, students will be able to implement and apply foundational machine learning methods.\nHomework, Case Studies, and Project\n\n\nAfter completing this course, students will be able to present and communicate results obtained via data analysis in an effective manner.\nCase Studies and Project"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\nGrade Policy\nThe overall course grade will be based on (roughly twelve) weekly homework assignment totaling 30% of the overall course grade, two data modeling case study assignments totaling 40% of the overall course grade, and a semester project totaling 30% of the overall course grade.\n\n\nGrade Scale\nLetter grades will be assigned based on the following scale:\n\n\n\n\nTable 2: Letter grade scale.\n\n\nGrade Range\nLetter Grade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n83-86\nB\n\n\n80-82\nB-\n\n\n76-79\nC+\n\n\n72-75\nC\n\n\n69-71\nC-\n\n\n65-68\nD+\n\n\n60-64\nD\n\n\n&lt;60\nF"
  },
  {
    "objectID": "syllabus.html#weekly-schedule",
    "href": "syllabus.html#weekly-schedule",
    "title": "Syllabus",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\nWeek 1: Review of background concepts\nWeek 2: Data mining\nWeek 3: Machine learning\nWeek 4: Regression\nWeek 5: Classification; Project component 1 due\nWeek 6: Resampling methods\nWeek 7: Model selection and regularization; Case Study 1 due\nWeek 8: CARTs\nWeek 9: Support vector machines\nWeek 10: Neural Networks\nWeek 11: Deep learning; Project component 2 due\nWeek 12: Association rule learning\nWeek 13: Unsupervised methods; Case Study 2 due\nWeek 14: Reinforcement\nWeek 15: Ethical considerations; Project final version due"
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important Dates",
    "text": "Important Dates\n\n\n\n\nTable 3: Important dates.\n\n\nEvent\nDate\n\n\n\n\nClasses begin\n08-28\n\n\nLast day to add classes\n90-01\n\n\nHoliday, no classes\n09-04\n\n\n100% tuition refund\n09-06\n\n\nDrop (no grade)\n09-27\n\n\nFall break\n10-07 to 10-10\n\n\nMid-semester\n10-18\n\n\nWithdraw with W\n11-10\n\n\nThanksgiving break\n11-22 to 11-26\n\n\nLast week\n12-05 to 12-11\n\n\nFinals\n12-12 to 12-16"
  },
  {
    "objectID": "syllabus.html#students-with-disabilities",
    "href": "syllabus.html#students-with-disabilities",
    "title": "Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nReasonable academic accommodations may be provided to students who submit relevant and current documentation of their disability. Students are encouraged to contact the Center for Teaching and Learning Excellence (CTLE) at disabilityservices@scranton.edu or (570) 941-4038 if they have or think they may have a disability and wish to determine eligibility for any accommodations. For more information, please visit http://www.scranton.edu/disabilities."
  },
  {
    "objectID": "syllabus.html#writing-center-services",
    "href": "syllabus.html#writing-center-services",
    "title": "Syllabus",
    "section": "Writing Center Services",
    "text": "Writing Center Services\nThe Writing Center focuses on helping students become better writers. Consultants will work one-on-one with students to discuss students’ work and provide feedback at any stage of the writing process. Scheduling appointments early in the writing progress is encouraged.\nTo meet with a writing consultant, call (570) 941-6147 to schedule an appointment, or send an email with your available meeting times, the course for which you need assistance, and your phone number to: writing-center@scranton.edu. The Writing Center does offer online appointments for our distance learning students."
  },
  {
    "objectID": "syllabus.html#academic-honesty-and-integrity",
    "href": "syllabus.html#academic-honesty-and-integrity",
    "title": "Syllabus",
    "section": "Academic Honesty and Integrity",
    "text": "Academic Honesty and Integrity\nEach student is expected to do their own work. It is also expected that each student respect and abide by the Academic Code of Honesty as set forth in the University of Scranton student handbook. Conduct that violates the Academic Code of Honesty includes plagiarism, duplicate submission of the same work, collusion, providing false information, unauthorized use of computers, theft and destruction of property, and unauthorized possession of tests and other materials. Steps taken in response to suspected violations may include a discussion with the instructor, an informal meeting with the dean of the college, and a hearing before the Academic Dishonesty Hearing Board. Students who are found to have violated the Code will ordinarily be assigned the grade F by the instructor and may face other sanctions. The complete Academic Code of Honesty is located on the University website at https://www.scranton.edu/academics/wml/acad-integ/acad-code-honesty.shtml."
  },
  {
    "objectID": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "href": "syllabus.html#my-reporting-obligation-as-a-responsible-employee",
    "title": "Syllabus",
    "section": "My Reporting Obligation as a Responsible Employee",
    "text": "My Reporting Obligation as a Responsible Employee\nAs a faculty member, I am deeply invested in the well-being of each student I teach. I am here to assist you with your work in this course. Additionally, if you come to me with other non-course-related concerns, I will do my best to help. It is important for you to know that all faculty members are required to report incidents of sexual harassment or sexual misconduct involving students. This means that I cannot keep information about sexual harassment, sexual assault, sexual exploitation, intimate partner violence or stalking confidential if you share that information with me. I will keep the information as private as I can but am required to bring it to the attention of the University’s Title IX Coordinator, Elizabeth M. Garcia, or Deputy Title IX Coordinator, Diana M. Collins, who, in conversation with you, will explain available support, resources, and options. I will not report anything to anybody without first letting you know and discussing choices as to how to proceed. The University’s Counseling Center (570-941-7620) is available to you as a confidential resource; counselors (in the counseling center) do not have an obligation to report to the Title IX Coordinator."
  },
  {
    "objectID": "syllabus.html#non-discrimination-statement",
    "href": "syllabus.html#non-discrimination-statement",
    "title": "Syllabus",
    "section": "Non-discrimination Statement",
    "text": "Non-discrimination Statement\nThe University is committed to providing an educational, residential, and working environment that is free from harassment and discrimination. Members of the University community, applicants for employment or admissions, guests, and visitors have the right to be free from harassment or discrimination based on race, color, religion, ancestry, gender, sex, pregnancy, sexual orientation, gender identity or expression, age, disability, genetic information, national origin, veteran status, or any other status protected by applicable law.\nStudents who believe they have been subject to harassment or discrimination based on any of the above class of characteristics, or experience sexual harassment, sexual misconduct or gender discrimination should contact Elizabeth M. Garcia, Title IX Coordinator, (570) 941-6645 elizabeth.garcia2@scranton.edu, Deputy Title IX Coordinators Diana M. Collins (570) 941-6645 diana.collins@scranton.edu, or Ms. Lauren Rivera, AVP for Student Life and Dean of Students, at (570)941-7680 lauren.rivera@scranton.edu. The United States Department of Education’s Office for Civil Rights (OCR) enforces Title IX. Information regarding OCR may be found at &lt;www.ed.gov/about/offices/list/ocr/index.html&gt;\nThe University of Scranton Sexual Harassment and Sexual Misconduct Policy can be found online at https://www.scranton.edu/diversity. All reporting options and resources are available at https://www.scranton.edu/CARE.\n\nAbout Pronouns\nIt is easy to make assumptions about an individual’s pronouns, but we try not to! Please tell us in class or via a private email if you would like to let us know what your pronouns are, if/when you would like us (and others) to use them, and certainly feel free to correct us or others if we make a mistake. Using the pronouns that a person has indicated they prefer is considered both professional and polite, and as such we ask that all members of our class use the appropriate pronouns.\nIf you have questions about this, please feel free to look up more information here (https://www.mypronouns.org/) or email jason.graham@scranton.edu with any questions."
  },
  {
    "objectID": "syllabus.html#student-mental-health-suggestions-and-resources",
    "href": "syllabus.html#student-mental-health-suggestions-and-resources",
    "title": "Syllabus",
    "section": "Student Mental Health: Suggestions and Resources",
    "text": "Student Mental Health: Suggestions and Resources\nMany students experience mental health challenges at some point in college. Struggles vary and might be related to academics, anxiety, depression, relationships, grief/loss, substance abuse, and other challenges. There are resources to help you and getting help is the smart and courageous thing to do.\n\nCounseling Center (6th Floor O’Hara Hall; 570-941-7620) – Free, confidential individual and group counseling is available on campus.\nTeletherapy – For students who wish to access therapy via video, phone, and/or chat, the University offers a teletherapy resource. Please contact the Counseling Center (570-941-7620) to inquire about teletherapy.\nMental Health Screenings – Confidential, online “check up from your neck up” to help you determine if you should connect with a mental health professional.\nDean of Students Office (201 DeNaples Center; 570-941-7680) – Private support and guidance for students navigating personal challenges that may impact success at the University"
  },
  {
    "objectID": "syllabus.html#final-note",
    "href": "syllabus.html#final-note",
    "title": "Syllabus",
    "section": "Final Note",
    "text": "Final Note\nThe instructor reserve the right to modify this syllabus; students will immediately be notified of any such changes and an updated syllabus will be made available to the class via the course learning management system."
  },
  {
    "objectID": "lesson01/index.html#preparation-for-the-next-lesson",
    "href": "lesson01/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 1",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead section 2.1 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nWatch the corresponding video lectures:\nMotivating problems for machine (statistical) learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSupervised and unsupervised learning. Watch video on YouTube."
  },
  {
    "objectID": "syllabus.html#use-of-ai",
    "href": "syllabus.html#use-of-ai",
    "title": "Syllabus",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial intelligence (AI) can be an effective tool in data science. For example, AI-based programming assistants like GitHub Copilot or generative model platforms like ChatGPT now help programmers and developers to write better code in less time. Learning to use AI is essentially becoming a basic skill for the modern data scientist. Because of this, I do not want to completely discourage the use of AI assistance.\nHowever, I ask that you avoid using AI platforms or tools in a manner that is inappropriate in the context of this course. This course teaches a variety of concepts, skills, and critical thinking. Using AI in such a way as to avoid learning, developing skills, or critical thinking is not appropriate. If you find yourself using AI to look up answers, search for complete solutions to problems, or things like this, then your use of AI is not acceptable. It might be helpful to think of AI as an analog to a calculator. If the goal of an assignment is for you to demonstrate that you can do a certain calculation, then using a calculator is not appropriate. On the other hand, if the goal of an assignment is for you to demonstrate that you can solve a problem for which a minor step involves doing a calculation, then using a calculator is okay. AI should be treated analogously.\nIn particular, it is expected that students will be able to explain independently and in detail what any line of code submitted as part of an assignment this semester does. Also, it is expected that students can explain independently and in detail the solution to any problem submitted as part of an assignment this semester.\nIf you have any doubts about your use of AI, then either ask the instructor if your use of AI is acceptable or just don’t use AI."
  },
  {
    "objectID": "lesson01/index.html#course-overview",
    "href": "lesson01/index.html#course-overview",
    "title": "Lesson 1",
    "section": "Course Overview",
    "text": "Course Overview\nThis course provides coverage of essential topics in data science at the intermediate level with an emphasis on machine learning. Broadly, we will cover algorithms that are commonly used for gaining insight from data. The things you learn in the class will be applicable in a variety of different areas, professions, and even other classes.\nThere is a website for the course, view the website. For course logistics, see the official course syllabus, view the syllabus. Assignments and other information specific to the course in a given semester will be posted on the course learning management system (LMS). The course website provides links to many additional resources, view the links.\nWhile we will refer often to several texts (most of which have been published online as open access materials) throughout the course, much of the content will be delivered via “notebooks” like the one you’re reading now1 that intermix text, mathematical notation, figures, programming language code, and web links. In some cases, you will be asked to go through the notebooks on your own and sometimes we will go through the notebooks together. Either way, any time you encounter code in a notebook, it is expected that you will take the time to run any code (mostly by copying and pasting) for yourself. The only way to master the material is through active participation.\nFor this course, we assume that students have some prior experience in using a programming language like R or Python to analyze data. In particular, it is assumed that students in the course can load, clean and plot data in R or Python. A facility in working with data frames via an R package like dplyr or the pandas Python library is assumed. We also assume students can use ggplot2 in R or matplotlib in Python for making appropriate plots of data. A great resource for the assumed programming background is R for Data Science (Wickham and Grolemund 2016). View the online version of R for Data Science. You can also review material from the prerequisite course DS 201 Introduction to Data Science, view the DS 201 course website.\nDuring the course, our preference tends toward using R for examples and application. Coding will be essential for assignments and it is recommended that students use R on coding assignments. However, students may request to use another language such as Python, requests will be granted or denied by the instructor on a case-by-case basis with a rationale for the decision provided.\n\nWhy R?\nThe R language (R Core Team 2023) for statistical computing is one of the most popular computing tools for data science, among the other most popular are Python and Julia. Some of the strengths of R include\n\nfree and open source which facilitates reproducibility and auditability,\necosystem of packages that greatly extend the functionality of R,\nrmarkdown (Xie, Dervieux, and Riederer 2020) and Quarto frameworks for literate programming enable presentation and communication of data analyses and facilitate reproducibility and auditability,\nRStudio integrated development environment (IDE) by Posit enhances programming, RStudio also supports other languages like Python and Julia making it possible to work across different languages,\na strong and collaborative user community, see R Community Explorer website, view the website..\n\n\n\nAn   refresher\nThis section reviews some coding in R with which the student is assumed to have some familiarity such as from the prerequisite course DS 201 Introduction to Data Science.\n\n\n\nArtwork by Allison Horst\n\n\nThroughout the course, we will make use of many R packages. For example, we will use the tidyverse package (which is a meta package that contains packages such as dplyr and ggplot2), the tidytuesdayR package for importing data from the Tidy Tuesday data repository (Mock 2018), and the ISLR2 package corresponding to the textbook An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). Packages contain data, functions, etc. An R package must be installed before it can be loaded and used. You only need to install a package once, but you have to load packages at each new R session.\nThere are two common ways to install packages:\n\nIf using the RStudio IDE (highly recommended), use the Install button on the Packages tab. All you have to do then is to search for the package you want to install and click the Install option. Note that this is most useful for R packages available through the Comprehensive R Archive Network (CRAN).\nWith the install.packages function. For example,\n\n\n# install packages with install.packages function\ninstall.packages(c(\"tidyverse\",\"tidytuesdayR\",\"ISLR2\"))\n\nIf you want to install a package from a repository other than CRAN, refer to the documentation, do a web search, or ask the instructor.\nTo load packages, use\n\n# load packages \nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(ISLR2)\n\nNow, functions, data, etc. from these packages will be available for reference by name 2. For example, we can access the documentation for the tt_load function from tidytuesdayR package by running the command\n\n?tt_load\n\nThis informs us that to load data from the TidyTuesday Github, we need to input a character corresponding to the date for that particular data set. For example,\n\ntt_data_examp &lt;- tt_load(\"2023-04-11\")\n\ndownloads two data files, egg-production.csv and cage-free-percentages.csv from the April 11, 2023 Tidy Tuesday post. We can access the egg production data as a data frame as follows:\n\negg_prod_df &lt;- tt_data_examp$`egg-production`\n\nThe following line of code is not run but demonstrates how to load a data file such as a comma-separated values file (CSV)\n\negg_prod_df &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\n\nThe previous code uses the read_csv function from the readr package (part of tidyverse) to directly read in the egg-production.csv from the specified location. An excellent review on reading data into R is provided in Chapter 2 of (Timbers, Campbell, and Lee 2022). View the chapter online.\nThe first few rows of this data set look as follows:\n\n\n\n\n\nobserved_month\nprod_type\nprod_process\nn_hens\nn_eggs\nsource\n\n\n\n\n2016-07-31\nhatching eggs\nall\n57975000\n1147000000\nChicEggs-09-23-2016.pdf\n\n\n2016-08-31\nhatching eggs\nall\n57595000\n1142700000\nChicEggs-10-21-2016.pdf\n\n\n2016-09-30\nhatching eggs\nall\n57161000\n1093300000\nChicEggs-11-22-2016.pdf\n\n\n2016-10-31\nhatching eggs\nall\n56857000\n1126700000\nChicEggs-12-23-2016.pdf\n\n\n2016-11-30\nhatching eggs\nall\n57116000\n1096600000\nChicEggs-01-24-2017.pdf\n\n\n2016-12-31\nhatching eggs\nall\n57750000\n1132900000\nChicEggs-02-28-2017.pdf\n\n\n\n\n\n\n\nLet’s use the glimpse function to get a quick sense of what’s in this data:\n\nglimpse(egg_prod_df)\n\nRows: 220\nColumns: 6\n$ observed_month &lt;date&gt; 2016-07-31, 2016-08-31, 2016-09-30, 2016-10-31, 2016-1…\n$ prod_type      &lt;chr&gt; \"hatching eggs\", \"hatching eggs\", \"hatching eggs\", \"hat…\n$ prod_process   &lt;chr&gt; \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\",…\n$ n_hens         &lt;dbl&gt; 57975000, 57595000, 57161000, 56857000, 57116000, 57750…\n$ n_eggs         &lt;dbl&gt; 1147000000, 1142700000, 1093300000, 1126700000, 1096600…\n$ source         &lt;chr&gt; \"ChicEggs-09-23-2016.pdf\", \"ChicEggs-10-21-2016.pdf\", \"…\n\n\nQuestion: What are some initial questions we might want to address using the egg production data? Read through the repository for the egg production data view repository. Further, see the report summary by The Human League project. What do the researchers report based on an analysis of the data?\nExercise: Examine the data downloaded as cage-free-percentages.csv. What information does it contain?\nThe following R code essentially reproduces Figure 2 from the egg production report by The Human League, see the report:\n\n\nCode\ncage_free_df %&gt;%\n  mutate(year=lubridate::year(observed_month)) %&gt;%\n  group_by(year) %&gt;%\n  summarise(percent_hens=mean(percent_hens)) %&gt;%\n  ggplot(aes(x=year,y=percent_hens)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(breaks=2007:2021) + \n  scale_y_continuous(breaks=seq(0,30,by=5),labels=paste0(as.character(seq(0,30,by=5)),\"%\")) + \n  labs(x=\"Date (year)\",y=\"Percentage of US hens in cage-free housing\")\n\n\n\n\n\nFigure 1: Reproducing Figure 2 from The Human League report on egg production.\n\n\n\n\nQuestion: What does the plot in Figure 1 show?\nExercise: Carefully examine the code used to make Figure 1. What does each line of the code do? Note that these are the types of basic data manipulations and plots you should be comfortable making entering into DS 362.\nExercise: Look at the list of data sets contained in the ISLR2 package by running the command data(package=\"ISLR2\"). Pick two of the data sets and determine what information is contained in your chosen data sets. Make a couple of basic plots using one of the data sets you chose.\n\n\nReview of the Data Science Workflow\nBefore starting our study of data mining and machine learning, let’s review the basic data science workflow as covered in an introductory level data science course. Generally, the steps in the workflow are:\n\nGather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application. If you’re going to use machine learning as part of your data analysis, this is a good place to make sure that you have enough data and also to split your data into a training set and test set.\nDocument the data and data collection process.\n\nIt is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See The Alignment Problem by Brian Christian for a thoughtful discussion on the issues related to these types of considerations (Christian 2020).\n\nImport the data for analysis. If you have not already, you may want to split your data into a training set and test set.\nExplore, clean, and transform the data. Data visualization is essential at this step. Make sure to apply the same manipulations used for cleaning and transformation to both the training and test sets.\nGenerate initial insight or more detailed questions.\n\nSteps 4-5 constitute an exploratory data analysis (EDA). It is typical to conduct EDA before doing any more sophisticated analyses such as machine learning, and this is a habit you should get into. Students are assumed to be familiar with the EDA process from DS 201. If you need a refresher, the following video is recommended:\nExploratory data analysis worked example video by Hadley Wickham, watch the video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\n\nDecide what type(s) of analysis or analyses are to be performed. This is the stage at which machine learning enters into the process.\n\nMake sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See Table 1.1 from (Timbers, Campbell, and Lee 2022) for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table here.\n\nAssess the analysis. In particular, use an appropriate metric to estimate model error.\nAt this stage, it may be necessary to repeat steps 1 - 7.\nReport your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis.\n\nIt is essential the our data science workflow be reproducible and auditable. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility). Figure 2 illustrates the concept of reproducibility.\n\n\n\nFigure 2: Artwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#footnotes",
    "href": "lesson01/index.html#footnotes",
    "title": "Lesson 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe notebooks are created using Quarto and R markdown, topics that we will cover in more detail later.↩︎\nOccasionally, there will be a conflict between functions from two different packages. R will issue a warning when such conflicts arise. You can also use the “double colon” notation to reference an object in a package without the need to load the package. For example, dplyr::select references the select function from the dplyr package.↩︎\nIf one or more of our predictor variables is categorical, then we will use a technique known as dummy variables to convert each categorical predictor variable into a sequence of numerical variables. This will add to the number of columns in our matrix representation of our predictors \\(X\\). We will cover the concept of dummy variables later in the course.↩︎"
  },
  {
    "objectID": "lesson01/index.html#review-of-the-data-science-workflow",
    "href": "lesson01/index.html#review-of-the-data-science-workflow",
    "title": "Lesson 1",
    "section": "Review of the Data Science Workflow",
    "text": "Review of the Data Science Workflow\nBefore starting our exploration of machine learning, let’s review the basic data science workflow as covered in an introductory level data science course. Generally, the steps in the workflow are:\n\nGather data with the goal of using it to gain insight to answer questions or address problems in a specific domain of application. If you’re going to use machine learning as part of your data analysis, this is a good place to make sure that you have enough data and also to split your data into a training set and test set.\nDocument the data and data collection process.\n\nIt is important to think carefully about the type of data that is collected and how the data is generated and collected. Data is often generated or collected in such a way that it will contain or reflect biases, misinformation, incomplete information, or other problematic features. When you use data or models to make a decision, the problematic features of the data may influence the decision made with unforeseen negative consequences. See The Alignment Problem by Brian Christian for a thoughtful discussion on the issues related to these types of considerations (Christian 2020).\n\nImport the data for analysis. If you have not already, you may want to split your data into a training set and test set.\nExplore and clean the data. Data visualization is essential at this step.\nGenerate initial insight or more detailed questions.\nDecide what type(s) of analysis or analyses are to be performed. This is the stage at which machine learning enters into the process.\n\nMake sure to clearly state what the goals of an analysis are. There are several different types of analyses of data that are common to conduct. See Table 1.1 from (timbers2022data?) for a list of these typical analysis types with corresponding examples of questions appropriate for a particular type of analysis, link to table here.\n\nAssess the analysis. In particular, use an appropriate metric to estimate model error.\nAt this stage, it may be necessary to repeat steps 1 - 7.\nReport your findings/results documenting each step in the analysis, and state your conclusions in the context of the question, problem, or application that motivated your analysis.\n\nIt is essential the our data science workflow be reproducible and auditable. That is, each step in a data analysis should be accessible and understandable to others (auditability) and anyone with access to your analysis should be able to re-run the analysis from start to finish and get the same result you did (reproducibility). Figure 1 illustrates the concept of reproducibility.\n\n\n\nFigure 1: Artwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#an-refresher",
    "href": "lesson01/index.html#an-refresher",
    "title": "Lesson 1",
    "section": "An   refresher",
    "text": "An   refresher\n\n\n\nArtwork by Allison Horst"
  },
  {
    "objectID": "lesson01/index.html#overview-of-data-mining-and-machine-learning",
    "href": "lesson01/index.html#overview-of-data-mining-and-machine-learning",
    "title": "Lesson 1",
    "section": "Overview of data mining and machine learning",
    "text": "Overview of data mining and machine learning\nIn this course, we view data mining as processes that can be implemented as algorithms that can be used to gain useful insights from data. Figure 3 is an illustration that shows how various components of data science, including data mining all fit together.\n\n\n\nFigure 3: Illustration credit: https://vas3k.com/blog/machine_learning/\n\n\nMachine learning, also known as statistical learning uses mathematical models to learn from data. The methods of machine learning provide one approach to data mining, an approach that is currently very popular. Machine learning is also a currently prominent approach to developing artificial intelligence technologies. Figure 4 illustrates the relationship between AI, machine learning, and deep learning. Of course, what one means by a “useful insight” is highly dependent on the domain of specialization or area of application. Thus, data mining is an inherently interdisciplinary field that intersects with many disciplines such as computer science, data science, mathematics and statistics and a variety of other fields.\n\n\n\nFigure 4: Illustration credit: https://vas3k.com/blog/machine_learning/\n\n\nOne can break the processes of data mining into a number of components including:\n\nData collection - This may involve automating the collection, storage, and maintenance of data. The data collection process should be well-documented.\nFeature extraction and data cleaning - Real-world data is messy and will rarely be immediately suitable for any meaningful analysis.\nModeling and analysis - Where an attempt is made to gain useful insight from data. This step should include a critical assessment, often via a quantitative assessment of results from modeling and analysis.\n\nAs we proceed through the course, we will touch on each of components 1 - 3 in more detail. For the remainder of this lesson, let’s look at some questions about specific data to motivate our study of machine learning.\n\nA Motivating Example\nThe Tidy Tuesday entry for January 25, 2022 contains data related to ratings of board games. This data consists of two csv files that contain various information or features on some specific games. View the data repository. An obvious question that board game creators or players might be interested in is, what factors contribute to the rating of a board game?\nThe first thing we need to do is load the data. In the following code, we download the two csv files and combine them into one common data set:\n\n# read in data csv files\nratings &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/ratings.csv\")\ndetails &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-25/details.csv\")\n\n# combine into one data frame\nratings_joined &lt;-\n  ratings %&gt;%\n  left_join(details, by = \"id\")\n\n# glimpse the data\nglimpse(ratings_joined)\n\nRows: 21,831\nColumns: 32\n$ num.x                   &lt;dbl&gt; 105, 189, 428, 72, 103, 191, 100, 3, 15, 35, 3…\n$ id                      &lt;dbl&gt; 30549, 822, 13, 68448, 36218, 9209, 178900, 16…\n$ name                    &lt;chr&gt; \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders…\n$ year                    &lt;dbl&gt; 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016…\n$ rank                    &lt;dbl&gt; 106, 190, 429, 73, 104, 192, 101, 4, 16, 36, 3…\n$ average                 &lt;dbl&gt; 7.59, 7.42, 7.14, 7.74, 7.61, 7.41, 7.60, 8.42…\n$ bayes_average           &lt;dbl&gt; 7.487, 7.309, 6.970, 7.634, 7.499, 7.305, 7.50…\n$ users_rated             &lt;dbl&gt; 108975, 108738, 108024, 89982, 81561, 76171, 7…\n$ url                     &lt;chr&gt; \"/boardgame/30549/pandemic\", \"/boardgame/822/c…\n$ thumbnail               &lt;chr&gt; \"https://cf.geekdo-images.com/S3ybV1LAp-8SnHIX…\n$ num.y                   &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n$ primary                 &lt;chr&gt; \"Pandemic\", \"Carcassonne\", \"Catan\", \"7 Wonders…\n$ description             &lt;chr&gt; \"In Pandemic, several virulent diseases have b…\n$ yearpublished           &lt;dbl&gt; 2008, 2000, 1995, 2010, 2008, 2004, 2015, 2016…\n$ minplayers              &lt;dbl&gt; 2, 2, 3, 2, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2, 2, 2…\n$ maxplayers              &lt;dbl&gt; 4, 5, 4, 7, 4, 5, 8, 5, 2, 5, 5, 4, 5, 5, 5, 4…\n$ playingtime             &lt;dbl&gt; 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150…\n$ minplaytime             &lt;dbl&gt; 45, 30, 60, 30, 30, 30, 15, 120, 30, 30, 90, 3…\n$ maxplaytime             &lt;dbl&gt; 45, 45, 120, 30, 30, 60, 15, 120, 30, 150, 150…\n$ minage                  &lt;dbl&gt; 8, 7, 10, 10, 13, 8, 14, 12, 10, 12, 12, 10, 1…\n$ boardgamecategory       &lt;chr&gt; \"['Medical']\", \"['City Building', 'Medieval', …\n$ boardgamemechanic       &lt;chr&gt; \"['Action Points', 'Cooperative Game', 'Hand M…\n$ boardgamefamily         &lt;chr&gt; \"['Components: Map (Global Scale)', 'Component…\n$ boardgameexpansion      &lt;chr&gt; \"['Pandemic: Gen Con 2016 Promos – Z-Force Tea…\n$ boardgameimplementation &lt;chr&gt; \"['Pandemic Legacy: Season 0', 'Pandemic Legac…\n$ boardgamedesigner       &lt;chr&gt; \"['Matt Leacock']\", \"['Klaus-Jürgen Wrede']\", …\n$ boardgameartist         &lt;chr&gt; \"['Josh Cappel', 'Christian Hanisch', 'Régis M…\n$ boardgamepublisher      &lt;chr&gt; \"['Z-Man Games', 'Albi', 'Asmodee', 'Asmodee I…\n$ owned                   &lt;dbl&gt; 168364, 161299, 167733, 120466, 106956, 105748…\n$ trading                 &lt;dbl&gt; 2508, 1716, 2018, 1567, 2009, 930, 1110, 538, …\n$ wanting                 &lt;dbl&gt; 625, 582, 485, 1010, 655, 692, 340, 2011, 924,…\n$ wishing                 &lt;dbl&gt; 9344, 7383, 5890, 12105, 8621, 6620, 5764, 192…\n\n\nThere is a lot of information in this data. For a more specific question, suppose we want to know if/how the number of players, game length, and age recommendations for a game impact the average rating of a game. Let’s select those features of primary interest:\n\n# select relevant features\nratings_relevant &lt;- ratings_joined %&gt;%\n  select(average,minplayers,maxplayers,playingtime,minage)\n\nLet’s see the first few rows of this data:\n\n\n\n\n\naverage\nminplayers\nmaxplayers\nplayingtime\nminage\n\n\n\n\n7.59\n2\n4\n45\n8\n\n\n7.42\n2\n5\n45\n7\n\n\n7.14\n3\n4\n120\n10\n\n\n7.74\n2\n7\n30\n10\n\n\n7.61\n2\n4\n30\n13\n\n\n7.41\n2\n5\n60\n8\n\n\n\n\n\n\n\nThis problem involves predicting a numerical response variable, that is, average based on the value of a number of predictor variables. This is a type of supervised learning problem known as regression. Since the response variable is numerical, even if we include some categorical predictor variables, we still have a regression problem.\nThe data scientist Julia Silge analyzes the board game data in a blog from post from January 28, 2022. See the blog post. Throughout the course, we will refer to a number of Silge’s posts and data analyses as they provide excellent worked examples of data mining and machine learning.\n\n\nAnother Example\nNow, consider the Palmer penguins data set available through the palmerpenguins package. This data contains information on penguins from the Palmer Station, Antarctica.\nWe glimpse the data\n\n# load the library (don't forget to install if necessary)\nlibrary(palmerpenguins)\n# glimpse data\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nand examine the first few rows of the data\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\nOne question we may be interested in is, can the physical measurements of the penguins be used to distinguish the animals in terms of their species? That is, can the physical measurements be used to predict the species of a penguin without having to, say conduct a genetic analysis? Here, our response variable is categorical. This is another type of supervised learning problem know as classification  Whenever we have a categorical response, we have a classification problem.\nFor both the board game data and the penguins data, there is a response variable that we are interested to predict based on the values of certain predictor variables. Further, the data can be viewed as paired where each set of predictor values (which we denote generically by \\(X\\)) has a corresponding known response value (denoted \\(y\\)). This data provides a set of examples that can be used to learn the relationship between the predictors \\(X\\) and the response \\(y\\). This is why regression and classification problems are called supervised learning problems.\n\nSome Mathematical Notation\nIt is convenient to organize our data by using the mathematical notation of vectors and matrices, especially for machine learning problems. Then, many machine learning algorithms can be derived or implemented via the powerful methods of numerical linear algebra. Suppose that we have paired data consisting of \\(n\\) observations (think rows) and \\(p\\) numerical predictor variables 3. Then our predictor values \\(X\\) can be organized as an \\(n \\times p\\) matrix such as\n\\[\nX = \\left[\\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1p} \\\\\nx_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\cdots &  \\ddots & \\vdots  \\\\\nx_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{array}\\right]\n\\]\nWe then view the response variable \\(y\\) as a vector of length \\(p\\), that is,\n\\[\ny = \\left[\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{p}\n\\end{array}\\right]\n\\]\nEach row of the matrix representation for \\(X\\) corresponds to an observation and, in case of supervised learning, is associated with a single entry of \\(y\\). For example, row \\(j\\) of \\(X\\), that is \\([\\begin{array}{cccc} x_{j1} & x_{j2} & \\cdots & x_{jp} \\end{array}]\\), is labelled by \\(y_{j}\\).\nOur next step in the course will be to study some common supervised learning algorithms. There are also situations in which a data set does not consist of a natural pairing between predictors and a response. Unpaired or unlabeled data can be analysed using unsupervised learning methods. We will address unsupervised learning problems and algorithms later in the course."
  },
  {
    "objectID": "syllabus.html#homework-assignments",
    "href": "syllabus.html#homework-assignments",
    "title": "Syllabus",
    "section": "Homework Assignments",
    "text": "Homework Assignments\nThere will be roughly 12 weekly homework assignments throughout the semester totaling 30% of the overall course grade. These assignments with due dates will be posted to the course learning management system. Homework problems will be a mix of hand-written and computer assignments and the problems will relate to the material covered in lectures and readings."
  },
  {
    "objectID": "syllabus.html#data-package-assignment",
    "href": "syllabus.html#data-package-assignment",
    "title": "Syllabus",
    "section": "Data Package Assignment",
    "text": "Data Package Assignment"
  },
  {
    "objectID": "syllabus.html#semester-project",
    "href": "syllabus.html#semester-project",
    "title": "Syllabus",
    "section": "Semester Project",
    "text": "Semester Project\nThe semester project will incorporate all the components of a data analysis covered in the course throughout the semester applied to a particular data set. Various components of the project will be due at different times but you will have the opportunity to revise some components prior to the submission of the final product.\nA complete project, counting for 30% of the overall course grade will consist of the following:\n\nAn initial exploratory data analysis for your data set.\nAn appropriate analysis of your data set with the goal to address a specific research question.\nA project report developed using Quarto.\nSlides for a presentation summarizing your project. You will not actually present the slides.\nA GitHub repository containing all code (appropriately documented) written and used in your project.\n\nYour project report and presentation should be written as if it is addressed to a stake holder with some subject matter knowledge in the domain of application but not necessarily with a quantitative or programming background. Further details on the semester project such as the parameters of the assignment and a grade rubric will be posted on the course learning management system."
  },
  {
    "objectID": "syllabus.html#data-modeling-case-studies",
    "href": "syllabus.html#data-modeling-case-studies",
    "title": "Syllabus",
    "section": "Data Modeling Case Studies",
    "text": "Data Modeling Case Studies\nOver the course of the semester, you will be asked to complete two data modeling case studies that will form a total of 40% of the overall course grade. For each case study, you will be provided with a data set and then asked to apply and compare multiple methods of analysis on the provided data set. A complete data modeling case study should consist of\n\nAppropriately documented code implementing appropriate models.\nA GitHub repository with all code.\nA report developed in Quarto that summarizes your conclusions.\n\nFurther details on the case study assignments such as the parameters of the assignments and grade rubrics will be posted on the course learning management system."
  },
  {
    "objectID": "lesson02/index.html",
    "href": "lesson02/index.html",
    "title": "Lesson 2",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine machine learning in general terms and distinguish between supervised and unsupervised learning problems.\nDistinguish between regression and classification problems.\nUnderstand the concept of error and the significance of test versus training error.\nAppreciate the trade-off between model flexibility and interpretability, and between bias and variance."
  },
  {
    "objectID": "lesson02/index.html#learning-objectives",
    "href": "lesson02/index.html#learning-objectives",
    "title": "Lesson 2",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine machine learning in general terms and distinguish between supervised and unsupervised learning problems.\nDistinguish between regression and classification problems.\nUnderstand the concept of error and the significance of test versus training error.\nAppreciate the trade-off between model flexibility and interpretability, and between bias and variance."
  },
  {
    "objectID": "lesson02/index.html#readings-etc.",
    "href": "lesson02/index.html#readings-etc.",
    "title": "Lesson 2",
    "section": "Readings, etc.",
    "text": "Readings, etc.\n\nRead Chapter 2 of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nThe following two video lectures are also recommended:\n\n\nMotivating problems for machine (statistical) learning. Watch video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nSupervised and unsupervised learning. Watch video on YouTube."
  },
  {
    "objectID": "lesson02/index.html#preparation-for-the-next-lesson",
    "href": "lesson02/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 2",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead section 3.1 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read section 2.1 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on regression. View on YouTube."
  },
  {
    "objectID": "lesson02/index.html#references",
    "href": "lesson02/index.html#references",
    "title": "Lesson 2",
    "section": "References",
    "text": "References\n\n\nAlexander, Rohan. n.d. “Telling Stories with Data: With Applications in r.”\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-19\n pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson02/index.html#introduction-to-machine-statistical-learning",
    "href": "lesson02/index.html#introduction-to-machine-statistical-learning",
    "title": "Lesson 2",
    "section": "Introduction to Machine (Statistical) Learning",
    "text": "Introduction to Machine (Statistical) Learning\nMachine learning or statistical learning generally refers to methods or tools that seek to derive insight or understanding from data by using models. Here by model we mean a mathematical or computational representation of some part of the real world. In machine learning, we fit a model or class of models to data. The goal of fitting models is usually one of the following:\n\nPrediction - using what is known to make informed (hopefully accurate) claims about what we want to know.\nInference - using a sample to make informed (hopefully accurate) claims about a larger population.\n\nFor an example of prediction, suppose that we are advertising experts working with a customer that sales video games. Our customer cannot directly control their sales but they can directly control their marketing by deciding how much to invest in advertising. Say for example that our customer has three ways to advertise: via YouTube, via podcasts, or via Spotify. We can use our past knowledge about how much our customer has spent i.e., their advertising budget and the corresponding sales to make predictions using a model about how sales will be in the future depending on how the company changes its advertising in each of the three media.\nFor an example of inference, suppose we have data on how a sample of patients respond to a particular drug. We can use this to make claims about how a larger population of patients will respond to this same drug.\nThere are two prominent broad classes of machine learning models:\n\nSupervised - In supervised learning, data comes in pairs \\((y_{i},{\\bf x}_{i})\\) where we view \\({\\bf x}_{i}\\) (which may be a vector) as a predictor and \\(y_{i}\\) as a response. Often, We the predictors are something we can influence directly like the advertising budget from our earlier example while the response is something we don’t have direct control over like the sales from our example. Thus, there is an assumed functional relationship between predictors and the response of the form\n\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] where we think of \\(f({\\bf x})\\) as the mean value for \\(y\\) viewed as a random variable and \\(\\epsilon\\) as containing the variance of \\(y\\) so that \\(E[\\epsilon] = 0\\).\nWe note that \\(y\\) may be numerical in which case we have a regression problem or it may be categorical in which case we have a classification problem.\n\nUnsupervised - In unsupervised learning, there is no response variable. Some common unsupervised problems include clustering and density estimation. Both of these essentially seek to discover a pattern in the data.\n\nFigure 1 illustrates the distinction between supervised and unsupervised learning models.\n\n\n\nFigure 1: Illustration credit: https://vas3k.com/blog/machine_learning/\n\n\n\nFitting Supervised Models\nFitting a supervised learning model typically amounts to estimating the function \\(f\\) in the assumed relationship\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] between the predictor and response variables. When we estimate \\(f\\) we denote the estimate by \\(\\hat{f}\\). Then, we can use \\(\\hat{f}\\) to predict the response for each predictor value \\({\\bf x}\\) by computing\n\\[\n\\hat{y} = \\hat{f}({\\bf x})\n\\]\nHow do we estimate a function \\(f\\)? In machine learning, we use the data together with some algorithm to construct \\(\\hat{f}\\).\n\nRegression\nLet’s consider an illustrative example where \\({\\bf x}\\) represents the years of education of some individuals and \\(y\\) is the income they earn in their profession. Thus, both variables are numerical so we are dealing with a regression problem. We are assuming that there is a true but unknown functional relationship between the years of education and the income they earn.\nThe left panel of Figure 2 shows a scatter plot of our education versus income data while the right panel shows the data again but with a curve corresponding to the graph of a function \\(\\hat{f}\\) that passes through the data.\n\n\n\nFigure 2: Illustration of supervised learning through a regression problem. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nHow did we come up with the function \\(\\hat{f}\\)? Basically, minimized the error between our predicted and observed response. That is, for each response value \\({\\bf x}\\) we minimized how far \\(y=f({\\bf x})\\) can be from \\(\\hat{y}=\\hat{f}({\\bf x})\\). There are three important points that need to be addressed before we can implement regression in a practical situation.\n\nThe set of all functions is too large to work with in practice so we must make some choices that allow us to narrow down the class of functions from which \\(\\hat{f}\\) will be taken. For example, we could restrict to only linear functions, or only quadratic functions, or only polynomial functions. These classes of functions are easy to describe because these types of functions are uniquely described by a finite number of parameters. However, sometimes data can not be modeled well by, e.g., polynomials so more sophisticated non-parametric ways of describing classes of functions have been developed that allow for more flexible modeling.\nWe must decide on how we will define and measure error. For regression problems, a typical way to measure error is the squared-error. Referring back to the right side of Figure 2, we define the \\(i\\)-th residual \\(r_{i}\\) to be the vertical (signed) distance between the observed response value \\(y_{i}\\) and the corresponding predicted value \\(\\hat{y}_{i} = \\hat{f}({\\bf x}_{i})\\). That is,\n\n\\[\nr_{i} = y_{i} - \\hat{y}_{i}\n\\] Then the squared error (SE) is\n\\[\n\\text{SE} = \\sum_{i=1}^{n}r_{i}^{2} = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2} = \\sum_{i=1}^{n}(y_{i} - \\hat{f}({\\bf{x}_{i}}))^{2}\n\\]\nIn this case, we take \\(\\hat{f}\\) to be the function from some specified class of functions such that it minimizes the corresponding SE.\nImportant Point: A main component of many if not most supervised machine learning problems is solving some kind of optimization problem. Usually when one speaks of a machine learning algorithm (or learning algorithm), what they are actually referring to is some algorithm that is used to solve an appropriate optimization problem.\n\nWe have to distinguish between reducible error and irreducible error. No machine learning model will ever be perfect. Suppose that we have an estimate \\(\\hat{f}\\) that yields a prediction \\(\\hat{y} = \\hat{f}({\\bf x})\\). Since in reality the response is a random variable\n\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] we have\n\\[\n\\begin{align*}\n\\text{E}[(y - \\hat{y})^{2}] &= \\text{E}[(f({\\bf x}) + \\epsilon - \\hat{f}({\\bf x}))^2] \\\\\n&= \\text{E}[((f({\\bf x}) - \\hat{f}({\\bf x})) + \\epsilon)^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2 - 2\\epsilon (f({\\bf x}) - \\hat{f}({\\bf x})) + \\epsilon^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2] - 2(f({\\bf x}) - \\hat{f}({\\bf x}))\\text{E}[\\epsilon] + \\text{E}[(\\epsilon - 0)^2] \\\\\n&= \\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2] + \\text{Var}[\\epsilon]\n\\end{align*}\n\\]\nBy choosing a good enough family of functions or a good enough learning algorithm we can reduce \\(\\text{E}[(f({\\bf x}) - \\hat{f}({\\bf x}))^2]\\) as much as we want. This corresponds to the reducible error. However, we have no control over \\(\\text{Var}[\\epsilon]\\) and this corresponds to the irreducible error.\n\n\nClassification\nFor classification problems in supervised machine learning, the response variable is categorical. Figure 3 illustrates this, showing a scatter plot of data where coloring is used to distinguish the data points as belonging to one of two different classes.\n\n\n\nFigure 3: Illustration of a classification problem in which the response variable is a binary categorical variable. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nFor classification problems, our goal is still to estimate a functional relationship of the form \\(y = f({\\bf x}) + \\epsilon\\). However, we can no longer measure error using the squared error because the response values are not numerical. A common method for measuring error in classification problems is classification error (CE) defined by\n\\[\n\\text{CE} = \\sum_{i=1}^{n}I(y_{i} \\neq \\hat{y}_{i})\n\\]\nwhere \\(I\\) is the indicator function that is equal to 1 whenever \\(y_{i} \\neq \\hat{y}_{i}\\) and equal to 0 whenever \\(y_{i} = \\hat{y}_{i}\\). Essentially, CE counts the number of misclassifications.\nSimilar to regression, fitting a classification model involves finding a function \\(\\hat{f}\\) from some specified class of functions such that the corresponding CE is minimized.\nNote that it is possible to convert a regression problem to a classification problem by binning or discretizing the response variable in some way.\n\n\n\nComplexity Vs. Interpretability\nAnother issue that needs to be taken into account when fitting models is the tradeoff between how easy it is to interpret a model versus the maximum degree of accuracy for the model. Figure 4 illustrates this through a representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity.\n\n\n\nFigure 4: A representation of the tradeoff between model flexibility and the degree of interpretability of the model. The more flexible the model, the easier it will be to reduce the reducible error. However, highly flexible models tend to be difficult to interpret because they involve many more parameters or possess other types of complexity. Figure from (Tibshirani, James, and Trevor 2017).\n\n\n\n\nTraining Error Vs. Test Error\nWhen we fit a model to data, say by minimizing the error the resulting estimate function we get depends on the data used to fit the model. We refer to this data as the training data and the corresponding error as the training error. By choosing a sufficiently flexible set of functions from which to fit to the data, we can make the training error as small as we want. This might seem like a great thing, but there is a major problem with it.\nSuppose we want to use a model to make predictions about future unseen values of our predictor \\({\\bf x}\\). If a model is fit too well to the training data, then in general it tends not to be very good at making accurate predictions for future values. One says that models that are overfit to the training data are poor at generalization.\nHow do we build models that generalize well and avoid overfitting? A common approach is to separate data into a training set that is used to fit a model and a test set which is used to assess how well the models generalizes to unseen data via the test error. Figure 5 shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data.\n\n\n\nFigure 5: The left panel shows sample data are several different model fits of varying complexity. The right panel shows the corresponding training and test error for each of the different models. The dashed horizontal line is the minimum possible test error. We see that the most complex model massively overfits the training data. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nWhile the training/test set approach to fitting accurate models while avoiding overfitting is very good in principle, there are some practical limitations. For example,\n\nHow do we know the training data is sufficiently representative?\nWhat if we don’t have a sufficiently large data set to split into a training and a test set?\nHow do we know what the minimum possible test error is?\n\nWe will spend a lot of time later talking more about these issues and ways to deal with them.\n\n\nThe Bias-Variance Trade-Off\nReferring back to Figure 5, notice the distinct U-shape in the curve for the test error. This is more than just a curiosity, it is the result of another type of trade-off known as the bias-variance trade-off.\nLet’s try to get a sense for this starting with some intuition. Suppose we having a regression problem with a single predictor. If we restrict to the class of linear functions, that is functions with graph that is a straight line in the plane, then any such function is uniquely specified by two parameters, the slope and intercept. Intuitively, such as model is highly biased because it’s going to make very rigid predictions. However, linear functions have low variance in the sense than models fit to similar data will have very similar slope and intercept values. On the other hand, a cubic polynomial being described uniquely by four parameters is much less biased than a linear function but will have higher variance.\nIt is outside the scope of this course, but it can be shown that the expected squared error for an observed value \\({\\bf x}_{0}\\) can be decomposed as follows:\n\\[\n\\text{E}[(y_{0} - \\hat{f}({\\bf x}))^2] = \\text{Var}(\\hat{f}({\\bf x}_{0})) + [\\text{Bias}(\\hat{f}({\\bf x}_{0}))]^2 + \\text{Var}(\\epsilon)\n\\]\nWe refer to the first two terms as\n\nthe variance of \\(\\hat{f}({\\bf x}_{0})\\)\nthe squared bias of \\(\\hat{f}({\\bf x}_{0})\\)\n\nFigure 6 illustrates this formula.\n\n\n\nFigure 6: Squared bias (blue curve), variance (orange curve), Var(ε) (dashed line), and test MSE (red curve). The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nImportant: What you should keep in mind as we proceed through the course is the following:\n\nSimple models tend to have high bias but much lower variance.\nComplex models tend to have lower bias but much higher variance.\n\nAnytime you choose a particular modeling approach for a specific application or data set, you should take into account the bias-variance trade-off.\n\n\nUnsupervised Learning\nIn unsupervised learning problems, data is unlabeled in the sense that there are no response values. It’s a but tricky to give a general description of all unsupervised learning problems. The most common types of unsupervised learning problems are clustering, dimension reduction, and pattern recognition. Figure 7 illustrates clustering problems.\n\n\n\nFigure 7: A clustering data set involving three groups. Each group is shown using a different colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more chal lenging. Figure from (Tibshirani, James, and Trevor 2017).\n\n\nWe will introduce unsupervised learning in more detail later in the course."
  },
  {
    "objectID": "lesson02/index.html#some-data-sources",
    "href": "lesson02/index.html#some-data-sources",
    "title": "Lesson 2",
    "section": "Some Data Sources",
    "text": "Some Data Sources\nOne thing students often struggle with is finding and picking a good data set for their projects. Appendix B of the online textbook contains a very helpful list of data sources (Alexander, n.d.). View appendix B. Two other very interesting and useful sources of data are the Tidy Tuesday data repositories and Kaggle. There are also many R packages that either include data or that can be used to download data. The ROpenSci project is a good resource for finding R packages that can be used to obtain data, view the project."
  },
  {
    "objectID": "lesson03/index.html#readings-etc.",
    "href": "lesson03/index.html#readings-etc.",
    "title": "Lesson 3",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson:\n\nRead chapter 3 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 2 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on regression. View on YouTube."
  },
  {
    "objectID": "lesson03/index.html#preparation-for-the-next-lesson",
    "href": "lesson03/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 3",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson:\n\nRead chapter 4 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 3 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on classification. View on YouTube."
  },
  {
    "objectID": "lesson03/index.html#references",
    "href": "lesson03/index.html#references",
    "title": "Lesson 3",
    "section": "References",
    "text": "References\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-16\n pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson03/index.html",
    "href": "lesson03/index.html",
    "title": "Lesson 3",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine linear regression and appreciate it from the perspective of machine learning.\nDefine the norm of a vector.\nUnderstand the role of systems of linear equations, matrices, and \\(QR\\) factorization in linear regression.\nUse R to compute the \\(QR\\) factorization of a matrix.\nUse the \\(QR\\) factorization to compute the coefficients for linear regression."
  },
  {
    "objectID": "lesson03/index.html#learning-objectives",
    "href": "lesson03/index.html#learning-objectives",
    "title": "Lesson 3",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine linear regression and appreciate it from the perspective of machine learning.\nDefine the norm of a vector.\nUnderstand the role of systems of linear equations, matrices, and \\(QR\\) factorization in linear regression.\nUse R to compute the \\(QR\\) factorization of a matrix.\nUse the \\(QR\\) factorization to compute the coefficients for linear regression."
  },
  {
    "objectID": "lesson03/index.html#motivation-for-linear-regression",
    "href": "lesson03/index.html#motivation-for-linear-regression",
    "title": "Lesson 3",
    "section": "Motivation for Linear Regression",
    "text": "Motivation for Linear Regression\nRecall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:\n\\[\ny = f({\\bf x}) + \\epsilon\n\\] and then we seek to find a function \\(\\hat{f}\\) from a predetermined class of functions that does a good job in approximating \\(f\\). Let’s study this problem in more detail but in a very simplest setting. Specifically, we will assume that \\({\\bf x}\\) and \\(y\\) are both single numerical variables and that \\(f\\) is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers \\(\\beta_{0}\\) and \\(\\beta_{1}\\) such that\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\] for all values of \\(x\\) and \\(y\\). Recall that we are assuming that \\(\\text{E}[\\epsilon] = 0\\) so \\(\\epsilon\\) is a random variable with expected value (or mean) equal to zero.\nIf we restrict ourselves to the class of single-variable linear functions, then finding an approximation to \\(f(x) = \\beta_{0} + \\beta_{1} x\\) is equivalent to finding values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) so that\n\\[\n\\hat{f}(x) = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x \\approx f(x) = \\beta_{0} + \\beta_{1} x\n\\] Thus, this would be a parametric model since any candidate approximating function is uniquely specified by specifying the values for the parameters \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\).\nFigure 1 shows the plot of data that has been generated by a relationship of the form \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\). You should examine the code used to create or simulate the data in this example and see how it relates to the expression \\(y = \\beta_{0} + \\beta_{1} x + \\epsilon\\).\n\n\nCode\nset.seed(1287)\nN &lt;- 25\nx &lt;- rnorm(N,mean=72,sd=12)\ny &lt;- 1.2 + 0.75 * x + rnorm(N,sd=2)\nxy_data &lt;- tibble(x=x,y=y)\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point()\n\n\n\n\n\nFigure 1: A data set with two numerical variables \\(x\\) and \\(y\\) generated by an underlying linear function so that \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\).\n\n\n\n\nFrom a (supervised) machine learning perspective, fitting a line to such data means “learning” the values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) from the data. How do we learn \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\)? Figure 2 shows the same data as in Figure 1 but where we have added a best fit line as well as a single residual value.\n\n\nCode\nfitted_linear_model &lt;- lm(y ~ x, data=xy_data) %&gt;%\n  augment()\n\na_point &lt;- fitted_linear_model[1,1:3] %&gt;% as.numeric()\n\nxy_data %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) + \n  geom_point(data=NULL,aes(x=a_point[2],y=a_point[3]),color=\"purple\",size=3) + \n  geom_segment(aes(x = a_point[2], y = a_point[1], xend = a_point[2], yend = a_point[3]), \n               data = NULL,\n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 2: The same data as shown in Figure 1 but with a best fit line as well as a single residual also shown.\n\n\n\n\nFigure 3 shows the same data as in Figure 1 but where we have added a best fit line as well as all the residual values. One way to learn the values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_{1}\\) is to minimize the squared error for the residuals.\n\n\nCode\nfitted_linear_model %&gt;%\n  ggplot(aes(x=x,y=y)) + \n  geom_point() + \n  geom_smooth(method=\"lm\",se = FALSE) +\n  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), \n               color=\"red\",lwd=1)\n\n\n\n\n\nFigure 3: The same data as shown in Figure 1 but with a best fit line as well as all residuals also shown.\n\n\n\n\nNotice that we can write the squared error for the residuals as a function of two variables \\(L(\\beta_{0},\\beta_{1})\\) defined by\n\\[\nL(\\beta_{0},\\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2\n\\]\nThen, in order to minimize this function we need to find the critical values for the function \\(L\\) by computing partial derivatives and solving\n\\[\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta_{0}} &= 0 \\\\\n\\frac{\\partial L}{\\partial \\beta_{1}} &= 0\n\\end{align*}\n\\]\nHowever, there is an alternative approach that uses tools from linear algebra such as matrices and we will examine this approach for a few reasons:\n\nIt motivates the use of linear algebra and matrices in machine learning.\nIt helps provide a geometric perspective to machine learning.\nIt generalizes well to the situation when we have more than one predictor variable.\n\nThe next section treats the linear algebra tools we will use for linear regression and the section after that applies linear algebra to do linear regression.\nNote: Linear regression can and often is used even in situations where we do not know a priori that \\(f\\) in the relation \\(y = f(x) + \\epsilon\\) is linear.\nQuestion: What do you think some of the pros and cons of using linear regression for supervised learning even if the function \\(f\\) in the relationship \\(y = f(x) + \\epsilon\\) might not be linear?"
  },
  {
    "objectID": "lesson03/index.html#overdetermined-systems-and-qr-factorization",
    "href": "lesson03/index.html#overdetermined-systems-and-qr-factorization",
    "title": "Lesson 3",
    "section": "Overdetermined Systems and \\(QR\\) Factorization",
    "text": "Overdetermined Systems and \\(QR\\) Factorization\nRecall that a system of linear equations is an expression of the form\n\\[\n\\begin{align*}\na_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1p} x_{p}  &= b_{1} \\\\\na_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2p} x_{p} &= b_{2} \\\\\n&\\vdots \\\\\na_{n1}x_{1} + a_{n2}x_{2} + \\cdots + a_{np} x_{p} &= b_{n}\n\\end{align*}\n\\]\nand such a system can be rewritten using matrix notation as\n\\[\n\\left[\\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ a_{21} & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\ddots & \\cdots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{np} \\end{array}\\right] \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{p} \\end{array} \\right] = \\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{array} \\right]\n\\]\nor even more concisely as\n\\[\nA {\\bf x} = {\\bf b}\n\\]\n\n\\(QR\\) Factorization\n\\[\n\\left[\\begin{array}{cc} 3 & 1 \\\\ 4 & 2 \\end{array}\\right] = \\left[\\begin{array}{cc} \\frac{3}{5} & -\\frac{4}{5} \\\\ \\frac{4}{5} & \\frac{3}{5} \\end{array}\\right]\\left[\\begin{array}{cc} 5 & \\frac{11}{5} \\\\ 0 & \\frac{2}{5} \\end{array}\\right]\n\\]\n\n\nCode\nA &lt;- matrix(c(3,1,4,2),2,2,byrow = TRUE)\nA_qr &lt;- qr(A)\nQ &lt;- qr.Q(A_qr)\nR &lt;- qr.R(A_qr)\n\n\n\nQ %*% R\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    4    2\n\n\n\nround(Q %*% t(Q),2)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\nround(t(Q) %*% Q,2)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1"
  },
  {
    "objectID": "lesson03/index.html#overdetermined-linear-systems-and-qr-factorization",
    "href": "lesson03/index.html#overdetermined-linear-systems-and-qr-factorization",
    "title": "Lesson 3",
    "section": "Overdetermined Linear Systems and \\(QR\\) Factorization",
    "text": "Overdetermined Linear Systems and \\(QR\\) Factorization\nRecall that a system of linear equations is an expression of the form\n\\[\n\\begin{align*}\na_{11}x_{1} + a_{12}x_{2} + \\cdots + a_{1p} x_{p}  &= b_{1} \\\\\na_{21}x_{1} + a_{22}x_{2} + \\cdots + a_{2p} x_{p} &= b_{2} \\\\\n&\\vdots \\\\\na_{n1}x_{1} + a_{n2}x_{2} + \\cdots + a_{np} x_{p} &= b_{n}\n\\end{align*}\n\\]\nwhere there are \\(p\\) unknowns \\(x_{j}\\), \\(n \\times p\\) coefficients \\(a_{ij}\\), and \\(n\\) given values \\(b_{i}\\). Such a system can be rewritten using matrix notation as\n\\[\n\\left[\\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1p} \\\\ a_{21} & a_{22} & \\cdots & a_{2p} \\\\ \\vdots & \\ddots & \\cdots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{np} \\end{array}\\right] \\left[\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{p} \\end{array} \\right] = \\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{array} \\right]\n\\]\nor even more concisely as\n\\[\nA {\\bf x} = {\\bf b}\n\\]\nand we say that \\(A\\) is an \\(n \\times p\\) matrix.\nDefinition: A linear system \\(A {\\bf x} = {\\bf b}\\) is said to be overdetermined if there are more equations that unknowns. That is, if \\(n &gt; p\\).\nWe will soon see that the problem of linear regression typically corresponds to “solving” an overdetermined linear system. There’s a problem though, overdetermined linear systems do not usually have a solution. For example, consider the following linear system:\n\\[\n\\begin{align*}\nx &= 1 \\\\\nx &= 2\n\\end{align*}\n\\] which is an overdetermined linear system since it has two equations in one unknown. This system clearly does not possess a solution.\nGiven an \\(n \\times p\\) matrix \\(A\\) and an \\(n\\)-vector \\({\\bf b}\\), for any vector \\(p\\)-vector \\({\\bf x}\\) we can always form the residual \\({\\bf r}\\) defined by\n\\[\n{\\bf r} = {\\bf b} - A{\\bf x}\n\\] Note that \\({\\bf x}\\) is a solution to the linear system \\(A{\\bf x} = {\\bf b}\\) is and only if the residual \\({\\bf r}\\) is the zero vector.\nNow, based on the last example we cannot generally make the the residual zero for an overdetermined system. However, we could instead search for a vector \\({\\bf x}\\) that makes the residual as small as possible. In order to do so, we need a way to measure the size of a residual. Vector norms are a mathematical object that allow us to measure the size of residuals.\n\nNorms: A Technical Tool\nDefinition: A vector norm, denoted by \\(\\|\\cdot \\|\\) is an object that takes as input a vector and returns a real number while satisfying the following properties:\n\nPositivity: For any vector \\(\\|{\\bf v}\\| \\geq 0\\) and \\(\\|{\\bf v}\\| = 0\\) if and only if \\({\\bf v}\\) is the zero vector.\nHomogeneity: If \\(\\alpha\\) is a number and \\({\\bf v}\\) is a vector, then \\(\\|\\alpha {\\bf v}\\| = |\\alpha| \\|{\\bf v}\\|\\).\nTriangle inequality: For any vectors \\({\\bf u}\\) and \\({\\bf v}\\), we have\n\n\\[\n\\|{\\bf u} + {\\bf v} \\| \\leq \\|{\\bf u}\\| + \\|{\\bf v}\\|\n\\] Example: The most relevant example for us, at least in the setting of linear regression is the 2-norm \\(\\|\\cdot\\|_{2}\\) which for a vector \\({\\bf v} = [\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{n} \\end{array}]^{T}\\) is defined by\n\\[\n\\|{\\bf v}\\|_{2} = \\sqrt{v_{1}^2 + v_{2}^{2} + \\cdots + v_{n}^{2}}\n\\]\nDefintion: Given a vector norm \\(\\|\\cdot\\|\\), the distance between two vectors \\({\\bf u}\\) and \\({\\bf v}\\) is defined to be \\(\\|{\\bf u} - {\\bf v}\\|\\).\nNote: It is possible to generalize vector norms to norms on sets of functions. This is also useful in machine learning since this allows us to define a notion of distance between functions.\n\n\nLinear Regression and Linear Least Squares\nThe 2-norm allows us to define the linear least squares problem:\n\nGiven an \\(n \\times p\\) matrix \\(A\\) and a \\(n\\)-vector \\({\\bf b}\\), find a vector \\({\\bf x}\\) that satifies\n\n\\[\n\\|{\\bf b} - A{\\bf x}\\|_{2} \\leq \\|{\\bf b} - A{\\bf \\xi}\\|_{2} , \\ \\ \\text{for all vectors } {\\bf \\xi}\n\\] That is, the linear least squares problem is to find a vector that minimizes the corresponding residual with respect to the 2-norm.\nExample: Consider again the linear system\n\\[\n\\begin{align*}\nx &= 1 \\\\\nx &= 2\n\\end{align*}\n\\]\nThen the linear least squares problem in this case is to find the value \\(x\\) such that\n\\[\n\\sqrt{(x-1)^2 + (x-2)^2}\n\\]\nis as small as possible. Note that this is equivalent to minimizing\n\\[\n(x-1)^2 + (x-2)^2 = 2x^2 - 6x + 5\n\\]\nIn general, minimizing the residual in the 2-norm corresponds to minimizing a multi-variable quadratic function. However, we can use linear algebra instead of calculus to solve the linear least squares problem. The main tool we need is what is known as the \\(QR\\) factorization or decomposition of a matrix.\n\n\n\\(QR\\) Factorization\nConsider the following example that can easily be checked by hand.\n\\[\n\\left[\\begin{array}{cc} 3 & 1 \\\\ 4 & 2 \\end{array}\\right] = \\left[\\begin{array}{cc} \\frac{3}{5} & -\\frac{4}{5} \\\\ \\frac{4}{5} & \\frac{3}{5} \\end{array}\\right]\\left[\\begin{array}{cc} 5 & \\frac{11}{5} \\\\ 0 & \\frac{2}{5} \\end{array}\\right]\n\\]\nThis has the form \\(A = QR\\) and what is really interesting here is that\n\nThe matrix \\(Q = \\left[\\begin{array}{cc} \\frac{3}{5} & -\\frac{4}{5} \\\\ \\frac{4}{5} & \\frac{3}{5} \\end{array}\\right]\\) satisfies that \\(QQ^{T} = I\\) and \\(Q^{T}Q = I\\). We refer to any matrix that satisfies such properties as an orthogonal matrix.\nThe matrix \\(R = \\left[\\begin{array}{cc} 5 & \\frac{11}{5} \\\\ 0 & \\frac{2}{5} \\end{array}\\right]\\) is upper triangular since all the entries below the main diagonal are zero.\n\nEvery matrix has a \\(QR\\) factorization and it is unique if \\(n \\geq p\\) and \\(A\\) is of full rank. Further, the \\(QR\\) factorization can be used to solve the linear least squares problem as follows:\n\nCompute \\(A = QR\\), the \\(QR\\) factorization 1. There are efficient algorithms for doing this.\nForm the vector \\(Q^{T}{\\bf b}\\).\nSolve the linear system \\(R{\\bf x} = Q^{T}{\\bf b}\\).\n\nThe system in point 3 is relatively easy to solve. Since \\(R\\) is upper triangular, the system \\(R{\\bf x} = Q^{T}{\\bf b}\\) can be solved by a simple algorithm known as backward substitution which is implemented in R by the function backsolve.\n\nComputing the \\(QR\\) Factorization with R\nThe following R code shows how the create a matrix, compute its \\(QR\\) factorization with the function qr, and then extract the matrices \\(Q\\) and \\(R\\):\n\nA &lt;- matrix(c(3,1,4,2),2,2,byrow = TRUE)\nA_qr &lt;- qr(A)\nQ &lt;- qr.Q(A_qr)\nR &lt;- qr.R(A_qr)\n\nLet’s check that \\(QR = A\\):\n\nQ %*% R\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    4    2\n\n\nLet’s check that \\(Q\\) is an orthogonal matrix:\n\nround(Q %*% t(Q),2)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\nround(t(Q) %*% Q,2)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1"
  },
  {
    "objectID": "lesson03/index.html#footnotes",
    "href": "lesson03/index.html#footnotes",
    "title": "Lesson 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically we are assuming that \\(A = QR\\) is the reduced \\(QR\\) factorization meaning that \\(R\\) has size \\(p \\times p\\).↩︎"
  },
  {
    "objectID": "lesson03/index.html#simple-linear-regression-an-example",
    "href": "lesson03/index.html#simple-linear-regression-an-example",
    "title": "Lesson 3",
    "section": "Simple Linear Regression: An Example",
    "text": "Simple Linear Regression: An Example\nReturn to the data shown in Figure 1 and Figure 2. Fitting a line to this data means finding values for the intercept \\(\\hat{\\beta}_{0}\\) and slope \\(\\hat{\\beta}_{1}\\) so that for each data point \\((x_{i},y_{i})\\) we have that\n\\[\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i} = \\hat{\\beta}_{0} \\cdot 1 + \\hat{\\beta}_{1} x_{i}\n\\]\nis as close as possible to \\(y_{i}\\). We can rewrite the last expression as a matrix vector product:\n\\[\nX {\\bf \\beta} =  \\left[\\begin{array}{cc} 1 & x_{1} \\\\ 1 & x_{2} \\\\ \\vdots & \\vdots \\\\ 1 & x_{p} \\end{array}\\right]\\left[\\begin{array}{c} \\beta_{0} \\\\ \\beta_{1} \\end{array}\\right]\n\\]\nWe call the matrix \\(X\\) so formed the data matrix and the vector \\({\\bf \\beta}\\) the parameter vector.\n\ny_vect &lt;- xy_data %&gt;% pull(y)\ny_vect &lt;- matrix(y_vect,ncol = 1)\nX_mat &lt;- xy_data %&gt;% \n  mutate(int_ones = rep(1,nrow(xy_data))) %&gt;% \n  select(int_ones,x) %&gt;% \n  as.matrix()\n\nX_qr &lt;- qr(X_mat)\nR_mat &lt;- qr.R(X_qr) \nQ_mat &lt;- qr.Q(X_qr)\n(beta_vals &lt;- backsolve(R_mat,t(Q_mat) %*% y_vect))\n\n         [,1]\n[1,] 1.774549\n[2,] 0.742658\n\n\nLet’s compare this with what R does when we use the function lm to fit a linear model.\n\nlm_fit &lt;- lm(y ~ x,data=xy_data)\n\ncoefficients(lm_fit)\n\n(Intercept)           x \n   1.774549    0.742658 \n\n\nThis is what R is doing to compute the slope and intercept for the line in Figure 2 and Figure 3."
  },
  {
    "objectID": "lesson03/index.html#simple-linear-regression",
    "href": "lesson03/index.html#simple-linear-regression",
    "title": "Lesson 3",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nReturn to the data shown in Figure 1 and Figure 2. Fitting a line to this data means finding values for the intercept \\(\\hat{\\beta}_{0}\\) and slope \\(\\hat{\\beta}_{1}\\) so that for each data point \\((x_{i},y_{i})\\) we have that\n\\[\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i} = \\hat{\\beta}_{0} \\cdot 1 + \\hat{\\beta}_{1} x_{i}\n\\]\nis as close as possible to \\(y_{i}\\). We can rewrite the last expression as a matrix vector product:\n\\[\nX {\\bf \\beta} =  \\left[\\begin{array}{cc} 1 & x_{1} \\\\ 1 & x_{2} \\\\ \\vdots & \\vdots \\\\ 1 & x_{n} \\end{array}\\right]\\left[\\begin{array}{c} \\beta_{0} \\\\ \\beta_{1} \\end{array}\\right]\n\\]\nIn order to account for the intercept coefficient, we have to add the column of ones. We call the matrix \\(X\\) so formed the data matrix and the vector \\({\\bf \\beta}\\) the parameter vector. Thus, we can write the linear regression problem as a linear least squares problem to minimize the residual \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\). Minimizing this is the 2-norm is the same as minimizing the squared error. Let’s see this worked out in a computational example.\n\nAn Example\nThe following R code constructs the data matrix \\(X\\), computes the \\(QR\\) factorization for \\(X\\), and then applies the backward substitution algorithm to solve \\(R{\\bf \\beta} = Q^{T}{\\bf y}\\):\n\ny_vect &lt;- xy_data %&gt;% pull(y)\ny_vect &lt;- matrix(y_vect,ncol = 1)\nX_mat &lt;- xy_data %&gt;% \n  mutate(int_ones = rep(1,nrow(xy_data))) %&gt;% \n  select(int_ones,x) %&gt;% \n  as.matrix()\n\nX_qr &lt;- qr(X_mat)\nR_mat &lt;- qr.R(X_qr) \nQ_mat &lt;- qr.Q(X_qr)\n(beta_vals &lt;- backsolve(R_mat,t(Q_mat) %*% y_vect))\n\n         [,1]\n[1,] 1.774549\n[2,] 0.742658\n\n\nNow that we have estimated model coefficients, we can use that information to make predictions with the model. For example,\n\nbeta_vals_vect &lt;- matrix(beta_vals,ncol=1,byrow = TRUE)\nX_new &lt;- matrix(c(1,67),1,2,byrow=TRUE)\n(y_pred &lt;- X_new %*% beta_vals_vect)\n\n         [,1]\n[1,] 51.53264\n\n\n\n\nFitting Linear Models with R\nThere are many functions in base R and other packages that can be used to fit not only linear models but a variety of many different types of models. In base R, we have a function lm that can be used to fit a linear regression model. Let’s examine the documentation for lm. We see that the first argument for lm is a formula. Many modeling functions in base R and other packages accept utilize a formula to represent the model specification. The easiest way to understand this is to see some examples.\nLet’s compare what R does when we use the function lm to fit a linear model with our earlier approach of using \\(QR\\) factorization.\n\nlm_fit &lt;- lm(y ~ x,data=xy_data)\n\ncoefficients(lm_fit)\n\n(Intercept)           x \n   1.774549    0.742658 \n\n\nUnder the hood, the lm function in R is using the \\(QR\\) factorization to compute the slope and intercept for the line in figures like Figure 2 and Figure 3.\nWe can also make predictions on new data with lm models:\n\npredict(lm_fit,newdata=tibble(x=67))\n\n       1 \n51.53264"
  },
  {
    "objectID": "lesson03/index.html#multiple-linear-regression",
    "href": "lesson03/index.html#multiple-linear-regression",
    "title": "Lesson 3",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSuppose that we have data of the form \\((y_{i},{\\bf x}_{i}) = (y_{i},x_{i1},x_{i2},\\ldots , x_{ip})\\) so that there are \\(p\\) predictor variables. The multiple linear regression model takes the form\n\\[\ny = \\beta_{0} + \\beta_{1}{\\bf x}_{1} + \\beta_{2}{\\bf x}_{2} + \\cdots + \\beta_{p}{\\bf x}_{p} + \\epsilon\n\\]\nTaking into account the column of ones we can form a \\(n \\times (p + 1)\\) sized data matrix and again use \\(QR\\) factorization to solve the corresponding linear least squares problem for the residual \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\). Now, our coefficient vector \\({\\bf \\beta}\\) will have length \\(p+1\\).\nMultiple linear regression is a significant generalization of simple linear regression because it not only allows us to account for multiple predictor variables, but also allows us to account for certain types of nonlinearity and also predictor variables that are categorical. This is because:\n\nThe “linear” part of linear regression refers to linearity with respect to the coefficients \\({\\bf \\beta}\\).\nWe can use dummy variables to represent categorical predictor variables.\n\nThe point is, as long as our data can be represented by a data matrix \\(X\\), then we can try to use \\(QR\\) factorization to solve the linear least squares problem to minimize the residuals \\({\\bf r} = {\\bf y} - X{\\bf \\beta}\\).\nWe can use the lm function to easily fit multiple linear regression models. Let’s work through some examples together."
  },
  {
    "objectID": "lesson03/index.html#linear-regression-in-machine-learning",
    "href": "lesson03/index.html#linear-regression-in-machine-learning",
    "title": "Lesson 3",
    "section": "Linear Regression in Machine Learning",
    "text": "Linear Regression in Machine Learning\nThrough our study of linear regression, we have derived and implemented our first supervised machine learning algorithm. We have also seen in our worked examples how to use tidymodels to apply linear regression as a machine learning algorithm. In particular, we have seen how to:\n\nAccount for certain types of nonlinearity.\nAccount for categorical predictors.\nSeparate data into a training set and a test set.\nSet up and fit a model.\nUse a model to make predictions.\nAssess model accuracy by computing the root mean square error.\n\nHowever, there are several additional considerations we still need to address. For example,\n\nAssessing model uncertainty.\nChoosing which predictors to include or not in a model.\nDeciding between different classes of models.\n\nWe will take up these issues soon but before doing so, we will first look at a learning algorithm for a classification problem."
  },
  {
    "objectID": "lesson04/index.html",
    "href": "lesson04/index.html",
    "title": "Lesson 4",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine the problem of classification.\nFit a logistic regression model.\nDefine the k-Nearest Neighbor algorithm.\nCreate and interpret an ROC curve."
  },
  {
    "objectID": "lesson04/index.html#learning-objectives",
    "href": "lesson04/index.html#learning-objectives",
    "title": "Lesson 4",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine the problem of classification.\nFit a logistic regression model.\nDefine the k-Nearest Neighbor algorithm.\nCreate and interpret an ROC curve."
  },
  {
    "objectID": "lesson04/index.html#readings-etc.",
    "href": "lesson04/index.html#readings-etc.",
    "title": "Lesson 4",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson:\n\nRead chapter 3 of Statistical Learning with Math and R (Suzuki 2020). You may also want to read chapter 4 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nWatch the corresponding video lecture on classification. View on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nThe video on logistic regression might also be helpful. View on YouTube."
  },
  {
    "objectID": "lesson04/index.html#preparation-for-the-next-lesson",
    "href": "lesson04/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 4",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson we will cover resampling methods. To prepare for the next lesson, please do the following:\n\nRead chapter 5 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 4 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on cross validation. View on YouTube."
  },
  {
    "objectID": "lesson04/index.html#references",
    "href": "lesson04/index.html#references",
    "title": "Lesson 4",
    "section": "References",
    "text": "References\n\n\nDinov, Ivo D. 2018. “Data Science and Predictive Analytics.” Cham, Switzerland: Springer.\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-18\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)\n patchwork    * 1.1.3   2023-08-14 [1] CRAN (R 4.3.0)\n pROC         * 1.18.4  2023-07-06 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidytuesdayR * 1.0.2   2022-02-01 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson04/index.html#overview",
    "href": "lesson04/index.html#overview",
    "title": "Lesson 4",
    "section": "Overview",
    "text": "Overview\nIn a classification problem, we assume a functional relationship of the form\n\\[\ny = f({\\bf x}) + \\epsilon\n\\]\nwhere the response variable \\(y\\) takes values in a finite set. The \\(y\\) values will often correspond to classes or categories and this is why we call this problem classification. Classification is a common problem for machine learning, perhaps even more common than regression. There are two general approaches to classification:\n\npredict the values of \\(y\\), that is, the classes directly, or\npredict the probabilities for \\(y\\) to be in each of the relevant classes.\n\nThe approach in 2 has the benefit that we can think of classification as a regression problem since probabilities are numerical quantities. On the other hand, any regression problem can be recast as a classification by binning or otherwise discretizing the response variable.\nA special case of classification is binary classification which is the situation where the response \\(y\\) can take on but two distinct values. We will begin out study of classification by looking at binary classification and logistic regression which is a common approach to binary classification."
  },
  {
    "objectID": "lesson04/index.html#logistic-regression",
    "href": "lesson04/index.html#logistic-regression",
    "title": "Lesson 4",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nSuppose that we have labelled data \\(({\\bf x}_{1},y_{1}), ({\\bf x}_{2},y_{2}), \\ldots , ({\\bf x}_{n},y_{n})\\) such that for each \\(i\\), \\(y_{i} \\in \\{0,1\\}\\). That is, the response variables can take on only two distinct values. Our goal is to build a model that can predict\n\\[\np({\\bf x}) := P(y = 1 | {\\bf x})\n\\] The method of logistic regression is to build a model of the form\n\\[\np({\\bf x}) = \\frac{e^{\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\cdots + \\beta_{p}x_{p}}}{1 + e^{\\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\cdots + \\beta_{p}x_{p}}}\n\\]\nFrom which we can derive the expression\n\\[\n\\log\\left( \\frac{p({\\bf x})}{1 - p({\\bf x})} \\right) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\cdots + \\beta_{p}x_{p}\n\\] We call the expression\n\\[\n\\log\\left( \\frac{p({\\bf x})}{1 - p({\\bf x})} \\right)\n\\]\nthe log odds or logit.\nNote: While the probability \\(P(Y = 1| {\\bf x})\\) is not a linear function of the predictor variables, the log odds is a linear function of the predictor.\nWe refer to a function of the form\n\\[\nf(x) = \\frac{e^x}{1 + e^x}\n\\]\nas a logistic function.\nLet’s develop some motivation and intuition for logistic regression. Consider the Default data set from the ISLR2 package. The first few rows of the data are shown below:\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\n\n\nOur goal is to use one or more of the variables student, balance, or income to predict whether an individual is likely to default on their loan. Here we will think of the values for the response default as \\(\\text{No} = 0\\) and \\(\\text{Yes} = 1\\). To do this, we create a 0-1 version of the response:\n\n\nCode\nDefault &lt;- Default %&gt;%\n  mutate(default_01 = ifelse(default == \"No\",0,1))\n\nglimpse(Default)\n\n\nRows: 10,000\nColumns: 5\n$ default    &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ student    &lt;fct&gt; No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No…\n$ balance    &lt;dbl&gt; 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885…\n$ income     &lt;dbl&gt; 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491…\n$ default_01 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\nLet’s begin by focusing on just balance as a predictor. We can plot the default variable versus the balance:\n\n\nCode\nDefault %&gt;%\n  ggplot(aes(x=balance,y=default_01)) + \n  geom_point(alpha=0.2) + \n  labs(x=\"Loan balance\",y=\"Default, No=0, Yes=1\") \n\n\n\n\n\nFigure 1: Plot of defaults versus loan balance for the Default data from the ISLR2 package.\n\n\n\n\nQuestion: What does the plot in Figure 1 suggest?\nThe goal of logistic regression is to fit the in some sense best logistic function to the data. Let’s compare a couple of different logistic functions as candidate models for the Default data. Figure 2 shows two different logistic curves plotted over the Default data.\n\n\nCode\nf_1 &lt;- function(x){ exp(-5 + 0.01 * x) / (1 + exp(-5 + 0.01 * x))}\nf_2 &lt;- function(x){ exp(-11 + 0.008 * x) / (1 + exp(-11 + 0.008 * x))}\n\np_1 &lt;- Default %&gt;%\n  ggplot(aes(x=balance,y=default_01)) + \n  geom_point(alpha=0.2) + \n  stat_function(fun = f_1,color=\"orange\",linewidth=1.1) + \n  labs(x=\"Loan balance\",y=\"Default, No=0, Yes=1\") \n\np_2 &lt;- Default %&gt;%\n  ggplot(aes(x=balance,y=default_01)) + \n  geom_point(alpha=0.2) + \n  stat_function(fun = f_2, color=\"darkgreen\",linewidth=1.1) + \n  labs(x=\"Loan balance\",y=\"Default, No=0, Yes=1\") \n\np_1 + p_2\n\n\n\n\n\nFigure 2: Comparison of two different logistics curves plotted over the defaults versus loan balance for the Default data from the ISLR2 package.\n\n\n\n\nQuestion: Which of the two logistic curves in Figure 2 do you think is a better fit for the Default data? Explain your answer.\nThe problem for us now is, how do we define and determine the “best-fit” logistic function for binary classification? Minimizing the squared error for the residuals is not going to work well in this situation (why not?).\n\nLikelihood Optimization\nThe likelihood function is the probability of the observed data viewed as a function of the model parameters. Then, we define the best-fit logistic curve to be the one that maximizes the likelihood.\nNote: We point out again that machine learning methods often involve solving some kind of optimization problem and we see that this is the case with logistic regression.\nHow should we maximize the likelihood function which is a multi-variable function? One approach is of course to find critical points by computing and setting the partial derivatives to zero and then use the Hessian criteria to confirm there is a maximum. There is a trick that makes this a little easier, that is, rather than maximize the likelihood function, it’s typical to maximize the log-likelihood. Because the \\(\\log\\) function converts products into sums, this makes it easier to compute partial derivatives.\nLuckily for us, the maximum likelihood method for logistic regression is already implemented in R through the glm function, where glm stands for generalized linear model. For example,\n\ndefault_fit_1 &lt;- glm(default_01 ~ balance, data=Default, family=\"binomial\")\ncoef(default_fit_1)\n\n  (Intercept)       balance \n-10.651330614   0.005498917 \n\n\ngives the maximum likelihood estimated values for \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Let’s plot the corresponding logistic function:\n\n\nCode\nf_best &lt;- function(x){ exp(-10.65 + 0.0055 * x) / (1 + exp(-10.65 + 0.0055 * x))}\n\n\nDefault %&gt;%\n  ggplot(aes(x=balance,y=default_01)) + \n  geom_point(alpha=0.2) + \n  stat_function(fun = f_best, color=\"purple\",linewidth=1.1) + \n  labs(x=\"Loan balance\",y=\"Default, No=0, Yes=1\") \n\n\n\n\n\nFigure 3: Maximum likelihood best fit logistic curve plotted over the defaults versus loan balance for the Default data from the ISLR2 package.\n\n\n\n\nQuestion: How does the maximum likelihood estimated logistic curve compare with the two from Figure 2?\nWe can use our logistic regression model to make predictions. For example, suppose our friend has a balance of $800, are they likelihood to default on their loan? We can answer this as follows:\n\n(default_800_odds &lt;- predict(default_fit_1,newdata=tibble(balance=800)))\n\n        1 \n-6.252197 \n\n(default_800_prob &lt;- predict(default_fit_1,newdata=tibble(balance=800),type=\"response\"))\n\n          1 \n0.001922514 \n\n\nWe see that the log-odds is very small which corresponds to a very small probability of default. Notice the relationship between the probability and the log-odds:\n\nlog(default_800_prob / (1 - default_800_prob))\n\n        1 \n-6.252197 \n\n\nOn the other hand, if our friend has a balance of $2,200 then\n\n(default_2200_odds &lt;- predict(default_fit_1,newdata=tibble(balance=2200)))\n\n       1 \n1.446287 \n\n(default_2200_prob &lt;- predict(default_fit_1,newdata=tibble(balance=2200),type=\"response\"))\n\n        1 \n0.8094263 \n\n\nand we see that the predicted probability of default is high. A question we will return to soon is, how does the probability of default relate to classifying defaults as “no” or “yes”?\nWe can just as easily fit a logistic regression model with more than one predictor. For example,\n\ndefault_fit_2 &lt;- glm(default_01 ~ balance + income + student, data=Default, family=\"binomial\")\ncoef(default_fit_2)\n\n  (Intercept)       balance        income    studentYes \n-1.086905e+01  5.736505e-03  3.033450e-06 -6.467758e-01 \n\n\nWhich of the two models is better remains an open question and we will address this and other topics related to logistic regression later."
  },
  {
    "objectID": "lesson04/index.html#roc-curves",
    "href": "lesson04/index.html#roc-curves",
    "title": "Lesson 4",
    "section": "ROC Curves",
    "text": "ROC Curves\n\nggroc(roc(Default$default_01,predict(default_fit_1,type=\"response\")),size=2)"
  },
  {
    "objectID": "lesson04/index.html#section",
    "href": "lesson04/index.html#section",
    "title": "Lesson 4",
    "section": "",
    "text": "ROC Curves\n\nggroc(roc(Default$default_01,predict(default_fit_1,type=\"response\")),size=2)"
  },
  {
    "objectID": "lesson04/index.html#classification-errors",
    "href": "lesson04/index.html#classification-errors",
    "title": "Lesson 4",
    "section": "Classification Errors",
    "text": "Classification Errors\nSuppose we work for a loan company and our boss isn’t interested in the probability that a borrower will default and just wants to know if someone is likely to default or not. One way to deal with this is to choose a cutoff value for the probability of default and classify borrowers with probability of default greater than the threshold value as likely to default. For example, suppose we classify any borrower with a probability of defaulting greater than 50% as likely to default. This will inevitably lead to some amount of misclassification. We can calculate this for our data and model:\n\ndefault_fit_1_preds &lt;- predict(default_fit_1,type=\"response\")\ndefault_fit_1_class &lt;- (default_fit_1_preds &gt; 0.5) %&gt;% as.numeric()\n\n(cm_tbl &lt;- table(Default$default_01,default_fit_1_class))\n\n   default_fit_1_class\n       0    1\n  0 9625   42\n  1  233  100\n\n\nThis says that there were 42 loans that did not default but the model classified as defaults. That is, the model produced 42 false positives. On the other hand, the model classified 233 loans that did default as non-defaults. That is, the model produced 233 false negatives. From these numbers, we can compute the accuracy:\n\n\nCode\n(cm_tbl[1,1] + cm_tbl[2,2]) / (sum(cm_tbl))\n\n\n[1] 0.9725\n\n\nor using a function from tidymodels\n\n\nCode\ntibble(true_defaults = as.factor(Default$default_01),\n       pred_defaults = as.factor(default_fit_1_class)) %&gt;%\n  accuracy(true_defaults,pred_defaults)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.972\n\n\nThese numbers will change if we change the threshold value for classification. For example, what if we set our threshold to 45%?\n\ndefault_fit_1_preds &lt;- predict(default_fit_1,type=\"response\")\ndefault_fit_1_class &lt;- (default_fit_1_preds &gt; 0.45) %&gt;% as.numeric()\n\ntable(Default$default_01,default_fit_1_class)\n\n   default_fit_1_class\n       0    1\n  0 9609   58\n  1  221  112\n\n\nFor later purposes, let’s define the following quantities:\n\\[\n\\begin{align*}\n\\text{sensitivity or true positive rate} &= \\frac{\\text{true positive}}{\\text{true positive} + \\text{false negative}} \\\\\n\\text{specificity or false positive rate} &= \\frac{\\text{true negative}}{\\text{true negative} + \\text{false positive}}\n\\end{align*}\n\\]\nThen, in our example with the 50% threshold value, the sensitivity is 0.3003003 and the specificity is 0.9956553.\nNotice that the sensitivity and specificity are both functions of the probability threshold value used to make the decision about how to classify predicted response values. Let’s write an R function that computes the sensitivity and specificity for any threshold value.\n\nspec_sens &lt;- function(threshold_val){\n\n  default_fit_1_class &lt;- (default_fit_1_preds &gt; threshold_val) %&gt;% as.numeric()\n\n  conf_mat &lt;- table(Default$default_01,default_fit_1_class)\n  \n  sensitivity &lt;- conf_mat[2,2] / (sum(conf_mat[2, ]))\n  specificity &lt;- conf_mat[1,1] / (sum(conf_mat[1, ]))\n  \n  tibble(sensitivity = sensitivity,specificity = specificity)\n  \n}\n\nNow, let’s test our function\n\nspec_sens(0.5)\n\n# A tibble: 1 × 2\n  sensitivity specificity\n        &lt;dbl&gt;       &lt;dbl&gt;\n1       0.300       0.996\n\n\n\nROC Curves\nLet’s use our function to compute and plot the sensitivity and specificity values for a range of threshold values.\n\n\nCode\nthresh_vals &lt;- seq(0.001,0.981,by=0.001)\nspec_sens_df &lt;- map_df(thresh_vals,spec_sens) %&gt;%\n  mutate(thresh_vals = thresh_vals)\n\nspec_sens_df %&gt;%\n  ggplot(aes(x=specificity,y=sensitivity,color=thresh_vals)) + \n  geom_path(linewidth=1.1) + \n  scale_x_reverse() + \n  xlim(c(1,0)) + \n  ylim(c(0,1))\n\n\n\n\n\nThis creates what is known as a receiver operating characteristic (ROC) curve which is is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\nThe package pROC contains functions that will create an ROC curve for us. For example,\n\nggroc(roc(Default$default_01,predict(default_fit_1,type=\"response\")),size=2)\n\n\n\n\nFigure 4: An ROC curve for our logistic regression model of defaults as predicted by balance. The curve is created usign functions from the pROC package.\n\n\n\n\nThe area under the ROC curve (AUC) is a widely used measure of overall performance of a binary classifier because it summarizes the performance over a range of threshold values. Again, the pROC package contains functions that allows us to calculate the AUC:\n\nauc(Default$default_01,predict(default_fit_1,type=\"response\"))\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nArea under the curve: 0.948\n\n\nNote that the largest possible value for AUC is 1. Further, we expect a classifies that performs no better than change to have an AUC of 0.5. We see that as a classifier on the Deafault data logistic regression appears to perform well. However, we only computed the AUC for the training data and ultimately what we are interested in is how the model performs on test data. We will return to this issue later.\nAn important application of ROC curves and AUC is in comparing different types of classifiers. So, we will spend some time learning about other types of classifiers."
  },
  {
    "objectID": "lesson04/index.html#knn",
    "href": "lesson04/index.html#knn",
    "title": "Lesson 4",
    "section": "KNN",
    "text": "KNN\nAnother approach to classification (and also regression) is to use distance-based methods such as K Nearest Neighbors (KNN). The basic model assumption is that data points that are closer together are more likely to belong to the same class. To implement such models, we need to address two points:\n\nHow do we measure distance between data points?\nWhat is our decision rule for classifying a data point when it is close to multiple points from different classes.\n\nGiven a particular measure of distance, the KNN algorithm proceeds as follows:\n\nChoose a value \\(k\\) that is less than or equal to, but typically much less than the total size \\(n\\) of the observed data.\nFor a new value \\({\\bf x}\\), find the \\(k\\) data points that are closest to \\({\\bf x}\\).\nAssign to \\({\\bf x}\\) the majority class among its \\(k\\) nearest neighbors. Note that it may also be necessary to establish a rule for breaking ties.\n\nThere is a KNN demo associated with the textbook (Dinov 2018). View the KNN demo.\nLet’s work through some KNN examples together in a new R project.\nNote: An important application of KNN is to imputation of missing data.\nThere is one major issue with distance-based classification (or regression) methods. This is an issue known as the curse of dimensionality. The issue is basically that as the dimension of the space that data points belong to increases, the more sparse the data points are within the space. You will be asked to explore this issue further in the homework exercises."
  },
  {
    "objectID": "lesson04/index.html#multi-class-classifiers",
    "href": "lesson04/index.html#multi-class-classifiers",
    "title": "Lesson 4",
    "section": "Multi-class Classifiers",
    "text": "Multi-class Classifiers\nIn our notebook on KNN and other classification examples, we saw that KNN allows us to build classification models in which there are more than two levels for our categorical response variable. There are other approaches to multi-class classification problems other than KNN. For example, we can extend logistic regression using a so-called softmax coding. Suppose that our response variable \\(y\\) is categorical with \\(K\\) levels, then for \\(k=1,2,\\ldots, K\\), we model\n\\[\nP(y = k|{\\bf x}) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_{1} + \\cdots + \\beta_{kp}x_{p}}}{\\sum_{l=1}^{K}e^{\\beta_{l0} + \\beta_{l1}x_{1} + \\cdots + \\beta_{lp}x_{p}}}\n\\]\nWe refer to the right-hand-side of the last expression as a softmax function. The softmax function converts a vector of K real numbers into a probability distribution of K possible outcomes. Notice that for \\(k \\neq k'\\) we have\n\\[\n\\log\\left(\\frac{P(y = k|{\\bf x})}{P(y = k'|{\\bf x})} \\right) = (\\beta_{k0} - \\beta_{k'0}) + (\\beta_{k1} - \\beta_{k'1})x_{1} + \\cdots (\\beta_{kp} - \\beta_{k'p})x_{p}.\n\\]\nThus, the log-odds for any pair of of classes is a linear function of our predictor variables. Later in the course, we will utilize the softmax function in our discussion of deep learning methods for classification. Such a regression problem is class multinomial regression.\n\nMulti-class Classification Example Problem\nConsider again the penguins data set, the first few rows of which are displayed in Table 1.\n\n\n\n\nTable 1: The first few rows of the Palmer Penguins data set.\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n\n\n\n\n\n\n\n\nA problem for this data set is to use the various body measurements of penguins as a way to classify the penguin species. Let’s return to our classification examples project and attempt to model this data using multi-nomial regression."
  },
  {
    "objectID": "lesson05/index.html",
    "href": "lesson05/index.html",
    "title": "Lesson 5",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine the bootstrap method.\nDefine cross-validation and distinguish between different approaches to cross-validation.\nUse the methods in the rsample package to generate appropriate resamples.\nApply bootstrap and cross-validation methods to obtain relevant information about regression and classification models."
  },
  {
    "objectID": "lesson05/index.html#learning-objectives",
    "href": "lesson05/index.html#learning-objectives",
    "title": "Lesson 5",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine the bootstrap method.\nDefine cross-validation and distinguish between different approaches to cross-validation.\nUse the methods in the rsample package to generate appropriate resamples.\nApply bootstrap and cross-validation methods to obtain relevant information about regression and classification models."
  },
  {
    "objectID": "lesson05/index.html#readings-etc.",
    "href": "lesson05/index.html#readings-etc.",
    "title": "Lesson 5",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson:\n\nRead chapter 5 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 4 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on cross validation. View on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nWatch the “Spending your data budget” section of the following video. View on YouTube."
  },
  {
    "objectID": "lesson05/index.html#preparation-for-the-next-lesson",
    "href": "lesson05/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 5",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nFor the next lesson we will cover tree-based methods. To prepare for the next lesson, please do the following:\n\nRead chapter 8 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 8 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on decision trees. View on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nGo through the following two blog posts by Julia Silge:\n\nPredicting injuries for Chicago traffic crashes\nPredict availability in water sources with random forest models"
  },
  {
    "objectID": "lesson05/index.html#references",
    "href": "lesson05/index.html#references",
    "title": "Lesson 5",
    "section": "References",
    "text": "References\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Ventura 13.5.2\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-09-21\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4   2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson05/index.html#overview",
    "href": "lesson05/index.html#overview",
    "title": "Lesson 5",
    "section": "Overview",
    "text": "Overview\nMachine learning algorithms estimate parameters or other features of a model. It is important to assess the accuracy or uncertainty associated with any estimate. For the purposes of prediction, we are ultimately concerned with how a model performs on a previously unseen set of test data. However, as soon as the model sees the test data, it should not be used again. Thus, in choosing a good model we need to be able to estimate test error without directly use of the test set. Furthermore, we have seen that the specification of some types of models requires the selection of values for one or more hyperparameters.\nNotice that in both cases of model fitting and model assessment, there is something that we want to estimate and the value of the estimate will depend on the sampled data. For example, in model fitting we may estimate a parameter while in model assessment we will estimate the test error. From a statistical perspective, we can view whatever it is we are estimating as a random variable so that our problem becomes to learn as much as we can about the distribution of that random variable.\nResampling methods involve judiciously choosing multiple subsets of our data, typically the training set as a way to solve the problems outlined in the last paragraph. The bootstrap samples with replacement from the data in order to assess the accuracy or uncertainty associated with an estimate. Cross-validation (CV) holds out pieces of the data in order to assess the predictive accuracy of a model or to select hyperparameter values.\nResampling methods are computation intensive because they involve refitting or re-estimating a model or parameters many times. On the other hand, resampling methods can be used in many more situations than analytic procedures can. Further, with the increased power and speed of modern computers and efficient implementations of algorithms used for resampling, resampling methods have become indispensable tools in modern statistics and machine learning.\nThe R package rsample, which is part of the tidymodels family provides functions to create different types of resamples and facilitates their use in many analyses. The rsample package provides a set of methods that can be used for:\n\nResampling for estimating the sampling distribution of a statistic.\nEstimating model performance using a holdout set.\n\nIn this lesson, we will explain in detail the bootstrap and cross-validation resampling methods and learn how to use rsample for generating resamples. We will also see some use cases for the bootstrap and cross-validation."
  },
  {
    "objectID": "lesson05/index.html#bootstrap",
    "href": "lesson05/index.html#bootstrap",
    "title": "Lesson 5",
    "section": "Bootstrap",
    "text": "Bootstrap\nA common assumption of statistical models is that we have an independent and identically distributed sequence of random variables. Mathematically, \\(X_{1}, X_{2}, \\ldots , X_{n} \\sim F\\), where \\(F\\) is a (typically unknown) cumulative distribution function. The empirical distribution function (EDF) \\(\\hat{F}_{n}\\) is defined by\n\\[\n\\hat{F}_{n}(x) = \\frac{1}{n}\\sum_{i=1}^{n}I(X_{i} \\leq x)\n\\]\nwhere \\(I\\) is the indicator function we have seen before. The EDF satisfies for any fixed value \\(x\\):\n\n\\(\\text{E}[\\hat{F}_{n}(x)] = F(x)\\), and\n\\(\\text{Var}[\\hat{F}_{n}(x)] = \\frac{F(x)(1 - F(x))}{n}\\)\n\nFor a statistic, that is, any function of the sample \\(T_{n} = g(X_{1},X_{2},\\ldots, X_{n})\\) the bootstrap estimates the variance \\(\\text{Var}_{F}(T_{n})\\) by \\(\\text{Var}_{\\hat{F}_{n}}(T_{n})\\). This is what’s called a plug-in estimator. Here’s how it works in practice:\n\nDraw a sample \\(X_{1}^{\\ast}, \\ldots , X_{n}^{\\ast} \\sim \\hat{F}_{n}\\).\nCompute \\(T_{n}^{\\ast} = g(X_{1}^{\\ast}, \\ldots , X_{n}^{\\ast})\\).\nRepeat steps 1 and 2, \\(B\\) times to get \\(T_{n,1}^{\\ast}, \\ldots T_{n,B}^{\\ast}\\).\nDefine the bootstrap variance by\n\n\\[\n\\mathcal{v}_{\\text{boot}} = \\frac{1}{B}\\sum_{b=1}^{B}\\left(T_{n,b}^{\\ast} - \\frac{1}{B}\\sum_{r=1}^{B}T_{n,r}^{\\ast} \\right)^{2}\n\\] It can be shown that \\(\\mathcal{v}_{\\text{boot}}\\) converges in a specific sense to \\(\\text{Var}_{\\hat{F}_{n}}(T_{n})\\) as \\(B \\rightarrow \\infty\\). To do this, we need to simulate \\(\\hat{F}_{n}\\). Since \\(\\hat{F}_{n}\\) give probability \\(\\frac{1}{n}\\) to each data point, drawing \\(n\\) points at random from \\(\\hat{F}_{n}\\) is the same as drawing a sample of size \\(n\\) with replacement from the original data. So, we would replace step 1 above with\n1’. Draw \\(X_{1}^{\\ast}, \\ldots , X_{n}^{\\ast}\\) with replacement from \\(X_{1}, \\ldots , X_{n}\\).\nIntuitively, if a sample is representative of a population, then many samples with replacement from the sample should be representative of the sampling distribution. Figure 1 illustrates the bootstrap resampling process.\n\n\n\nFigure 1: An illustration of the bootstrap idea.\n\n\nLet’s look at a simple example by bootstraping the median of some sample data:\n\nset.seed(1234)\nsample_data &lt;- rnorm(45,mean = 5, sd = 2.75)\n\n(t_med &lt;- median(sample_data))\n\n[1] 3.594724\n\nmed_resamp &lt;- function(id_val){\n ts_med &lt;- median(sample(sample_data,replace=TRUE)) \n}\n\nB &lt;- 1000\n\nts_med_vals &lt;- map_dbl(1:B,med_resamp)\n\n(se_boot &lt;- sd(ts_med_vals))\n\n[1] 0.2782101\n\n\nWe can plot our bootstrapped median estimates to get an even better understanding of the variation of our statistic:\n\n\nCode\nts_meds_df &lt;- tibble(ts_meds = ts_med_vals)\n\nts_meds_df %&gt;%\n  ggplot(aes(x=ts_meds)) + \n  geom_histogram(color=\"white\",fill=\"orange\",bins=25) + \n  labs(x = \"Bootstrap median\")\n\n\n\n\n\nFigure 2: The distribution of our boootstrapped median estimates.\n\n\n\n\nIn Figure 2 we see that the mean of our bootstrap estimates is the value of the statistic for the original sample data.\nLater in this lesson we will see how to use the rsample package to efficiently generate the resamples needed for step 1’. Further, we will look at a detailed application of the bootstrap to a more complicated statistical model."
  },
  {
    "objectID": "lesson05/index.html#cross-validation",
    "href": "lesson05/index.html#cross-validation",
    "title": "Lesson 5",
    "section": "Cross-validation",
    "text": "Cross-validation\nThere are difference approaches to cross-validation but the most common method is \\(V\\)-fold cross-validation1. This method involves randomly dividing the training set into \\(V\\) groups called folds of approximately equal size. The first fold is treated as a validation set and the model is fit on the remaining \\(V-1\\) folds. Then, the error \\(E_{1}\\) is computed for the held out set. This procedure is repeated \\(V\\) times and the \\(V\\)-fold CV estimate for the test error is\n\\[\n\\text{CV}_{V} = \\frac{1}{V}\\sum_{i=1}^{k}E_{i}\n\\] Figure 3 illustrates \\(V\\)-fold cross-validation for \\(V=5\\).\n\n\n\nFigure 3: An illustration of the. idea of V-fold cross-validation.\n\n\nWhenever \\(V=1\\), we have what is known as leave one out cross-validation (LOOCV). The most common values for \\(V\\) are five and ten. In the next section, we will see how to efficiently create the folds for cross-validation using the rsample package. Then later, we will look at a detailed application of cross-validation to some machine learning models."
  },
  {
    "objectID": "lesson05/index.html#using-rsample",
    "href": "lesson05/index.html#using-rsample",
    "title": "Lesson 5",
    "section": "  Using rsample",
    "text": "Using rsample\nThe rsample package contains functions to create various types of “data splits” and can be used for purposes of resampling. Some examples of functions from rsample include:\n\ninitial_split - creates a single binary split of the data into a training set and testing set. We have seen this function before, it creates a so-called rsplit object.\n\n\ninitial_split(penguins)\n\n&lt;Training/Testing/Total&gt;\n&lt;258/86/344&gt;\n\n\n\ninitial_validation_split - creates a random three-way split of the data into a training set, a validation set, and a testing set. This can be used for very simple tuning tasks or in case you have a very large data set. It creates a rsplot object similar to initial_split.\n\n\ninitial_validation_split(penguins)\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;206/69/69/344&gt;\n\n\n\nbootstraps - creates samples that are the same size as the original data set that is made using replacement. This creates a data frame with a column of rsplit objects named splits.\n\n\nbootstraps(penguins,times=5)\n\n# Bootstrap sampling \n# A tibble: 5 × 2\n  splits            id        \n  &lt;list&gt;            &lt;chr&gt;     \n1 &lt;split [344/125]&gt; Bootstrap1\n2 &lt;split [344/136]&gt; Bootstrap2\n3 &lt;split [344/126]&gt; Bootstrap3\n4 &lt;split [344/140]&gt; Bootstrap4\n5 &lt;split [344/121]&gt; Bootstrap5\n\n\n\nvfold_cv - randomly splits the data into V groups of roughly equal size (called “folds”). Also creates a data frame with a column of rsplit objects named splits.\n\n\nvfold_cv(penguins,v=5)\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [275/69]&gt; Fold1\n2 &lt;split [275/69]&gt; Fold2\n3 &lt;split [275/69]&gt; Fold3\n4 &lt;split [275/69]&gt; Fold4\n5 &lt;split [276/68]&gt; Fold5\n\n\nNote: There are a couple of other similar functions such as loo_cv and mc_cv that you will be asked to explore in the homework.\nNote that resampled data sets created by rsample are directly accessible in a resampling object but do not contain much overhead in memory. Since the original data is not modified, R does not make an automatic copy. For example, creating 50 bootstraps of a data set does not create an object that is 50-fold larger in memory.\nTable 1 shows the results of a workflow involving rsample created with the following code:\n\n\nCode\nbootstraps(penguins,times=5) %&gt;%\n  mutate(analysis_df = map(splits, ~analysis(.x)),\n         assessment_df = map(splits, ~assessment(.x))) %&gt;%\n  unnest(analysis_df) %&gt;%\n  group_by(id) %&gt;%\n  summarise(boot_mean_bill_length_mm = mean(bill_length_mm ,na.rm=TRUE),\n            boot_median_bill_length_mm = median(bill_length_mm ,na.rm=TRUE),\n            boot_sd_bill_length_mm = sd(bill_length_mm ,na.rm=TRUE)) %&gt;%\n  kable()\n\n\n\n\nTable 1: Out put from an example workflow for rsample.\n\n\nid\nboot_mean_bill_length_mm\nboot_median_bill_length_mm\nboot_sd_bill_length_mm\n\n\n\n\nBootstrap1\n43.96268\n44.5\n5.380798\n\n\nBootstrap2\n44.22551\n45.1\n5.257893\n\n\nBootstrap3\n43.44575\n44.0\n5.331782\n\n\nBootstrap4\n43.95306\n44.5\n5.333140\n\n\nBootstrap5\n43.88692\n44.9\n5.473746\n\n\n\n\n\n\n\n\nEssentially what the code has done is to create 5 bootstrap samples from a data set, extract the data frame of each bootstrap resample, and then compute some statistics for a variable in the data frame of each bootstrap resample. Let’s experiment with this code in our own R session and work together on some further examples of using rsample."
  },
  {
    "objectID": "lesson05/index.html#footnotes",
    "href": "lesson05/index.html#footnotes",
    "title": "Lesson 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(V\\)-fold cross-validation is also commonly referred to as \\(k\\)-fold cross-validation but we avoid the \\(k\\) notation because \\(k\\) will appear as a hyperparameter for several of the machine learning algorithms we will discuss.↩︎"
  },
  {
    "objectID": "lesson06/index.html",
    "href": "lesson06/index.html",
    "title": "Lesson 6",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine decision tree and understand the splitting algorithm for tree-based regression and classification.\nDefine the concepts of bagging, boosting, and random forests.\nUse the tidymodels workflow to fit and tune various tree-based models."
  },
  {
    "objectID": "lesson06/index.html#learning-objectives",
    "href": "lesson06/index.html#learning-objectives",
    "title": "Lesson 6",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDefine decision tree and understand the splitting algorithm for tree-based regression and classification.\nDefine the concepts of bagging, boosting, and random forests.\nUse the tidymodels workflow to fit and tune various tree-based models."
  },
  {
    "objectID": "lesson06/index.html#readings-etc.",
    "href": "lesson06/index.html#readings-etc.",
    "title": "Lesson 6",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead chapter 8 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 8 of Statistical Learning with Math and R (Suzuki 2020).\nWatch the corresponding video lecture on decision trees. View on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nGo through the following two blog posts by Julia Silge:\n\nPredicting injuries for Chicago traffic crashes\nPredict availability in water sources with random forest models"
  },
  {
    "objectID": "lesson06/index.html#overview",
    "href": "lesson06/index.html#overview",
    "title": "Lesson 6",
    "section": "Overview",
    "text": "Overview\nTree-based models are a class of nonparametric algorithms that work by partitioning the feature space, that is, the predictors into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. Predictions are obtained by averaging response values in each region. Such divide-and-conquer methods can produce simple rules that are easy to interpret and visualize. However, simple decision trees typically lack in predictive performance compared to more complex algorithms. In order to address this, techniques such as bagging, boosting, and random forests have been developed. Each of these approaches involve fitting multiple trees which are combined in some way that results in improved accuracy but at a cost of some loss in interpretability."
  },
  {
    "objectID": "lesson06/index.html#preparation-for-the-next-lesson",
    "href": "lesson06/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 6",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nRead chapter 9 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 9 of Statistical Learning with Math and R (Suzuki 2020).\n\nWatch the following video lectures on support vector machines:\n\nView Supoprt Vector Classifier video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nView Supoprt Vector Classifiers in R video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nRead chapter 10 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\n\nWatch the following video lectures on neural networks:\n\nView Introduction to Neural Networks video on YouTube."
  },
  {
    "objectID": "lesson06/index.html#references",
    "href": "lesson06/index.html#references",
    "title": "Lesson 6",
    "section": "References",
    "text": "References\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-09\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0      2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3      2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3      2023-08-14 [1] CRAN (R 4.3.0)\n infer        * 1.0.5      2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2      2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3      2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0      2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1      2023-08-17 [1] CRAN (R 4.3.0)\n parttree     * 0.0.1.9004 2023-10-02 [1] Github (grantmcdermott/parttree@d2b60ac)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8      2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0      2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1      2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2      2023-08-23 [1] CRAN (R 4.3.0)\n vip          * 0.4.1      2023-08-21 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3      2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1      2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0      2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson06/index.html#basic-decision-trees",
    "href": "lesson06/index.html#basic-decision-trees",
    "title": "Lesson 6",
    "section": "Basic Decision Trees",
    "text": "Basic Decision Trees\n\nMotivating Example\nTo motivate decision trees, let’s consider a regression problem. The Hitters data from the ISLR2 package contains data on major league baseball players. We would like to use this data to predict player salaries based on information about the players and their performance. Let’s start by getting a glimpse of the data.\n\n\nRows: 322\nColumns: 20\n$ AtBat     &lt;int&gt; 293, 315, 479, 496, 321, 594, 185, 298, 323, 401, 574, 202, …\n$ Hits      &lt;int&gt; 66, 81, 130, 141, 87, 169, 37, 73, 81, 92, 159, 53, 113, 60,…\n$ HmRun     &lt;int&gt; 1, 7, 18, 20, 10, 4, 1, 0, 6, 17, 21, 4, 13, 0, 7, 3, 20, 2,…\n$ Runs      &lt;int&gt; 30, 24, 66, 65, 39, 74, 23, 24, 26, 49, 107, 31, 48, 30, 29,…\n$ RBI       &lt;int&gt; 29, 38, 72, 78, 42, 51, 8, 24, 32, 66, 75, 26, 61, 11, 27, 1…\n$ Walks     &lt;int&gt; 14, 39, 76, 37, 30, 35, 21, 7, 8, 65, 59, 27, 47, 22, 30, 11…\n$ Years     &lt;int&gt; 1, 14, 3, 11, 2, 11, 2, 3, 2, 13, 10, 9, 4, 6, 13, 3, 15, 5,…\n$ CAtBat    &lt;int&gt; 293, 3449, 1624, 5628, 396, 4408, 214, 509, 341, 5206, 4631,…\n$ CHits     &lt;int&gt; 66, 835, 457, 1575, 101, 1133, 42, 108, 86, 1332, 1300, 467,…\n$ CHmRun    &lt;int&gt; 1, 69, 63, 225, 12, 19, 1, 0, 6, 253, 90, 15, 41, 4, 36, 3, …\n$ CRuns     &lt;int&gt; 30, 321, 224, 828, 48, 501, 30, 41, 32, 784, 702, 192, 205, …\n$ CRBI      &lt;int&gt; 29, 414, 266, 838, 46, 336, 9, 37, 34, 890, 504, 186, 204, 1…\n$ CWalks    &lt;int&gt; 14, 375, 263, 354, 33, 194, 24, 12, 8, 866, 488, 161, 203, 2…\n$ League    &lt;fct&gt; A, N, A, N, N, A, N, A, N, A, A, N, N, A, N, A, N, A, A, N, …\n$ Division  &lt;fct&gt; E, W, W, E, E, W, E, W, W, E, E, W, E, E, E, W, W, W, W, W, …\n$ PutOuts   &lt;int&gt; 446, 632, 880, 200, 805, 282, 76, 121, 143, 0, 238, 304, 211…\n$ Assists   &lt;int&gt; 33, 43, 82, 11, 40, 421, 127, 283, 290, 0, 445, 45, 11, 151,…\n$ Errors    &lt;int&gt; 20, 10, 14, 3, 4, 25, 7, 9, 19, 0, 22, 11, 7, 6, 8, 0, 10, 1…\n$ Salary    &lt;dbl&gt; NA, 475.000, 480.000, 500.000, 91.500, 750.000, 70.000, 100.…\n$ NewLeague &lt;fct&gt; A, N, A, N, N, A, A, A, N, A, A, N, N, A, N, A, N, A, A, N, …\n\n\nWe see that there are some missing values for the Salary response variable so let’s remove the rows with the missing observations.\n\nHitters &lt;- Hitters %&gt;%\n  filter(!is.na(Salary))\n\nLet’s also look at the distribution for Salary.\n\n\nCode\nHitters %&gt;%\n  ggplot(aes(x=Salary)) + \n  geom_histogram(color=\"white\")\n\n\n\n\n\nFigure 1: Histrogram of salaries for players recorded in the Hitters data set from the ISLR2 package.\n\n\n\n\nSince the data is skewed, we should \\(\\log\\) transform the response variable.\n\n\nCode\nHitters &lt;- Hitters %&gt;%\n  mutate(Salary_log = log(Salary))\n\nHitters %&gt;%\n  ggplot(aes(x=Salary_log)) + \n  geom_histogram(color=\"white\") + \n  labs(x = \"Salary (log)\")\n\n\n\n\n\nFigure 2: Histrogram (\\(\\log\\) scale) of salaries for players recorded in the Hitters data set from the ISLR2 package.\n\n\n\n\nFor illustrative purposes, let’s suppose we want to model the \\(\\log\\) scaled salary as a response to the predictor variables Hits which records the number of by the player in their most recent year of play and Years which is the numbe of years the player has played. We first examine a plot of the relationship between these variables.\n\n\nCode\nHitters %&gt;%\n  ggplot(aes(x = Years, y = Hits, color=Salary_log)) + \n  geom_point() + \n  labs(color = \"Salary (log)\")\n\n\n\n\n\nFigure 3: Scatter plot of (\\(\\log\\) scale) of salaries versus hits for players recorded in the Hitters data set from the ISLR2 package.\n\n\n\n\nFor this problem, the basic decision tree algorithm for regression will separate the Years-Hits predictor or feature space (in this case the plane) into some number of distinct regions and then make predictions by averaging the \\(\\log\\) scaled salary values in each of the regions.\nThe question is, how to we divide up the predictor space?\n\n\nBasic Decison Tree Regression Algorithm\nTo build a regression tree, there are two basic steps:\n\nDivide the predictor space. Suppose there are \\(p\\) predictors so that \\({\\bf x} = (x_{1}, x_{2}, \\ldots , x_{p})\\). For the set of all possible predictor values, divide this up into \\(J\\) non-overlapping regions, \\(R_{1}, R_{2}, \\ldots ,R_{J}\\).\nFor every observation that falls into region \\(R_{j}\\) we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_{j}\\).\n\nThe question is, how do we construct the regions \\(R_{1}, R_{2}, \\ldots ,R_{J}\\)? A typical approach is to divide the predictor space into boxes, that is, each \\(R_{j}\\) will be a set of the form \\((a_{1j}, b_{1j}) \\times (a_{2j},b_{2j}) \\times \\cdots \\times (a_{pj},b_{pj})\\). Then the goal is to find boxes that minimize\n\\[\n\\text{RSS} = \\sum_{j=1}^{J}\\sum_{i \\in R_{j}}(y_{i} - \\bar{y}_{R_{j}})^2\n\\]\nThere is a major challenge to minimizing the RSS in this setting:\n\nIt is infeasible to consider every possible partition of the space of predictors into \\(J\\) boxes.\n\nInstead of an exhaustive search, the typical approach is what is known as recursive binary splitting. Here is how it works:\n\nSelect the predictor \\(x_{j}\\) and a cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{{\\bf x} | x_{j} &lt; s\\}\\) and \\(\\{{\\bf x} | x_{j} \\geq s\\}\\) produces the greatest possible reduction in RSS.\nRepeat the process, but this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. This will result in three regions and we attempt to split one of those, etc. The process continues until a stopping criterion is reached. This algorithm is an example of a greedy algorithm because at each step of the tree-building process, the best split is made at that step.\n\nFigure 4 shows an example of splitting applied to the Hitters data set from the ISLR2 package.\n\n\n\nFigure 4: A three-region partition for the Hitters data set from the ISLR2 package.\n\n\nWe can represent this result by the tree diagram shown in Figure 5.\n\n\n\nFigure 5: The tree diagram for the split shown in Figure 4.\n\n\nOnce the regions \\(R_{1}, R_{2}, \\ldots, R_{J}\\) have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\nHere is an example in R using the rpart package that fits and plots a similar decision tree model:\n\nhitters_dt &lt;- rpart::rpart(Salary_log ~ Hits + Years,\n                           data=Hitters,\n                           method=\"anova\",\n                           control=list(cp = 0.1,maxdepth=2,\n                                        minsplit=20,xval = 0))\n\nrpart.plot::rpart.plot(hitters_dt)\n\n\n\n\nFigure 6 shows the regions and predictions from the decision tree obtained with the rpart function. This figure is obtained by using functions from the parttree package.\n\n\nCode\nHitters %&gt;%\n  ggplot(aes(x=Years,y=Hits)) + \n  geom_parttree(data=hitters_dt,aes(fill = Salary_log),alpha=0.3) + \n  geom_point(aes(color=Salary_log)) +\n  scale_colour_viridis_c(aesthetics = c(\"color\", \"fill\"))\n\n\n\n\n\nFigure 6: The Hitters data from Figure 4 shows with the regions and predictions from the decision tree obtained with the rpart function.\n\n\n\n\nThe vip package allows us to visualize the variable importance scores for the predictors in a decision tree model. That is, a measure of the decrease in RSS due to splits over a given predictor. Figure 7 shows the variable importance scores for the predictors in the Hitters data based on the decision tree obtained with the rpart function.\n\n\nCode\nvip(hitters_dt,aesthetics = list(fill = \"midnightblue\", alpha = 0.8))\n\n\n\n\n\nFigure 7: The variable importance scores for the predictors in the Hitters data based on the decision tree obtained with the rpart function.\n\n\n\n\n\n\nPruning\nIt is easy to overfit a decision tree to training data. An approach to dealing with this issue is to use a technique known as pruning. The idea is to build an initially large tree \\(T_{0}\\), and then prune it back to a subtree that leads to the lowest test error rate. In order to do so, we will need to incorporate some way to estimate test error.\nThe approach we take, known as cost complexity pruning is to consider a sequence of subtrees indexed by a non-negative parameter \\(\\alpha\\). For each value of \\(\\alpha\\), there corresponds a subtree \\(T \\subset T_{0}\\) that minimizes the following expression:\n\\[\n\\sum_{m=1}^{|T|} \\sum_{i:\\ x_{i} \\in R_{m}} (y_{i} - \\bar{y}_{R_{m}})^2 + \\alpha |T|\n\\]\nHere, \\(|T|\\) is the number of terminal nodes of the subtree \\(T\\), \\(R_{m}\\) is the box corresponding to the \\(m\\)th terminal node, and \\(\\bar{y}_{R_{m}}\\) is the predicted response associated with \\(R_{m}\\). The tuning parameter \\(\\alpha\\) is a penalty that controls a trade-off between the subtree’s complexity and its fit to the training data. When \\(\\alpha = 0\\), then the subtree will be \\(T_{0}\\). As \\(\\alpha\\) increases, the cost of having more terminal nodes is higher so the higher \\(\\alpha\\), the smaller \\(T\\) will tend to be.\nTable 1 summarizes our algorithm for building regression tree by pruning.\n\n\nTable 1: Regression tree algorithm that uses pruning.\n\n\n\n\n\nAlgorithm: Building a Regression Tree\n\n\n\n\n1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n\n\n2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\n\n\n3. Use \\(V\\)-fold cross-validation to choose \\(\\alpha\\). That is, divide the training observations into \\(V\\) folds. For each \\(k=1,\\ldots , V\\):\n\n\n(a) Repeat Steps 1 and 2 on all but the \\(v\\)th fold of the training data.\n\n\n(b) Evaluate the mean squared prediction error on the data in the left-out \\(v\\)th fold, as a function of \\(\\alpha\\).\n\n\nAverage the results for each value of \\(\\alpha\\), and pick \\(\\alpha\\) to minimize the average error.\n\n\n4. Return the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\).\n\n\n\n\nIn the function rpart briefly introduced earlier, one can control the complexity parameter through setting a value named cp and the maximum depth with maxdepth. Let’s see what happens if we decrease cp and increase maxdepth in comparison with our earlier tree for the Hitters data.\n\nhitters_dt_2 &lt;- rpart::rpart(Salary_log ~ Hits + Years,\n                           data=Hitters,\n                           method=\"anova\",\n                           control=list(cp = 0.01,maxdepth=20,\n                                        minsplit=10,xval = 10))\n\nrpart.plot::rpart.plot(hitters_dt_2)\n\n\n\n\nWe can also plot the results of the cross-validation:\n\nrpart::plotcp(hitters_dt_2)\n\n\n\n\nFigure 8: The results of cross-validation to choose a value for the complexity parameter \\(\\alpha\\).\n\n\n\n\nNotice from Figure 8 we see that there is little gain in decreasing the complexity parameter, that is, the value we denoted by \\(\\alpha\\) beyond a certain point. Clearly, pruning has occurred because we could have grown a tree with up to 20 splits. In fact, let’s examine the larger tree:\n\nhitters_dt_3 &lt;- rpart::rpart(Salary_log ~ Hits + Years,\n                           data=Hitters,\n                           method=\"anova\",\n                           control=list(cp = 0.0,maxdepth=20,\n                                        minsplit=5,xval = 0))\n\nrpart.plot::rpart.plot(hitters_dt_3)\n\n\n\n\n\n\nClassification Trees\nClassification trees work very similarly to regression trees. The main difference is that RSS cannot be used in making splits since the response is categorical. There are three alternatives used in place of RSS to fit classification trees:\n\nClassification error rate: \\(E = 1 - \\max_{k}(\\hat{p}_{mk})\\),\nThe Gini index: \\(G = \\sum_{k=1}^{K}\\hat{p}_{mk}(1 - \\hat{p}_{mk})\\),\nEntropy: \\(D = -\\sum_{k=1}^{K}\\hat{p}_{mk}\\log(\\hat{p}_{mk})\\),\n\nwhere \\(\\hat{p}_{mk}\\) is the proportion of training observations in the \\(m\\)th region that are from the \\(k\\)th class.\nBoth the Gini index and entropy provide a measure of node purity, a value that measures the extent to which a node in a tree contains observations from a single class. A very small value for the node purity would correspond to a situation where a node contains predominantly observations from a single class.\nLet’s examine a small case study for classification trees. Consider the data Heart which we must read in from a .csv file. These data contain a binary outcome AHD for 303 patients who presented with chest pain. An outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease. There are 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements. Let’s import and glimpse the data.\n\nHeart &lt;- read_csv(\"https://www.statlearning.com/s/Heart.csv\")\n\nglimpse(Heart)\n\nRows: 303\nColumns: 15\n$ ...1      &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ Age       &lt;dbl&gt; 63, 67, 67, 37, 41, 56, 62, 57, 63, 53, 57, 56, 56, 44, 52, …\n$ Sex       &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, …\n$ ChestPain &lt;chr&gt; \"typical\", \"asymptomatic\", \"asymptomatic\", \"nonanginal\", \"no…\n$ RestBP    &lt;dbl&gt; 145, 160, 120, 130, 130, 120, 140, 120, 130, 140, 140, 140, …\n$ Chol      &lt;dbl&gt; 233, 286, 229, 250, 204, 236, 268, 354, 254, 203, 192, 294, …\n$ Fbs       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, …\n$ RestECG   &lt;dbl&gt; 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, …\n$ MaxHR     &lt;dbl&gt; 150, 108, 129, 187, 172, 178, 160, 163, 147, 155, 148, 153, …\n$ ExAng     &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ Oldpeak   &lt;dbl&gt; 2.3, 1.5, 2.6, 3.5, 1.4, 0.8, 3.6, 0.6, 1.4, 3.1, 0.4, 1.3, …\n$ Slope     &lt;dbl&gt; 3, 2, 2, 3, 1, 1, 3, 1, 2, 3, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, …\n$ Ca        &lt;dbl&gt; 0, 3, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ Thal      &lt;chr&gt; \"fixed\", \"normal\", \"reversable\", \"normal\", \"normal\", \"normal…\n$ AHD       &lt;chr&gt; \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"Y…\n\n\nLet’s fit a large classification tree without pruning.\n\nheart_dt &lt;- rpart::rpart(AHD ~ .,\n                           data=Heart,\n                           method=\"class\",\n                           control=list(cp = 0.0,xval=0))\n\nrpart.plot::rpart.plot(heart_dt)\n\n\n\n\nNow let’s include some pruning.\n\nheart_dt_2 &lt;- rpart::rpart(AHD ~ .,\n                           data=Heart,\n                           method=\"class\",\n                           control=list(cp = 0.032,xval=10))\n\nrpart.plot::rpart.plot(heart_dt_2)\n\n\n\n\nWe can see the cross-validation results.\n\nrpart::plotcp(heart_dt_2)\n\n\n\n\n\n\nAdvantages and Disadvantages of Trees\nTable 2 lists some of the advantages and disadvantages associated with the use of decision trees.\n\n\nTable 2: Advantages and disadvantages of decision trees.\n\n\nDescription\nAdvantage or Disadvantage\n\n\n\n\nEasy to explain\nAdvantage\n\n\nMirror decision-making\nAdvantage\n\n\nGraphical display\nAdvantage\n\n\nEasily handle categorical predictors\nAdvantage\n\n\nLacking in predictive accuracy\nDisadvantage\n\n\nNon-robust\nDisadvantage"
  },
  {
    "objectID": "lesson06/index.html#bagging",
    "href": "lesson06/index.html#bagging",
    "title": "Lesson 6",
    "section": "Bagging",
    "text": "Bagging\nDecision trees tend to have low bias but high variance. There is a fairly general trick that can be used to reduce variance, that is, by averaging. Here’s an illustration of the principle. Suppose we have an i.i.d. sequence \\(Z_{1}, Z_{2}, \\ldots , Z_{n}\\) each with variance \\(\\sigma^{2}\\). Then, the variance of the mean \\(\\bar{Z} = \\frac{1}{n}\\sum_{i=1}^{n}Z_{n}\\) is \\(\\text{Var}[\\bar{Z}] = \\frac{\\sigma^2}{n}\\) and \\(\\frac{\\sigma^2}{n} &lt; \\sigma^{2}\\). Thus, averaging tends to reduce variance.\nIn the context of machine learning, we can in principle fit a model on \\(B\\) separate training sets to get \\(\\hat{f}_{1}({\\bf x}), \\hat{f}_{2}({\\bf x}), \\ldots , \\hat{f}_{B}({\\bf x})\\) and the make a prediction\n\\[\n\\hat{f}_{\\text{avg}}({\\bf x}) = \\frac{1}{B}\\sum_{i=1}^{B}\\hat{f}_{i}({\\bf x})\n\\] which we expect to reduce the variance in predictions. However, this is impractical because it requires access to many training sets. Instead, we can bootstrap. We generate \\(B\\) bootstrap resamples, fit models \\(\\hat{f}_{1}^{\\ast}({\\bf x}), \\hat{f}_{2}^{\\ast}({\\bf x}), \\ldots , \\hat{f}_{B}^{\\ast}({\\bf x})\\) on each of the bootstrap resamples and then compute\n\\[\n\\hat{f}_{\\text{bag}}({\\bf x}) = \\frac{1}{B}\\sum_{i=1}^{B}\\hat{f}_{i}^{\\ast}({\\bf x})\n\\] This is know as bootstrap aggregation or bagging. Bagging is especially useful in the setting of decision trees although it can be applied to pretty much any type of model.\nAs described so far, bagging has been presented in the context of regression but it works in the context of classification too. The main difference is that rather than averaging, we take a majority vote to make predictions.\nNote: A single bootstrap resample does not typically make use of all the observations in the training set. Those observations in a bootstrap resample that are not included are called the out-of-bag (OOB) observations. This is useful because the OOB sets can be used to estimate test error without the need for cross-validation.\nWe will explore bagging in applications together in an RStudio project."
  },
  {
    "objectID": "lesson06/index.html#random-forests",
    "href": "lesson06/index.html#random-forests",
    "title": "Lesson 6",
    "section": "Random Forests",
    "text": "Random Forests\nRandom forests are a modification of bagged decision trees that build a large collection of de-correlated trees to further improve predictive performance. They have become a very popular “out-of-the-box” or “off-the-shelf” learning algorithm that enjoys good predictive performance with relatively little hyperparameter tuning.\nRandom forests are built using the same fundamental principles as decision trees and bagging. Bagging trees introduces a random component into the tree building process by building many trees on bootstrapped copies of the training data. Bagging then aggregates the predictions across all the trees; this aggregation reduces the variance of the overall procedure and results in improved predictive performance. However, simply bagging trees results in tree correlation that limits the effect of variance reduction.\nRandom forests help to reduce tree correlation by injecting more randomness into the tree-growing process. More specifically, while growing a decision tree during the bagging process, random forests perform split-variable randomization where each time a split is to be performed, the search for the split variable is limited to a random subset of \\(m_{\\text{try}}\\) of the original \\(p\\) features. Typical default values are \\(m_{\\text{try}} = \\frac{p}{3}\\) for regression and \\(m_{\\text{try}} = \\sqrt{p}\\) for classification but ideally \\(m_{\\text{try}}\\) should be considered a hyperparameter whose value needs to be tuned using cross-validation.\nWe will explore random forests in applications together in an RStudio project."
  },
  {
    "objectID": "lesson06/index.html#boosting",
    "href": "lesson06/index.html#boosting",
    "title": "Lesson 6",
    "section": "Boosting",
    "text": "Boosting\nBoosting works in a similar way to bagging, except that the trees are grown sequentially. That is, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set. Boosting is described in Table 3.\n\n\nTable 3: The booting algorithm described for regression trees.\n\n\n\n\n\nAlgorithm: Boosting for Regression Trees\n\n\n\n\n1. Set \\(\\hat{f}({\\bf x}) = 0\\) and \\(r_{i}=y_{i}\\) for all \\(i\\) in the training set.\n\n\n2. For \\(b=1,2,\\ldots, B\\), repeat:\n\n\n(a) Fit a tree \\(\\hat{f}_{b}\\) with \\(d\\) splits (\\(d+1\\) terminal nodes) to the training data \\((X,r)\\).\n\n\n(b) Update \\(\\hat{f}\\) by adding in a shrunken version of the new tree: \\(\\hat{f}({\\bf x}) + \\lambda \\hat{f}_{b} \\rightarrow \\hat{f}({\\bf x})\\).\n\n\n(c) Update the residuals \\(r_{i} - \\lambda \\hat{f}_{b} \\rightarrow r_{i}\\).\n\n\n3. Output the boosted model, \\(\\hat{f}({\\bf x}) = \\sum_{b=1}^{B}\\hat{f}_{b}({\\bf x})\\)."
  },
  {
    "objectID": "lesson06/index.html#summary",
    "href": "lesson06/index.html#summary",
    "title": "Lesson 6",
    "section": "Summary",
    "text": "Summary\nIn this lesson, we have covered the various tree-based methods:\n\nBasic decision trees.\nBootstrap aggregating (bagging) trees.\nRandom forests.\nBoosting.\n\nTree-based methods and random forests and boosting in particular are among the most popular and commonly used supervised machine learning methods in use today. It is important to note that fitting tree-based models involves the tuning of hyperparameters, some of which we have not touched on in the examples given in lecture. To get a better feel for hyperparameter tuning for tree-base models, it is highly recommended that you work through one or more of the following blog posts by Julia Silge:\n\nHyperparameter tuning and food consumption\nTuning random forest hyperparameters with trees data\nTune XGBoost with tidymodels and beach volleyball\nBagging with tidymodels and astronaut missions\nTune random forests for IKEA prices\nTune and interpret decision trees for wind turbines\nUse racing methods to tune xgboost models and predict home runs\nTune xgboost models with early stopping to predict shelter animal status\nTune an xgboost model with early stopping and childcare costs"
  },
  {
    "objectID": "lesson07/index.html",
    "href": "lesson07/index.html",
    "title": "Lesson 7",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe the structure of a neural network and explain the role of activation functions, loss functions, optimization algorithms, and regularization in neural networks and deep learning.\nImplement the gradient descent algorithm for optimization of simple functions.\nImplement a neural network in R using packages such as brulee, keras or torch."
  },
  {
    "objectID": "lesson07/index.html#learning-objectives",
    "href": "lesson07/index.html#learning-objectives",
    "title": "Lesson 7",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe the structure of a neural network and explain the role of activation functions, loss functions, optimization algorithms, and regularization in neural networks and deep learning.\nImplement the gradient descent algorithm for optimization of simple functions.\nImplement a neural network in R using packages such as brulee, keras or torch."
  },
  {
    "objectID": "lesson07/index.html#readings-etc.",
    "href": "lesson07/index.html#readings-etc.",
    "title": "Lesson 7",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead chapter 10 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\nRead section 7.1 from Mathematics for Machine Learning (Deisenroth, Faisal, and Ong 2020). This book is freely available online. View the book.\n\nWatch the following video lectures on neural networks:\n\nView Introduction to Neural Networks video on YouTube."
  },
  {
    "objectID": "lesson07/index.html#overview",
    "href": "lesson07/index.html#overview",
    "title": "Lesson 7",
    "section": "Overview",
    "text": "Overview\nDeep learning is an active area of research in machine learning and artificial intelligence and neural networks are the foundation of deep learning. In this lesson, we will introduce neural networks and discuss how they are used in deep learning.\nNeural networks are a class of machine learning models that are inspired by the structure of the brain. They are composed of a series of layers of neurons that are connected to each other. Each artificial neuron is a simple computational unit that takes in a set of inputs, performs a computation, and produces an output. The output of one neuron is then used as the input to the next neuron. The first layer of artificial neurons is called the input layer and the last layer of neurons is called the output layer. The layers in between the input and output layers are called hidden layers. The number of hidden layers in a neural network is called the depth of the network. The number of neurons in each layer is called the width of the network. Figure 1 shows a neural network with one hidden layer consisting of 4 neurons or nodes. Later we will develop notation to describe neural networks mathematically. From here on out we will ignore the biological analogy that is the historical origin of neural networks and focus on the mathematical model.\n\n\n\nFigure 1: A neural network with a single hidden layer consisting of four neurons or nodes.\n\n\nNeural networks are conceptually simple but the mathematical details can be confusing. The general idea is that a neural network takes an input of \\(p\\) predictor variables \\(X = (X_{1},X_{2},\\ldots , X_{p})\\) and builds a nonlinear function \\(f(X)\\) to predict the response \\(Y\\). What distinguishes neural networks for other nonlinear methods is the particular structure of the model function \\(f\\).\n\nExploring a Neural Network Interactively\nIn order to develop some intuition, we will start by exploring an interactive visualization of a neural network via the Neural Network Playground website. Visit the Neural Network Playground.\nThe visualization allows you to create a neural network and then train it on a dataset. The dataset can be a classification problem or a regression problem. The visualization allows you to change the activation function, the number of hidden layers, the number of neurons in each layer, and the learning rate. These are components related to the training of a network that we will define in detail later.\n\n\nExploring Neural Networks in R\nLet’s also take a look at a simple neural network in R. The brulee package provides a simple interface via a function brulee_mlp() (it’s a good idea to skim the brulee_mlp documentation) for creating neural networks in R and uses the tidymodels framework for modeling. The code, available via this GitHub repo, creates a neural network with one hidden layer and trains it on the penguins dataset. The repository also contains a script with an example of tuning a neural network with a single hidden layer. Let’s examine this together in an RStudio project.\n\n\nSuggestions for Further Reading\nHere are some additional resources on neural networks and deep learning that cover various aspects of the field that we do not have time to go over in this course:\n\nFor historical context, see Thinking Machines: The Quest for Artificial Intelligence–and Where It’s Taking Us Next by Dormehl or Deep Learning by Kelleher.\nFor an excellent overview of the types of problems that neural networks and deep learning are well-suited for, see (Krohn, Beyleveld, and Bassens 2019).\nFor a more detailed introduction to neural networks, see (Goodfellow, Bengio, and Courville 2016). This book is accessible online, view the book."
  },
  {
    "objectID": "lesson07/index.html#support-vector-machine-approaches",
    "href": "lesson07/index.html#support-vector-machine-approaches",
    "title": "Lesson 7",
    "section": "Support Vector Machine Approaches",
    "text": "Support Vector Machine Approaches\n\nMaximal Marginal Cllassifier\n\n\n\nFigure 1: Left: Separating hyperplanes for binary data. Right: The decision rule based on a separating hyperplane.\n\n\n\n\n\nFigure 2: The solid lines shows the maximal margin hyperplane. The margin is the distance between the two dotted lines."
  },
  {
    "objectID": "lesson07/index.html#preparation-for-the-next-lesson",
    "href": "lesson07/index.html#preparation-for-the-next-lesson",
    "title": "Lesson 7",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nRead chapter 10 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\n\nWatch the following video lectures on neural networks:\n\nView Introduction to Neural Networks video on YouTube."
  },
  {
    "objectID": "lesson07/index.html#references",
    "href": "lesson07/index.html#references",
    "title": "Lesson 7",
    "section": "References",
    "text": "References\n\n\nDeisenroth, Marc Peter, A Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge University Press.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nKrohn, Jon, Grant Beyleveld, and Aglaé Bassens. 2019. Deep Learning Illustrated. Addison-Wesley Professional.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-20\n pandoc   3.1.8 @ /opt/homebrew/bin/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)\n latex2exp    * 0.9.6   2022-11-28 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "ml_models/index.html",
    "href": "ml_models/index.html",
    "title": "Machine Learning Models",
    "section": "",
    "text": "Machine learning (ML) as we have defined it involves estimating a (mathematical) function and we divide problems into two general types: supervised and unsupervised learning. In supervised learning, we have a set of data that we use to train a model to predict a target variable. In unsupervised learning, we have a set of data that we use to train a model to find patterns in the data. In this chapter, we will focus on supervised learning. Further, we separate supervised learning into two types: regression and classification. In regression, the target variable is continuous. In classification, the target variable is categorical. We also consider two types of applications for machine learning: prediction and inference. In prediction, we are interested in predicting the target variable. In inference, we are interested in understanding the relationship between the target variable and the predictors.\n\n\nHere we provide lists of the most common machine learning models, separating the lists into one for supervised methods and one for unsupervised methods. Remember that the choice of the machine learning model depends on the specific problem, the nature of the data, and the trade-off between interpretability and predictive performance. It’s often a good practice to experiment with multiple models and evaluate their performance to select the most suitable one for a given task.\n\n\n\nLinear Regression:\n\nSummary: Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.\nCharacteristics:\n\nSuitable for regression tasks (predicting continuous numeric values).\nAssumes a linear relationship between predictors and the target.\nSimple and interpretable.\n\n\nLogistic Regression:\n\nSummary: Logistic regression is used for binary classification, modeling the probability that an instance belongs to a particular class.\nCharacteristics:\n\nSuitable for binary classification tasks.\nUses the logistic function to model probabilities.\nProvides probabilities and interpretable coefficients.\n\n\nDecision Trees:\n\nSummary: Decision trees divide the data into subsets based on the most significant attributes, making them suitable for both classification and regression tasks.\nCharacteristics:\n\nNon-linear and can capture complex relationships.\nProne to overfitting but can be regularized.\nEasily interpretable.\n\n\nRandom Forest:\n\nSummary: Random forests are an ensemble of decision trees that improve predictive accuracy and reduce overfitting.\nCharacteristics:\n\nCombines multiple decision trees for better performance.\nHandles feature importance and reduces variance.\nWorks well for classification and regression.\n\n\nSupport Vector Machines (SVM):\n\nSummary: SVMs aim to find a hyperplane that best separates data points into different classes.\nCharacteristics:\n\nEffective for binary classification and can be extended to multiclass.\nUses kernel functions for non-linear separations.\nGood for high-dimensional data.\n\n\nK-Nearest Neighbors (K-NN):\n\nSummary: K-NN assigns a class to a data point based on the majority class among its k-nearest neighbors in feature space.\nCharacteristics:\n\nSimple and intuitive.\nCan handle both classification and regression.\nSensitive to the choice of k.\n\n\nNaive Bayes:\n\nSummary: Naive Bayes is a probabilistic classifier based on Bayes’ theorem, assuming that features are conditionally independent.\nCharacteristics:\n\nEfficient and suitable for text classification.\nAssumes feature independence (naive assumption).\nWorks well for high-dimensional data.\n\n\nGradient Boosting Machines (GBM):\n\nSummary: GBMs build an ensemble of weak learners (usually decision trees) to create a strong predictive model.\nCharacteristics:\n\nCombines multiple weak learners for high accuracy.\nHandles regression and classification tasks.\nProne to overfitting, but can be regularized.\n\n\nNeural Networks (Deep Learning):\n\nSummary: Neural networks, especially deep learning models, are highly flexible and can model complex relationships in data.\nCharacteristics:\n\nExtremely powerful for various tasks.\nRequires large amounts of data and computational resources.\nInterpretability can be a challenge for deep models.\n\n\nEnsemble Methods:\n\nSummary: Ensemble methods combine multiple base models to improve predictive performance.\nCharacteristics:\n\nInclude bagging (e.g., Random Forest), boosting (e.g., AdaBoost), and stacking.\nOften more robust and accurate than individual models.\nHandle various types of data and tasks.\n\n\n\n\n\n\n\nK-Means Clustering:\n\nSummary: K-Means is a clustering algorithm that partitions data into clusters based on similarity.\nCharacteristics:\n\nUnsupervised clustering.\nAssigns data points to clusters with similar attributes.\nRequires specifying the number of clusters (k).\n\n\nHierarchical Clustering:\n\nSummary: Hierarchical clustering builds a tree of clusters, revealing hierarchical relationships.\nCharacteristics:\n\nCreates a tree-like structure of nested clusters.\nAgglomerative (bottom-up) or divisive (top-down) approaches.\nNo need to specify the number of clusters in advance.\n\n\nPrincipal Component Analysis (PCA):\n\nSummary: PCA is a dimensionality reduction technique that captures the most important features in data.\nCharacteristics:\n\nReduces the dimensionality of data while preserving variance.\nUsed for feature selection and visualization.\nFinds orthogonal linear combinations of features (principal components).\n\n\nIndependent Component Analysis (ICA):\n\nSummary: ICA separates a multivariate signal into additive, independent components.\nCharacteristics:\n\nUsed for blind source separation and feature extraction.\nAssumes that observed data is a linear combination of independent sources.\nApplied in signal processing and neuroscience.\n\n\nApriori Algorithm:\n\nSummary: Apriori is used for association rule mining in transaction data.\nCharacteristics:\n\nDiscovers frequent itemsets in transaction databases.\nUsed for market basket analysis and recommendation systems.\nGenerates association rules (if item A is bought, item B is also likely to be bought).\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n\nSummary: DBSCAN clusters data points based on density and identifies noise points.\nCharacteristics:\n\nAutomatically detects clusters of varying shapes and sizes.\nSuitable for data with irregular densities.\nDoes not require specifying the number of clusters in advance.\n\n\nGaussian Mixture Model (GMM):\n\nSummary: GMM models data as a mixture of Gaussian distributions.\nCharacteristics:\n\nAllows modeling complex data distributions.\nUseful for density estimation and clustering.\nCan be used for both soft and hard clustering.\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE):\n\nSummary: t-SNE is a dimensionality reduction technique known for preserving local structure in data.\nCharacteristics:\n\nUsed for high-dimensional data visualization.\nEffective for visualizing clusters and similarities.\nNon-linear dimensionality reduction.\n\n\nAutoencoders:\n\nSummary: Autoencoders are neural networks used for unsupervised feature learning.\nCharacteristics:\n\nLearn data representations by encoding and decoding input data.\nUsed for dimensionality reduction, denoising, and anomaly detection.\nNeural network architecture.\n\n\nLatent Dirichlet Allocation (LDA):\n\nSummary: LDA is a probabilistic model used for topic modeling in text data.\nCharacteristics:\n\nIdentifies topics in a collection of documents.\nAssumes documents are mixtures of topics, and words are mixtures of topic-specific words.\nWidely used in natural language processing.\n\n\n\n\n\n\n\nThe tidymodels package in R is an integrated ecosystem of packages designed to streamline the process of creating, evaluating, and deploying machine learning models while adhering to tidy data principles. The tidymodels framework follows a structured and consistent approach to machine learning, making it easier for data scientists and analysts to work with data and build predictive models. Here’s a brief description of the key components of the tidymodels package:\n\ntidyverse Integration:\n\nDescription: tidymodels seamlessly integrates with the popular tidyverse suite of packages, allowing for the use of tidy data frames and other tidy tools.\n\nparsnip:\n\nDescription: The parsnip package defines a common interface for specifying machine learning models, making it easier to work with different modeling engines.\n\nrecipes:\n\nDescription: The recipes package provides a systematic way to define and preprocess feature engineering steps for your data, creating a “recipe” that includes data preprocessing and variable transformations.\n\ntune:\n\nDescription: The tune package offers tools for hyperparameter tuning, allowing you to optimize model performance by systematically searching for the best hyperparameters.\n\nworkflows:\n\nDescription: The workflows package simplifies the process of building, tuning, and evaluating models by combining models, recipes, and tuning into a unified workflow.\n\nyardstick:\n\nDescription: The yardstick package provides a wide range of functions for model evaluation, including metrics for classification, regression, and survival analysis tasks.\n\nrsample:\n\nDescription: The rsample package is for resampling and creating data splits, essential for tasks such as cross-validation and bootstrapping.\n\nbroom:\n\nDescription: The broom package helps tidy up the results of model fits, making it easy to extract coefficients, predictions, and other model-related information in a tidy data format.\n\nCommunity and Extensibility:\n\nDescription: The tidymodels ecosystem has a growing community of users and contributors. It supports the creation of custom modeling engines, extending the framework to new algorithms.\n\nReproducibility and Best Practices:\n\nDescription: tidymodels promotes best practices in machine learning, emphasizing tidy data principles, clear model specification, and reproducibility.\n\n\nThe parsnip package in the tidymodels family allows one to specify a variety of supervised machine learning models. Here is a list of some of the parsnip models:\n\nlinear_reg:\n\nDescription: Linear Regression Model\nType: Regression\nDocumentation: linear_reg Documentation\n\nlogistic_reg:\n\nDescription: Logistic Regression Model\nType: Classification\nDocumentation: logistic_reg Documentation\n\ndecision_tree:\n\nDescription: Decision Tree Model\nType: Both (Regression and Classification)\nDocumentation: decision_tree Documentation\n\nrand_forest:\n\nDescription: Random Forest Model\nType: Both (Regression and Classification)\nDocumentation: rand_forest Documentation\n\nsvm_rbf:\n\nDescription: Support Vector Machine with Radial Basis Function Kernel Model\nType: Both (Regression and Classification)\nDocumentation: svm_rbf Documentation\n\nsvm_linear:\n\nDescription: Support Vector Machine with Linear Kernel Model\nType: Both (Regression and Classification)\nDocumentation: svm_linear Documentation\n\nnearest_neighbor:\n\nDescription: k-Nearest Neighbors Model\nType: Both (Regression and Classification)\nDocumentation: nearest_neighbor Documentation\n\nnaive_Bayes:\n\nDescription: Naive Bayes Model\nType: Classification\nDocumentation: naive_Bayes Documentation\n\nboost_tree:\n\nDescription: Gradient Boosting Model\nType: Both (Regression and Classification)\nDocumentation: boost_tree Documentation\n\nmlp:\n\nDescription: Multilayer Perceptron (Neural Network) Model\nType: Both (Regression and Classification)\nDocumentation: mlp Documentation"
  },
  {
    "objectID": "ml_models/index.html#overview",
    "href": "ml_models/index.html#overview",
    "title": "Machine Learning Models",
    "section": "",
    "text": "Machine learning (ML) as we have defined it involves estimating a (mathematical) function and we divide problems into two general types: supervised and unsupervised learning. In supervised learning, we have a set of data that we use to train a model to predict a target variable. In unsupervised learning, we have a set of data that we use to train a model to find patterns in the data. In this chapter, we will focus on supervised learning. Further, we separate supervised learning into two types: regression and classification. In regression, the target variable is continuous. In classification, the target variable is categorical. We also consider two types of applications for machine learning: prediction and inference. In prediction, we are interested in predicting the target variable. In inference, we are interested in understanding the relationship between the target variable and the predictors.\n\n\nHere we provide lists of the most common machine learning models, separating the lists into one for supervised methods and one for unsupervised methods. Remember that the choice of the machine learning model depends on the specific problem, the nature of the data, and the trade-off between interpretability and predictive performance. It’s often a good practice to experiment with multiple models and evaluate their performance to select the most suitable one for a given task.\n\n\n\nLinear Regression:\n\nSummary: Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.\nCharacteristics:\n\nSuitable for regression tasks (predicting continuous numeric values).\nAssumes a linear relationship between predictors and the target.\nSimple and interpretable.\n\n\nLogistic Regression:\n\nSummary: Logistic regression is used for binary classification, modeling the probability that an instance belongs to a particular class.\nCharacteristics:\n\nSuitable for binary classification tasks.\nUses the logistic function to model probabilities.\nProvides probabilities and interpretable coefficients.\n\n\nDecision Trees:\n\nSummary: Decision trees divide the data into subsets based on the most significant attributes, making them suitable for both classification and regression tasks.\nCharacteristics:\n\nNon-linear and can capture complex relationships.\nProne to overfitting but can be regularized.\nEasily interpretable.\n\n\nRandom Forest:\n\nSummary: Random forests are an ensemble of decision trees that improve predictive accuracy and reduce overfitting.\nCharacteristics:\n\nCombines multiple decision trees for better performance.\nHandles feature importance and reduces variance.\nWorks well for classification and regression.\n\n\nSupport Vector Machines (SVM):\n\nSummary: SVMs aim to find a hyperplane that best separates data points into different classes.\nCharacteristics:\n\nEffective for binary classification and can be extended to multiclass.\nUses kernel functions for non-linear separations.\nGood for high-dimensional data.\n\n\nK-Nearest Neighbors (K-NN):\n\nSummary: K-NN assigns a class to a data point based on the majority class among its k-nearest neighbors in feature space.\nCharacteristics:\n\nSimple and intuitive.\nCan handle both classification and regression.\nSensitive to the choice of k.\n\n\nNaive Bayes:\n\nSummary: Naive Bayes is a probabilistic classifier based on Bayes’ theorem, assuming that features are conditionally independent.\nCharacteristics:\n\nEfficient and suitable for text classification.\nAssumes feature independence (naive assumption).\nWorks well for high-dimensional data.\n\n\nGradient Boosting Machines (GBM):\n\nSummary: GBMs build an ensemble of weak learners (usually decision trees) to create a strong predictive model.\nCharacteristics:\n\nCombines multiple weak learners for high accuracy.\nHandles regression and classification tasks.\nProne to overfitting, but can be regularized.\n\n\nNeural Networks (Deep Learning):\n\nSummary: Neural networks, especially deep learning models, are highly flexible and can model complex relationships in data.\nCharacteristics:\n\nExtremely powerful for various tasks.\nRequires large amounts of data and computational resources.\nInterpretability can be a challenge for deep models.\n\n\nEnsemble Methods:\n\nSummary: Ensemble methods combine multiple base models to improve predictive performance.\nCharacteristics:\n\nInclude bagging (e.g., Random Forest), boosting (e.g., AdaBoost), and stacking.\nOften more robust and accurate than individual models.\nHandle various types of data and tasks.\n\n\n\n\n\n\n\nK-Means Clustering:\n\nSummary: K-Means is a clustering algorithm that partitions data into clusters based on similarity.\nCharacteristics:\n\nUnsupervised clustering.\nAssigns data points to clusters with similar attributes.\nRequires specifying the number of clusters (k).\n\n\nHierarchical Clustering:\n\nSummary: Hierarchical clustering builds a tree of clusters, revealing hierarchical relationships.\nCharacteristics:\n\nCreates a tree-like structure of nested clusters.\nAgglomerative (bottom-up) or divisive (top-down) approaches.\nNo need to specify the number of clusters in advance.\n\n\nPrincipal Component Analysis (PCA):\n\nSummary: PCA is a dimensionality reduction technique that captures the most important features in data.\nCharacteristics:\n\nReduces the dimensionality of data while preserving variance.\nUsed for feature selection and visualization.\nFinds orthogonal linear combinations of features (principal components).\n\n\nIndependent Component Analysis (ICA):\n\nSummary: ICA separates a multivariate signal into additive, independent components.\nCharacteristics:\n\nUsed for blind source separation and feature extraction.\nAssumes that observed data is a linear combination of independent sources.\nApplied in signal processing and neuroscience.\n\n\nApriori Algorithm:\n\nSummary: Apriori is used for association rule mining in transaction data.\nCharacteristics:\n\nDiscovers frequent itemsets in transaction databases.\nUsed for market basket analysis and recommendation systems.\nGenerates association rules (if item A is bought, item B is also likely to be bought).\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise):\n\nSummary: DBSCAN clusters data points based on density and identifies noise points.\nCharacteristics:\n\nAutomatically detects clusters of varying shapes and sizes.\nSuitable for data with irregular densities.\nDoes not require specifying the number of clusters in advance.\n\n\nGaussian Mixture Model (GMM):\n\nSummary: GMM models data as a mixture of Gaussian distributions.\nCharacteristics:\n\nAllows modeling complex data distributions.\nUseful for density estimation and clustering.\nCan be used for both soft and hard clustering.\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE):\n\nSummary: t-SNE is a dimensionality reduction technique known for preserving local structure in data.\nCharacteristics:\n\nUsed for high-dimensional data visualization.\nEffective for visualizing clusters and similarities.\nNon-linear dimensionality reduction.\n\n\nAutoencoders:\n\nSummary: Autoencoders are neural networks used for unsupervised feature learning.\nCharacteristics:\n\nLearn data representations by encoding and decoding input data.\nUsed for dimensionality reduction, denoising, and anomaly detection.\nNeural network architecture.\n\n\nLatent Dirichlet Allocation (LDA):\n\nSummary: LDA is a probabilistic model used for topic modeling in text data.\nCharacteristics:\n\nIdentifies topics in a collection of documents.\nAssumes documents are mixtures of topics, and words are mixtures of topic-specific words.\nWidely used in natural language processing.\n\n\n\n\n\n\n\nThe tidymodels package in R is an integrated ecosystem of packages designed to streamline the process of creating, evaluating, and deploying machine learning models while adhering to tidy data principles. The tidymodels framework follows a structured and consistent approach to machine learning, making it easier for data scientists and analysts to work with data and build predictive models. Here’s a brief description of the key components of the tidymodels package:\n\ntidyverse Integration:\n\nDescription: tidymodels seamlessly integrates with the popular tidyverse suite of packages, allowing for the use of tidy data frames and other tidy tools.\n\nparsnip:\n\nDescription: The parsnip package defines a common interface for specifying machine learning models, making it easier to work with different modeling engines.\n\nrecipes:\n\nDescription: The recipes package provides a systematic way to define and preprocess feature engineering steps for your data, creating a “recipe” that includes data preprocessing and variable transformations.\n\ntune:\n\nDescription: The tune package offers tools for hyperparameter tuning, allowing you to optimize model performance by systematically searching for the best hyperparameters.\n\nworkflows:\n\nDescription: The workflows package simplifies the process of building, tuning, and evaluating models by combining models, recipes, and tuning into a unified workflow.\n\nyardstick:\n\nDescription: The yardstick package provides a wide range of functions for model evaluation, including metrics for classification, regression, and survival analysis tasks.\n\nrsample:\n\nDescription: The rsample package is for resampling and creating data splits, essential for tasks such as cross-validation and bootstrapping.\n\nbroom:\n\nDescription: The broom package helps tidy up the results of model fits, making it easy to extract coefficients, predictions, and other model-related information in a tidy data format.\n\nCommunity and Extensibility:\n\nDescription: The tidymodels ecosystem has a growing community of users and contributors. It supports the creation of custom modeling engines, extending the framework to new algorithms.\n\nReproducibility and Best Practices:\n\nDescription: tidymodels promotes best practices in machine learning, emphasizing tidy data principles, clear model specification, and reproducibility.\n\n\nThe parsnip package in the tidymodels family allows one to specify a variety of supervised machine learning models. Here is a list of some of the parsnip models:\n\nlinear_reg:\n\nDescription: Linear Regression Model\nType: Regression\nDocumentation: linear_reg Documentation\n\nlogistic_reg:\n\nDescription: Logistic Regression Model\nType: Classification\nDocumentation: logistic_reg Documentation\n\ndecision_tree:\n\nDescription: Decision Tree Model\nType: Both (Regression and Classification)\nDocumentation: decision_tree Documentation\n\nrand_forest:\n\nDescription: Random Forest Model\nType: Both (Regression and Classification)\nDocumentation: rand_forest Documentation\n\nsvm_rbf:\n\nDescription: Support Vector Machine with Radial Basis Function Kernel Model\nType: Both (Regression and Classification)\nDocumentation: svm_rbf Documentation\n\nsvm_linear:\n\nDescription: Support Vector Machine with Linear Kernel Model\nType: Both (Regression and Classification)\nDocumentation: svm_linear Documentation\n\nnearest_neighbor:\n\nDescription: k-Nearest Neighbors Model\nType: Both (Regression and Classification)\nDocumentation: nearest_neighbor Documentation\n\nnaive_Bayes:\n\nDescription: Naive Bayes Model\nType: Classification\nDocumentation: naive_Bayes Documentation\n\nboost_tree:\n\nDescription: Gradient Boosting Model\nType: Both (Regression and Classification)\nDocumentation: boost_tree Documentation\n\nmlp:\n\nDescription: Multilayer Perceptron (Neural Network) Model\nType: Both (Regression and Classification)\nDocumentation: mlp Documentation"
  },
  {
    "objectID": "ml_models/index.html#supervised-learning",
    "href": "ml_models/index.html#supervised-learning",
    "title": "Machine Learning Models",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nRegression\n\n\nClassification"
  },
  {
    "objectID": "ml_models/index.html#references",
    "href": "ml_models/index.html#references",
    "title": "Machine Learning Models",
    "section": "References",
    "text": "References\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. \" O’Reilly Media, Inc.\".\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-09\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0      2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3      2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3      2023-08-14 [1] CRAN (R 4.3.0)\n glmnet       * 4.1-8      2023-08-22 [1] CRAN (R 4.3.0)\n infer        * 1.0.5      2023-09-06 [1] CRAN (R 4.3.0)\n kableExtra   * 1.3.4      2021-02-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3      2023-09-27 [1] CRAN (R 4.3.1)\n Matrix       * 1.6-1.1    2023-09-18 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0      2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1      2023-08-17 [1] CRAN (R 4.3.0)\n parttree     * 0.0.1.9004 2023-10-02 [1] Github (grantmcdermott/parttree@d2b60ac)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n ranger       * 0.15.1     2023-04-03 [1] CRAN (R 4.3.0)\n readr        * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8      2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0      2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1      2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2      2023-08-23 [1] CRAN (R 4.3.0)\n vip          * 0.4.1      2023-08-21 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3      2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1      2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0      2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "ml_models/index.html#tidymodels-machine-learning-workflow",
    "href": "ml_models/index.html#tidymodels-machine-learning-workflow",
    "title": "Machine Learning Models",
    "section": "tidymodels Machine Learning Workflow",
    "text": "tidymodels Machine Learning Workflow\nThe tidymodels framework provides a consistent set of steps for training and evaluating machine learning models. The following is a list of the steps in the tidymodels workflow:\n\nData Preparation:\n\nLoad and prepare your dataset following tidy data principles.\n\nData Splitting:\n\nUse the rsample package to create data splits for training and testing.\n\nPreprocessing and Feature Engineering:\n\nDefine a data preprocessing plan using the recipes package.\nCreate a “recipe” for data cleaning, transformation, and feature engineering.\n\nModel Specification:\n\nSpecify machine learning models with the parsnip package.\nChoose from a variety of models for regression, classification, etc.\n\nHyperparameter Tuning:\n\nUtilize the tune package for hyperparameter tuning.\nDefine a grid of hyperparameters and use resampling methods for evaluation.\n\nModel Training:\n\nTrain models with the specified data splits, preprocessing plan, and hyperparameters.\nUse the fit function to train on the training data.\n\nModel Evaluation:\n\nAssess model performance using the yardstick package.\nCalculate relevant evaluation metrics (e.g., accuracy, RMSE).\n\nModel Selection:\n\nCompare model performance and hyperparameter settings.\nSelect the best-performing model or create ensembles (e.g., stacking, bagging).\n\nModel Interpretation and Visualization:\n\nUse the broom package to extract and tidy model results.\nVisualize model outputs and interpret results.\n\nDeployment and Prediction:\n\nDeploy the final model for predictions on new data.\nUse the trained model in production environments.\n\nDocumentation and Reproducibility:\n\nDocument the entire workflow, including preprocessing, model specifications, tuning, and results.\nEnsure reproducibility and transparency.\n\nSharing and Collaboration:\n\nShare code and findings with collaborators for reproducibility and collaboration.\n\nOngoing Monitoring and Maintenance:\n\nIn production, monitor model performance and update models as needed.\n\n\nThe best references for tidymodels are the tidymodels.org website and the Tidy Modeling with R book. The Tidy Modeling with R book is available for free online and is a great resource for learning the tidymodels framework (Kuhn and Silge 2022). The blog posts by Julia Silge are also a great resource for learning tidymodels. View Silge’s blog."
  },
  {
    "objectID": "ml_models/index.html#some-examples",
    "href": "ml_models/index.html#some-examples",
    "title": "Machine Learning Models",
    "section": "Some Examples",
    "text": "Some Examples\n\nExample 1: tidymodels Workflow for Linear Regression for Predictive Modeling\nThe following is an example of the tidymodels workflow for linear regression for predictive modeling. The example uses the mtcars dataset and the parsnip and recipes packages. The example is adapted from the Tidy Modeling with R book (Kuhn and Silge 2022).\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the mtcars dataset\ndata(mtcars)\n\n# create a training and testing split\nmtcars_split &lt;- initial_split(mtcars, prop = 0.75)\n\n# create a training and testing dataset\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\n\n# create a recipe for preprocessing\nmtcars_recipe &lt;- recipe(mpg ~ ., data = mtcars_train) %&gt;%\n  step_center(all_predictors()) %&gt;%\n  step_scale(all_predictors())\n\n# create a linear regression model\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# fit the model\nlm_fit &lt;- workflow() %&gt;%\n  add_recipe(mtcars_recipe) %&gt;%\n  add_model(lm_spec) %&gt;%\n  fit(data = mtcars_train)\n\n# make predictions\nlm_pred &lt;- predict(lm_fit, mtcars_test) %&gt;%\n  bind_cols(mtcars_test)\n\n# evaluate the model\nlm_eval &lt;- lm_pred %&gt;%\n  metrics(truth = mpg, estimate = .pred) %&gt;%\n  bind_rows(\n    lm_pred %&gt;%\n      rsq(truth = mpg, estimate = .pred) %&gt;%\n      mutate(metric = \"rsq\")\n  )\n\n# tidy the model results\nlm_tidy &lt;- tidy(lm_fit)\n\n# visualize the model results\nlm_tidy %&gt;%\n  ggplot(aes(x = term, y = estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error)) +\n  coord_flip() +\n  labs(\n    title = \"Linear Regression Model Results\",\n    subtitle = \"Model: mpg ~ .\",\n    x = \"Term\",\n    y = \"Estimate\"\n  )\n\n\n\n\n\nFigure 1: Tidy Models Workflow for Regression\n\n\n\n\n\n\n\n\n\nTable 1: Linear Regression Model Results\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n21.0708333\n0.6035556\n34.9111701\n0.0000000\n\n\ncyl\n-1.8085954\n2.7450923\n-0.6588469\n0.5214933\n\n\ndisp\n2.7886147\n3.3586784\n0.8302714\n0.4213717\n\n\nhp\n-1.4572830\n2.4512678\n-0.5945018\n0.5623810\n\n\ndrat\n0.0941833\n1.1813897\n0.0797225\n0.9376722\n\n\nwt\n-4.4737683\n2.3983919\n-1.8653200\n0.0848621\n\n\nqsec\n1.6098052\n1.7188679\n0.9365497\n0.3660636\n\n\nvs\n0.4021956\n1.3188879\n0.3049506\n0.7652312\n\n\nam\n1.2501668\n1.4919647\n0.8379332\n0.4172084\n\n\ngear\n-0.0667576\n1.5665234\n-0.0426151\n0.9666559\n\n\ncarb\n0.5220534\n1.8506869\n0.2820863\n0.7823182\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Linear Regression Model Evaluation\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n2.4214858\n\n\nrsq\nstandard\n0.7487043\n\n\nmae\nstandard\n2.0481386\n\n\nrsq\nstandard\n0.7487043\n\n\n\n\n\n\n\n\n\n\nExample 2: tidymodels Workflow for Linear Regression for Inferential Modeling\nThe following is an example of the tidymodels workflow for linear regression for inferential modeling. The example uses the mtcars dataset and the parsnip and recipes packages.\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the mtcars dataset\ndata(mtcars)\n\n# create bootstrap samples\nmtcars_boot &lt;- reg_intervals(mpg ~ disp + cyl + wt + am, \n                             data = mtcars,\n                             model_fn = \"lm\",\n                             type = \"percentile\",\n                             keep_reps = TRUE)\n\n\nmtcars_boot %&gt;% \n  select(term, .replicates) %&gt;% \n  unnest(cols = .replicates) %&gt;% \n  ggplot(aes(x = estimate)) + \n  geom_histogram(bins = 30,color=\"white\",fill=\"lightblue\") + \n  facet_wrap(~ term, scales = \"free_x\") + \n  geom_vline(data = mtcars_boot, aes(xintercept = .lower), col = \"purple\") + \n  geom_vline(data = mtcars_boot, aes(xintercept = .upper), col = \"purple\") + \n  geom_vline(xintercept = 0, col = \"black\",linetype=\"dashed\")\n\n\n\n\n\nFigure 2: Tidy Models Workflow for Regression\n\n\n\n\n\n\nExample 3: tidymodels Workflow for Tuning Random Forest for Regression\nThe following is an example of the tidymodels workflow for tuning random forest for regression. The example uses the mtcars dataset and the parsnip and recipes packages.\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the mtcars dataset\ndata(mtcars)\n\n# create a training and testing split\nmtcars_split &lt;- initial_split(mtcars, prop = 0.75)\n\n# create a training and testing dataset\nmtcars_train &lt;- training(mtcars_split)\nmtcars_test &lt;- testing(mtcars_split)\n\n# create a cross-validation set\nmtcars_cv &lt;- vfold_cv(mtcars_train, v = 5)\n\n# create a recipe for preprocessing\nmtcars_recipe &lt;- recipe(mpg ~ ., data = mtcars_train)\n\n# create a random forest model\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  trees = 1000,\n  min_n = tune()\n) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"ranger\")\n\n# create a workflow\ntune_wf &lt;- workflow() %&gt;%\n  add_recipe(mtcars_recipe) %&gt;%\n  add_model(rf_spec)\n\n# create tuning grid\nrf_grid &lt;- grid_regular(\n  mtry(range = c(3, 11)),\n  min_n(),\n  levels = 15\n)\n\n# tune the model\ntune_res &lt;- tune_grid(\n  tune_wf,\n  resamples = mtcars_cv,\n  grid = rf_grid\n)\n\n# visualize the tuning results\ntune_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  mutate(min_n = factor(min_n)) %&gt;%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"RMSE\")\n\n\n\n\n\nFigure 3: Tidy Models Workflow for Random Forest\n\n\n\n\nNow we will select a final model based on the tuning results.\n\n\nCode\nbest_rmse &lt;- select_best(tune_res, \"rmse\")\n\nfinal_rf &lt;- finalize_model(\n  rf_spec,\n  best_rmse\n)\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(mtcars_recipe) %&gt;%\n  add_model(final_rf)\n\nfinal_res &lt;- final_wf %&gt;%\n  last_fit(mtcars_split)\n\nfinal_res %&gt;%\n  collect_metrics() %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n1.9587187\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.7234572\nPreprocessor1_Model1\n\n\n\n\n\n\n\nLet’s see how the model predictions compare with the observed data for the test set observations.\n\n\nCode\nfinal_res %&gt;%\n  collect_predictions() %&gt;%\n  ggplot(aes(mpg, .pred)) +\n  geom_abline(color = \"purple\") +\n  geom_point() +\n  labs(x = \"Observed\", y = \"Predicted\")\n\n\n\n\n\nFigure 4: Results on Test Set for Tuned Random Forest for Regression\n\n\n\n\n\n\nExample 4: tidymodels Workflow for Classification with Multinomial Logistic Regression\nThe following is an example of the tidymodels workflow for multinomial logistic regression. The example uses the penguins dataset and the parsnip and recipes packages.\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# load the penguins dataset\ndata(penguins)\n\n# remove missing values from the dataset\npenguins &lt;- penguins %&gt;% drop_na()\n\n# create a training and testing split\npenguins_split &lt;- initial_split(penguins, prop = 0.75)\n\n# create a training and testing dataset\npenguins_train &lt;- training(penguins_split)\npenguins_test &lt;- testing(penguins_split)\n\n# create a cross-validation set\npenguins_cv &lt;- vfold_cv(penguins_train, v = 5)\n\n# create a recipe for preprocessing\npenguins_recipe &lt;- recipe(island ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data = penguins_train)\n\n# create a multinomial logistic regression model\nlogit_spec &lt;- multinom_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")\n\n# create a workflow\nnber_wf &lt;- workflow(penguins_recipe, logit_spec)\n\n# create grid for tuning\nnber_grid &lt;- grid_regular(penalty(range = c(-5, 0)), \n                          mixture(range = c(0, 1)),\n                          levels = 20)\n\n# tune the model\nnber_rs &lt;-\n  tune_grid(\n    nber_wf,\n    penguins_cv,\n    grid = nber_grid\n  )\n\nautoplot(nber_rs)\n\n\n\n\n\nFigure 5: Tidy Models Workflow for Multinomial Logistic Regression\n\n\n\n\nShow best models.\n\n\nCode\nshow_best(nber_rs) %&gt;%\n  kable() %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"200px\")\n\n\n\n\n\n\npenalty\nmixture\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\n0.0000100\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model161\n\n\n0.0000183\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model162\n\n\n0.0000336\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model163\n\n\n0.0000616\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model164\n\n\n0.0001129\n0.4210526\nroc_auc\nhand_till\n0.8596058\n5\n0.0076867\nPreprocessor1_Model165\n\n\n\n\n\n\n\nSelect the best model.\n\n\nCode\nfinal_penalty &lt;-\n  nber_rs %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", desc(penalty))\n\n\nFit the final model.\n\n\nCode\nfinal_rs &lt;-\n  nber_wf %&gt;%\n  finalize_workflow(final_penalty) %&gt;%\n  last_fit(penguins_split)\n\n\nThe confusion matrix for the final model prdictions on the test set.\n\n\nCode\ncollect_predictions(final_rs) %&gt;%\n  conf_mat(island, .pred_class) %&gt;%\n  autoplot()\n\n\n\n\n\nFigure 6: Confusion Matrix for Final Model Predictions on Test Set\n\n\n\n\nThe ROC curve.\n\n\nCode\ncollect_predictions(final_rs) %&gt;%\n  roc_curve(truth = island, .pred_Biscoe:.pred_Torgersen) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(size = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  coord_fixed()\n\n\n\n\n\nFigure 7: ROC Curve for Final Model Predictions on Test Set"
  },
  {
    "objectID": "nns/index.html",
    "href": "nns/index.html",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:"
  },
  {
    "objectID": "nns/index.html#learning-objectives",
    "href": "nns/index.html#learning-objectives",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:"
  },
  {
    "objectID": "nns/index.html#readings-etc.",
    "href": "nns/index.html#readings-etc.",
    "title": "Lesson",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead chapter 10 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017).\n\nWatch the following video lectures on neural networks:\n\nView Introduction to Neural Networks video on YouTube."
  },
  {
    "objectID": "nns/index.html#overview",
    "href": "nns/index.html#overview",
    "title": "Lesson",
    "section": "Overview",
    "text": "Overview\nNeural Network Playground, view the page."
  },
  {
    "objectID": "nns/index.html#support-vector-machine-approaches",
    "href": "nns/index.html#support-vector-machine-approaches",
    "title": "Lesson",
    "section": "Support Vector Machine Approaches",
    "text": "Support Vector Machine Approaches\n\nMaximal Marginal Cllassifier\n\n\n\nFigure 1: Left: Separating hyperplanes for binary data. Right: The decision rule based on a separating hyperplane.\n\n\n\n\n\nFigure 2: The solid lines shows the maximal margin hyperplane. The margin is the distance between the two dotted lines."
  },
  {
    "objectID": "nns/index.html#preparation-for-the-next-lesson",
    "href": "nns/index.html#preparation-for-the-next-lesson",
    "title": "Lesson",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:\n\nRead chapter 9 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 9 of Statistical Learning with Math and R (Suzuki 2020).\nView Support Vector Classifier video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nView Support Vector Classifiers in R video on YouTube."
  },
  {
    "objectID": "nns/index.html#references",
    "href": "nns/index.html#references",
    "title": "Lesson",
    "section": "References",
    "text": "References\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-09\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n broom        * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0   2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3   2023-08-14 [1] CRAN (R 4.3.0)\n infer        * 1.0.5   2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2   2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0   2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1   2023-08-17 [1] CRAN (R 4.3.0)\n purrr        * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8   2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0   2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1   2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2   2023-08-23 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3   2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1   2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0   2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "svm/index.html",
    "href": "svm/index.html",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe the support vector machine (SVM) approach to classification.\nUse the tidymodels workflow to fit and tune various SVM classification models."
  },
  {
    "objectID": "svm/index.html#learning-objectives",
    "href": "svm/index.html#learning-objectives",
    "title": "Lesson",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe the support vector machine (SVM) approach to classification.\nUse the tidymodels workflow to fit and tune various SVM classification models."
  },
  {
    "objectID": "svm/index.html#readings-etc.",
    "href": "svm/index.html#readings-etc.",
    "title": "Lesson",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead chapter 9 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 9 of Statistical Learning with Math and R (Suzuki 2020).\nView Support Vector Classifier video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nView Support Vector Classifiers in R video on YouTube."
  },
  {
    "objectID": "svm/index.html#overview",
    "href": "svm/index.html#overview",
    "title": "Lesson",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "svm/index.html#support-vector-machine-approaches",
    "href": "svm/index.html#support-vector-machine-approaches",
    "title": "Lesson",
    "section": "Support Vector Machine Approaches",
    "text": "Support Vector Machine Approaches\n\nMaximal Marginal Cllassifier\n\n\n\nFigure 1: Left: Separating hyperplanes for binary data. Right: The decision rule based on a separating hyperplane.\n\n\n\n\n\nFigure 2: The solid lines shows the maximal margin hyperplane. The margin is the distance between the two dotted lines."
  },
  {
    "objectID": "svm/index.html#preparation-for-the-next-lesson",
    "href": "svm/index.html#preparation-for-the-next-lesson",
    "title": "Lesson",
    "section": "Preparation for the next lesson",
    "text": "Preparation for the next lesson\nTo prepare for the next lesson, please read:"
  },
  {
    "objectID": "svm/index.html#references",
    "href": "svm/index.html#references",
    "title": "Lesson",
    "section": "References",
    "text": "References\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-09\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0      2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3      2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3      2023-08-14 [1] CRAN (R 4.3.0)\n infer        * 1.0.5      2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2      2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3      2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0      2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1      2023-08-17 [1] CRAN (R 4.3.0)\n parttree     * 0.0.1.9004 2023-10-02 [1] Github (grantmcdermott/parttree@d2b60ac)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8      2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0      2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1      2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2      2023-08-23 [1] CRAN (R 4.3.0)\n vip          * 0.4.1      2023-08-21 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3      2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1      2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0      2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "ml_models/index.html#comparing-multiple-modeling-approaches",
    "href": "ml_models/index.html#comparing-multiple-modeling-approaches",
    "title": "Machine Learning Models",
    "section": "Comparing Multiple Modeling Approaches",
    "text": "Comparing Multiple Modeling Approaches\nIn her blog post Evaluate multiple modeling approaches for spam email, Julia Silge uses workflow sets to evaluate multiple possible models. Let’s work through that post together. View the post."
  },
  {
    "objectID": "lesson08/index.html",
    "href": "lesson08/index.html",
    "title": "Lesson 8",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe the support vector machine (SVM) approach to classification.\nUse the tidymodels workflow to fit and tune various SVM classification models."
  },
  {
    "objectID": "lesson08/index.html#learning-objectives",
    "href": "lesson08/index.html#learning-objectives",
    "title": "Lesson 8",
    "section": "",
    "text": "After this lesson, students will be able to:\n\nDescribe the support vector machine (SVM) approach to classification.\nUse the tidymodels workflow to fit and tune various SVM classification models."
  },
  {
    "objectID": "lesson08/index.html#readings-etc.",
    "href": "lesson08/index.html#readings-etc.",
    "title": "Lesson 8",
    "section": "Readings, etc.",
    "text": "Readings, etc.\nFor this lesson, refer to the following readings, etc.:\n\nRead chapter 9 from of An Introduction to Statistical Learning (Tibshirani, James, and Trevor 2017). You may also want to read chapter 9 of Statistical Learning with Math and R (Suzuki 2020).\nView Support Vector Classifier video on YouTube.\n\n\n\n\n\n\n\n\n\n\n\nView Support Vector Classifiers in R video on YouTube."
  },
  {
    "objectID": "lesson08/index.html#overview",
    "href": "lesson08/index.html#overview",
    "title": "Lesson 8",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "lesson08/index.html#support-vector-machine-approaches",
    "href": "lesson08/index.html#support-vector-machine-approaches",
    "title": "Lesson 8",
    "section": "Support Vector Machine Approaches",
    "text": "Support Vector Machine Approaches\n\nMaximal Marginal Cllassifier\n\n\n\nFigure 1: Left: Separating hyperplanes for binary data. Right: The decision rule based on a separating hyperplane.\n\n\n\n\n\nFigure 2: The solid lines shows the maximal margin hyperplane. The margin is the distance between the two dotted lines."
  },
  {
    "objectID": "lesson08/index.html#references",
    "href": "lesson08/index.html#references",
    "title": "Lesson 8",
    "section": "References",
    "text": "References\n\n\nSuzuki, Joe. 2020. Statistical Learning with Math and r. Springer.\n\n\nTibshirani, Hastie Robert, Gareth James, and Daniela Witten Trevor. 2017. An Introduction to Statistical Learning. springer publication.\n\n\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.0\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2023-10-11\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n quarto   1.3.450 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n broom        * 1.0.5      2023-06-09 [1] CRAN (R 4.3.0)\n dials        * 1.2.0      2023-04-03 [1] CRAN (R 4.3.0)\n dplyr        * 1.1.3      2023-09-03 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.3      2023-08-14 [1] CRAN (R 4.3.0)\n infer        * 1.0.5      2023-09-06 [1] CRAN (R 4.3.0)\n ISLR2        * 1.3-2      2022-11-20 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.3      2023-09-27 [1] CRAN (R 4.3.1)\n modeldata    * 1.2.0      2023-08-09 [1] CRAN (R 4.3.0)\n parsnip      * 1.1.1      2023-08-17 [1] CRAN (R 4.3.0)\n parttree     * 0.0.1.9004 2023-10-02 [1] Github (grantmcdermott/parttree@d2b60ac)\n purrr        * 1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n readr        * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n recipes      * 1.0.8      2023-08-25 [1] CRAN (R 4.3.0)\n rsample      * 1.2.0      2023-08-23 [1] CRAN (R 4.3.0)\n scales       * 1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo  * 1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidymodels   * 1.1.1      2023-08-24 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n tune         * 1.1.2      2023-08-23 [1] CRAN (R 4.3.0)\n vip          * 0.4.1      2023-08-21 [1] CRAN (R 4.3.0)\n workflows    * 1.1.3      2023-02-22 [1] CRAN (R 4.3.0)\n workflowsets * 1.0.1      2023-04-06 [1] CRAN (R 4.3.0)\n yardstick    * 1.2.0      2023-04-21 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "lesson07/index.html#neural-networks",
    "href": "lesson07/index.html#neural-networks",
    "title": "Lesson 7",
    "section": "Neural Networks",
    "text": "Neural Networks\nA neural network is a nonlinear function that is described by parameters called weights and bias. Each node or neuron in a layer of the network inputs a linear combination of the outputs from the nodes of the previous layer. The weights and bias parameters specify the linear combination which is passed through an activation function to produce the output of the node. The activation function is a nonlinear function, its output defines the output of the node. Later, we will define some typical activation functions.\nThe output of each node is then used as the input to the nodes of the next layer. The output of the last layer is the output of the neural network. The weights and bias parameters are learned during the training process. The training process involves finding the weights and bias parameters that minimize a loss function. The loss function is a measure of how well the neural network is performing on the training data. The goal of training a neural network is to find the weights and bias parameters that minimize the loss function.\nAs with all of the other machine learning algorithms we have covered so far, deep learning requires us to solve some kind of optimization problem. In the case of neural networks, we seek to find the weights and bias parameters that minimize the loss function. For regression problems, the loss function is typically the mean squared error (MSE). For classification problems, the loss function is typically the cross-entropy loss.\nThe weights and bias parameters are learned using an algorithm called gradient descent. Gradient descent is an optimization algorithm that is used to find the minimum of a function. In the context of neural networks, we use gradient descent to find the minimum of the loss function. The loss function is a function of the weights and biases of the neural network. The weights and biases are the parameters of the neural network. The loss function is a measure of how well the neural network is performing on the training data. The goal of training a neural network is to find the weights and bias parameters that minimize the loss function.\nStochastic gradient descent (SGD) is a variant of gradient descent that is used to train neural networks. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.\nGradient descent and SGD require us to compute the gradient (multi-variable derivative) of the loss function with respect to the weights and bias parameters. The activation function at each node of the network results in a nonlinear function of the parameters we want to optimize. Thus, computing the implementation of gradient descent for neural networks forces us to use the chain rule for derivatives and this becomes a very messy calculation.\nA major development in the field of neural networks was the introduction of the backpropagation algorithm by Rumelhart, Hinton, and Williams in 1986. The backpropagation algorithm is an algorithm for training neural networks. This algorithm is used to calculate the gradient of the loss function with respect to the weights and bias parameters. The gradient is then used to update the weights and bias parameters via gradient descent or something similar. The backpropagation algorithm is clever use of the chain rule for derivatives.\nWe will proceed by getting a feel for gradient descent in the context of functions that are a lot simpler than neural networks.\n\nGradient Descent for Optimization\nGradient descent is an iterative optimization algorithm for finding the minimum of a function \\(f:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\). The algorithm starts with an initial guess \\({\\bf x}_{0} \\in \\mathbb{R}^{d}\\) for the minimizing value for the function. The algorithm then iteratively updates the guess by an iteration of the form\n\\[\n{\\bf x}_{k+1} = {\\bf x}_{k} - \\alpha_{k} \\nabla f({\\bf x}_{k})\n\\]\nwhere \\(\\alpha_{k} &gt; 0\\) is the step size and \\(\\nabla f({\\bf x}_{k})\\) is the gradient of the function \\(f\\) at the point \\({\\bf x}_{k}\\). Recall that the gradient of a function is a vector that points in the direction of the steepest ascent of the function. In practical implementations one often takes the step size \\(\\alpha_{k} = \\text{constant}\\) for all \\(k\\) and this constant is called the learning rate. In the context of neural networks, each step in an iteration of gradient descent is called an epoch.\nLet’s start with a simple one-dimensional problem. Consider the function\n\\[\nf(x) = x^4 + 7x^3 + 5x^2 - 17x + 3\n\\] which is plotted in Figure 2. This function has a global minimum at \\(x = -4.5\\). We can use gradient descent to find at least approximately the value of \\(x\\) that minimizes \\(f(x)\\).\n\n\nCode\nf &lt;- function(x) {\n  x^4 + 7*x^3 + 5*x^2 - 17*x + 3\n}\n\nx &lt;- seq(-6, 2, length.out = 100)\n\ntibble(x = x) %&gt;% \n  mutate(y = f(x)) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_vline(xintercept = -4.5, linetype = \"dashed\") +\n  geom_point(aes(x = -4.5, y = f(-4.5)), color = \"purple\", size = 3) +\n  labs(\n    x = TeX(\"$x$\"),\n    y = TeX(\"$f(x)$\"),\n    title = \"A polynomial function with a global minimum\"\n  )\n\n\n\n\n\nFigure 2: A fourth-degree polynomial function with a global minimum at \\(x = -4.5\\).\n\n\n\n\nThe gradient (derivative) of \\(f(x)\\) is given by\n\\[\nf'(x) = 4x^3 + 21x^2 + 10x - 17\n\\]\nLet’s iterate gradient descent for 25 epochs with a learning rate of \\(\\alpha = 0.01\\) and an initial guess of \\(x_{0} = -3\\). We will plot the value of \\(x\\) at each epoch and the value of the function \\(f(x)\\) at each epoch.\n\n\nCode\nf_prime &lt;- function(x) {\n  4*x^3 + 21*x^2 + 10*x - 17\n}\n\nx &lt;- seq(-6, 2, length.out = 100)\n\ngd_min &lt;- function(x0, \n                  alpha=0.01, \n                  fun_to_min=f, \n                  fun_deriv=f_prime, \n                  n_epochs = 25){\n  x &lt;- x0\n  x_vals &lt;- c(x)\n  y_vals &lt;- c(fun_to_min(x))\n  grad_vals &lt;- c(fun_deriv(x))\n  \n  for (i in 1:n_epochs){\n    x &lt;- x - alpha*fun_deriv(x)\n    x_vals &lt;- c(x_vals, x)\n    y_vals &lt;- c(y_vals, fun_to_min(x))\n    grad_vals &lt;- c(grad_vals, fun_deriv(x))\n  }\n  \n  return(tibble(x = x_vals, y = y_vals, grad = grad_vals))\n}\n\ntst &lt;- gd_min(-3)\n\ntibble(x = x) %&gt;% \n  mutate(y = f(x)) %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_vline(xintercept = -4.5, linetype = \"dashed\") +\n  geom_point(aes(x = -4.5, y = f(-4.5)), color = \"purple\", size = 3) +\n  geom_point(data = tst, aes(x = x, y = y, color=y),size=2) +\n  labs(\n    x = TeX(\"$x$\"),\n    y = TeX(\"$f(x)$\"),\n    title = TeX(r'(Gradient descent for the function $f(x)$)')\n  )\n\n\n\n\n\nFigure 3: Gradient descent for the function \\(f(x)\\).\n\n\n\n\n\n\nActivation Functions\nShortly, we will describe a neural network with a single hidden layer in more detail. In ordre to do so, we need to introduce the concept of an activation function. An activation function is a function that is applied to the output of a node in a neural network. The activation function introduces non-linearity into the neural network. Without non-linearity, the neural network would be equivalent to a linear regression model. There are many different activation functions that are used in neural networks. Here are some of the most common activation functions:\n\nSigmoid Activation Function:\n\nDescription: The sigmoid activation function is commonly used in neural networks for binary classification tasks. It maps input values to the range \\((0, 1)\\), making it suitable for output layers of binary classifiers.\nMathematical Expression:\n\n\\[\n\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n\\]\nReLU (Rectified Linear Unit) Activation Function:\n\nDescription: ReLU is a widely used activation function that introduces non-linearity by returning the input for positive values and zero for negative values. It helps mitigate the vanishing gradient problem.\nMathematical Expression:\n\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nTanh (Hyperbolic Tangent) Activation Function:\n\nDescription: Tanh is another common activation function that maps input values to the range \\((-1, 1)\\). It provides zero-centered output, which can help training converge faster.\nMathematical Expression:\n\n\\[\n\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nLeaky ReLU Activation Function:\n\nDescription: Leaky ReLU is a variation of ReLU that allows a small gradient when the input is negative. It addresses the “dying ReLU” problem by preventing neurons from becoming inactive.\nMathematical Expression:\n\n\\[\n\\text{Leaky ReLU}(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\ \\alpha x, & \\text{if } x &lt; 0 \\end{cases}\n\\]\n\nwhere \\(\\alpha\\) is a small constant.\nFigure 4 shows the plot for each of the activation functions we defined.\n\n\nCode\n# Create a data frame with x values\nx &lt;- seq(-3, 3, length.out = 1000)\ndf &lt;- data.frame(x = x)\n\n# Sigmoid Activation Function\ndf$sigmoid &lt;- 1 / (1 + exp(-x))\n\n# ReLU (Rectified Linear Unit) Activation Function\ndf$relu &lt;- pmax(0, x)\n\n# Tanh (Hyperbolic Tangent) Activation Function\ndf$tanh &lt;- (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\n# Leaky ReLU Activation Function\nleaky_relu &lt;- function(x, alpha = 0.03) {\n  ifelse(x &gt;= 0, x, alpha * x)\n}\ndf$leaky_relu &lt;- leaky_relu(x)\n\n# Plot the activation functions\nggplot(df, aes(x)) +\n  geom_line(aes(y = sigmoid, color = \"Sigmoid\"), linewidth = 1) +\n  geom_line(aes(y = relu, color = \"ReLU\"), linewidth = 1) +\n  geom_line(aes(y = tanh, color = \"Tanh\"), linewidth = 1) +\n  geom_line(aes(y = leaky_relu, color = \"Leaky ReLU\"), linewidth = 1) +\n  labs(\n    title = \"Common Activation Functions for Neural Networks\",\n    x = \"Input (x)\",\n    y = \"Output\",\n    color = \"Activation Function\"\n  ) +\n  scale_color_manual(values = c(\"Sigmoid\" = \"#E69F00\", \"ReLU\" = \"#56B4E9\", \"Tanh\" = \"#009E73\", \"Leaky ReLU\" = \"#CC79A7\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 4: Activation functions used in neural networks.\n\n\n\n\n\n\nSingle Layer Networks\nA single layer neural network is the simplest type of neural network. It consists of a single layer of neurons that take in a set of inputs and produce a set of outputs. The output is computed by applying an activation function to a weighted sum of the inputs. We can describe a single layer neural network with \\(K\\) hidden units mathematically via the following expression:\n\\[\n\\begin{align*}\nf(X) &= \\beta_{0} + \\sum_{k=1}^{K}\\beta_{k}h_{k}(X) \\\\\n     &= \\beta_{0} + \\sum_{k=1}^{K}\\beta_{k}g\\left(w_{k0} + \\sum_{j=1}^{p}w_{kj}X_{j}\\right),\n\\end{align*}\n\\] where \\(g\\) is some activation function. Notice that a single layer neural network with \\(p\\) inputs and \\(K\\) hidden units has \\(1 + K + K + pK = 1 + (p+2)K\\) parameters. We can view a single layer neural network as a generalized linear model with \\(K\\) basis functions. That is, a linear regression in \\(K\\) activation functions.\nTo gain some perspective on what the nonlinearity in an single layer neural network allows us to capture, let’s look at a simple example. Suppose that we have \\(p=2\\) input variables \\(X_{1}\\) and \\(X_{2}\\) and \\(K=2\\) hidden units. Further, suppose that our activation function is \\(g(z) = z^2\\). Set the following parameters:\n\\[\n\\begin{array}{ccc} \\beta_{0} = 0, & \\beta_{1} = \\frac{1}{4}, & \\beta_{2} = -\\frac{1}{4} \\\\ w_{10} = 0, & w_{11} = 1, & w_{12} = 1, \\\\ w_{20} = 0, & w_{21} = 1, & w_{22} = -1.   \\end{array}\n\\]\nThen,\n\\[\n\\begin{align*}\nh_{1} &= (0 + X_{1} + X_{2})^2, \\\\\nh_{2} &= (0 + X_{1} - X_{2})^2.\n\\end{align*}\n\\]\nThus,\n\\[\n\\begin{align*}\nf(X) &= \\beta_{0} + \\beta_{1}h_{1} + \\beta_{2}h_{2} \\\\\n     &= 0 + \\frac{1}{4}(X_{1} + X_{2})^2 - \\frac{1}{4}(X_{1} - X_{2})^2 \\\\\n     &= \\frac{1}{4}(X_{1}^2 + 2X_{1}X_{2} + X_{2}^2) - \\frac{1}{4}(X_{1}^2 - 2X_{1}X_{2} + X_{2}^2) \\\\\n     &= X_{1}X_{2}.\n\\end{align*}\n\\]\nSo, we see that the sum of two nonlinear transformations of linear functions can produce an interaction term. This is a somewhat artificial example but the point is that the nonlinearity in a single layer neural network via an activation function can “detect” a variety of features in our data."
  },
  {
    "objectID": "lesson07/index.html#neural-networks-and-deep-learning-in-r",
    "href": "lesson07/index.html#neural-networks-and-deep-learning-in-r",
    "title": "Lesson 7",
    "section": "  Neural Networks and Deep Learning in R",
    "text": "Neural Networks and Deep Learning in R"
  },
  {
    "objectID": "lesson07/index.html#further-topics-on-deep-learning",
    "href": "lesson07/index.html#further-topics-on-deep-learning",
    "title": "Lesson 7",
    "section": "Further Topics on Deep Learning",
    "text": "Further Topics on Deep Learning\nThis is some further information on neural networks and deep learning based on questions raised by students.\n\nRecent Applications of Neural Networks and Deep Learning\n\nNatural Language Processing (NLP):\n\nExample: BERT, GPT-3, and other large transformer models for tasks like language translation, text generation, and sentiment analysis.\nLearn More: OpenAI’s GPT-3, Google’s BERT\n\nComputer Vision:\n\nExample: Convolutional Neural Networks (CNNs) for image classification, object detection, and facial recognition.\nLearn More: ImageNet Large Scale Visual Recognition Challenge\n\nHealthcare:\n\nExample: Using deep learning to analyze medical images, detect diseases, and predict patient outcomes.\nLearn More: Stanford’s CheXNet\n\nAutonomous Vehicles:\n\nExample: Self-driving cars rely on neural networks to perceive their surroundings and make decisions.\nLearn More: Waymo’s Self-Driving Technology\n\nRecommender Systems:\n\nExample: Recommending products, movies, or content to users based on their preferences and behavior.\nLearn More: Netflix’s Recommendation Algorithm\n\nFinance:\n\nExample: Predictive modeling for stock price forecasting, fraud detection, and algorithmic trading.\nLearn More: Stock Price Prediction with LSTM\n\nVoice Assistants:\n\nExample: Voice recognition and natural language understanding in smart speakers like Amazon Echo and Google Home.\nLearn More: Amazon Alexa\n\nGenerative Adversarial Networks (GANs):\n\nExample: Creating art, generating synthetic images, and deepfakes.\nLearn More: NVIDIA’s GAN Research\n\nRobotics:\n\nExample: Deep reinforcement learning for robot control and autonomous navigation.\nLearn More: OpenAI’s Robotics Research\n\nClimate Science:\n\nExample: Using neural networks to analyze climate data, model climate change, and predict extreme weather events.\nLearn More: DeepMind’s Climate Science\n\n\n\n\nSizes of Neural Networks in Various Applications\n\nNatural Language Processing (NLP):\n\nGPT-3: GPT-3, developed by OpenAI, is one of the largest language models with 175 billion parameters.\nBERT: Google’s BERT has 340 million parameters and is highly influential in NLP.\n\nComputer Vision:\n\nImageNet Models: Models like VGG-16, VGG-19, and ResNet used for image classification have tens of millions of parameters.\nLarge CNNs: In complex computer vision tasks, models can have hundreds of millions of parameters. Examples include Inception models and DenseNet.\n\nHealthcare:\n\nThe size of neural networks in healthcare applications varies, ranging from a few million to tens of millions of parameters for tasks like medical image analysis.\n\nAutonomous Vehicles:\n\nNeural networks used in autonomous vehicles vary in size. Perception networks processing sensor data may have tens of millions of parameters, while decision-making networks might be smaller.\n\nGenerative Adversarial Networks (GANs):\n\nLarge GANs, such as BigGAN, can have hundreds of millions of parameters and are used for image generation.\n\nClimate Science:\n\nClimate models using deep learning can have varying numbers of parameters, typically in the millions to tens of millions, depending on the model’s complexity.\n\n\n\n\nChoosing Depth and Width of Neural Networks\nSelecting the appropriate depth and width of neural networks is a crucial decision in deep learning. Here are common techniques and considerations for making this choice:\n\nModel Complexity vs. Data Size:\n\nConsider the balance between the model’s complexity and the size of the available data. Smaller datasets may benefit from simpler, shallower networks, while larger datasets can support deeper and wider architectures.\n\nEmpirical Exploration:\n\nStart with a basic architecture and experiment with deeper or wider networks to find the optimal balance. This involves training and evaluating different architectures to identify the best-performing one for the task.\n\nRegularization Techniques:\n\nImplement techniques like dropout, weight decay (L2 regularization), and batch normalization to prevent overfitting, enabling deeper networks without sacrificing generalization performance.\n\nTransfer Learning:\n\nFor certain applications, consider transfer learning from pretrained models. Fine-tuning a pretrained model may require fewer layers and parameters, saving training time.\n\nArchitectural Variations:\n\nExplore different architectural variations, such as the number of layers, hidden units, and filter sizes. Techniques like grid search or random search can be employed to discover promising architectures.\n\nArchitectural Search Algorithms:\n\nUtilize automated neural architecture search (NAS) algorithms, including reinforcement learning-based methods and evolutionary algorithms, to discover optimal neural network architectures automatically.\n\nModel Size and Computational Resources:\n\nBe mindful of computational resources when choosing the depth and width. Deeper and wider models demand more training time and memory.\n\nTask Complexity:\n\nThe complexity of the task can guide architectural decisions. Simple tasks may require shallower networks, while complex tasks may benefit from deeper and wider architectures.\n\nPruning and Quantization:\n\nAfter training, apply pruning or quantization techniques to reduce the size and complexity of trained models without sacrificing performance.\n\nEnsemble Methods:\n\nConsider using ensemble methods that combine predictions from multiple neural networks with different architectures to enhance overall performance.\n\nDomain Expertise:\n\nKnowledge of the domain and task can guide architectural choices. For example, specific data types, like sequential data in NLP, may benefit from recurrent or attention-based architectures.\n\nValidation and Cross-Validation:\n\nLeverage cross-validation and validation performance metrics to identify the appropriate trade-off between model capacity and generalization.\n\n\nThe choice of architecture often involves a trade-off between model capacity and the risk of overfitting. Experimentation may be required to determine the ideal architecture for a particular task. The deep learning field continues to evolve, offering new techniques and tools to assist with architecture selection."
  },
  {
    "objectID": "lesson07/index.html#neural-networks-in-r",
    "href": "lesson07/index.html#neural-networks-in-r",
    "title": "Lesson 7",
    "section": "  Neural Networks in R",
    "text": "Neural Networks in R\nWe have already seen one example of training a neural network in R using the brulee package which fits in with the tidymodels framework. There are another of other packages which facilitate deep learning in R and it is worth being aware of them since some of them allow one to go beyond what is capable with brulee and implement CNNs or RNNs.\n\nR Packages for Neural Networks and Deep Learning\n\nkeras:\n\nDescription: The keras package in R provides an interface to the Keras deep learning framework. Keras is known for its ease of use and flexibility and can run on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK).\nDocumentation: keras Documentation\nTutorials: keras Tutorials\nLab from Textbook: The webpage for the ISLR2 textbook contains a link to a lab that uses the keras package. View lab.\n\ntensorflow:\n\nDescription: The tensorflow package for R provides a low-level interface to the TensorFlow deep learning framework. It allows for fine-grained control over model architecture and training.\nDocumentation: tensorflow Documentation\nTutorials: tensorflow Tutorials\n\ntorch:\n\nDescription: The torch package offers an interface to PyTorch, a deep learning framework known for its flexibility and dynamic computation graph. It is suitable for both research and production.\nDocumentation: torch Documentation\nTextbook: Deep Learning and Scientific Computing with R\nLab from Textbook: The webpage for the ISLR2 textbook contains a link to a lab that uses the torch package. View lab.\n\ncaret:\n\nDescription: While not a deep learning library, the caret package is a versatile tool for training and evaluating machine learning models, including neural networks. It provides a unified interface for various R packages and algorithms.\nDocumentation: caret Documentation\n\nh2o:\n\nDescription: The h2o package is designed for scalable machine learning and deep learning. It provides an easy-to-use interface for building deep learning models, autoML, and more.\nDocumentation: h2o Documentation\n\n\nThese R packages cover a range of deep learning needs, from high-level and user-friendly interfaces to low-level control and flexibility. Explore the provided documentation and tutorials to get started with deep learning using these tools."
  },
  {
    "objectID": "lesson07/index.html#freely-available-libraries-and-packages-for-neural-networks",
    "href": "lesson07/index.html#freely-available-libraries-and-packages-for-neural-networks",
    "title": "Lesson 7",
    "section": "Freely Available Libraries and Packages for Neural Networks",
    "text": "Freely Available Libraries and Packages for Neural Networks\n\nOther Languages\n\nTensorFlow:\n\nDescription: TensorFlow is an open-source deep learning framework by Google, widely used for building and training neural networks.\nDocumentation: TensorFlow Official Documentation\nTutorials: TensorFlow Tutorials\n\nKeras:\n\nDescription: Keras is a high-level neural networks API that runs on top of TensorFlow and other frameworks, simplifying model building.\nDocumentation: Keras Official Documentation\nTutorials: Keras Tutorials\n\nPyTorch:\n\nDescription: PyTorch is a deep learning framework known for its flexibility and dynamic computation graph, ideal for research and development.\nDocumentation: PyTorch Official Documentation\nTutorials: PyTorch Tutorials\n\nscikit-learn:\n\nDescription: While primarily focused on traditional machine learning, scikit-learn offers neural network capabilities for integration with other machine learning tasks.\nDocumentation: scikit-learn Official Documentation\nTutorials: scikit-learn Neural Networks Guide\n\nFastai:\n\nDescription: Fastai is a high-level deep learning library built on PyTorch, designed to make deep learning more accessible for practitioners.\nDocumentation: Fastai Documentation\nTutorials: Fastai Tutorials\n\nFlux.jl:\n\nDescription: Flux is a widely used deep learning library in Julia known for its simplicity and flexibility. It provides a user-friendly API for defining and training neural networks.\nDocumentation: Flux.jl Documentation\n\nKnet.jl:\n\nDescription: Knet (pronounced “kay-net”) is a deep learning framework that aims to be as fast and efficient as possible. It’s suitable for both research and production use.\nDocumentation: Knet.jl Documentation\n\n\nThese libraries and packages offer diverse capabilities and features for implementing and training neural networks. Explore the provided documentation and tutorials to get started with deep learning using these tools."
  },
  {
    "objectID": "lesson07/index.html#r-packages-for-neural-networks-and-deep-learning",
    "href": "lesson07/index.html#r-packages-for-neural-networks-and-deep-learning",
    "title": "Lesson 7",
    "section": "R Packages for Neural Networks and Deep Learning",
    "text": "R Packages for Neural Networks and Deep Learning\n\nkeras:\n\nDescription: The keras package in R provides an interface to the Keras deep learning framework. Keras is known for its ease of use and flexibility and can run on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK).\nDocumentation: keras Documentation\nTutorials: keras Tutorials\n\ntensorflow:\n\nDescription: The tensorflow package for R provides a low-level interface to the TensorFlow deep learning framework. It allows for fine-grained control over model architecture and training.\nDocumentation: tensorflow Documentation\nTutorials: tensorflow Tutorials\n\ntorch:\n\nDescription: The torch package offers an interface to PyTorch, a deep learning framework known for its flexibility and dynamic computation graph. It is suitable for both research and production.\nDocumentation: torch Documentation\nTextbook: Deep Learning and Scientific Computing with R\n\ncaret:\n\nDescription: While not a deep learning library, the caret package is a versatile tool for training and evaluating machine learning models, including neural networks. It provides a unified interface for various R packages and algorithms.\nDocumentation: caret Documentation\n\nh2o:\n\nDescription: The h2o package is designed for scalable machine learning and deep learning. It provides an easy-to-use interface for building deep learning models, autoML, and more.\nDocumentation: h2o Documentation\n\n\nThese R packages cover a range of deep learning needs, from high-level and user-friendly interfaces to low-level control and flexibility. Explore the provided documentation and tutorials to get started with deep learning using these tools.\n\nOther Languages\n\nTensorFlow:\n\nDescription: TensorFlow is an open-source deep learning framework by Google, widely used for building and training neural networks.\nDocumentation: TensorFlow Official Documentation\nTutorials: TensorFlow Tutorials\n\nKeras:\n\nDescription: Keras is a high-level neural networks API that runs on top of TensorFlow and other frameworks, simplifying model building.\nDocumentation: Keras Official Documentation\nTutorials: Keras Tutorials\n\nPyTorch:\n\nDescription: PyTorch is a deep learning framework known for its flexibility and dynamic computation graph, ideal for research and development.\nDocumentation: PyTorch Official Documentation\nTutorials: PyTorch Tutorials\n\nscikit-learn:\n\nDescription: While primarily focused on traditional machine learning, scikit-learn offers neural network capabilities for integration with other machine learning tasks.\nDocumentation: scikit-learn Official Documentation\nTutorials: scikit-learn Neural Networks Guide\n\nFastai:\n\nDescription: Fastai is a high-level deep learning library built on PyTorch, designed to make deep learning more accessible for practitioners.\nDocumentation: Fastai Documentation\nTutorials: Fastai Tutorials\n\nFlux.jl:\n\nDescription: Flux is a widely used deep learning library in Julia known for its simplicity and flexibility. It provides a user-friendly API for defining and training neural networks.\nDocumentation: Flux.jl Documentation\n\nKnet.jl:\n\nDescription: Knet (pronounced “kay-net”) is a deep learning framework that aims to be as fast and efficient as possible. It’s suitable for both research and production use.\nDocumentation: Knet.jl Documentation\n\n\nThese libraries and packages offer diverse capabilities and features for implementing and training neural networks. Explore the provided documentation and tutorials to get started with deep learning using these tools."
  },
  {
    "objectID": "lesson07/index.html#activation-functions-for-neural-networks",
    "href": "lesson07/index.html#activation-functions-for-neural-networks",
    "title": "Lesson 7",
    "section": "Activation Functions for Neural Networks",
    "text": "Activation Functions for Neural Networks\n\nSigmoid Activation Function:\n\nDescription: The sigmoid activation function is commonly used in neural networks for binary classification tasks. It maps input values to the range ((0, 1)), making it suitable for output layers of binary classifiers.\nMathematical Expression:\n\n\\[\n\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n\\]\nReLU (Rectified Linear Unit) Activation Function:\n\nDescription: ReLU is a widely used activation function that introduces non-linearity by returning the input for positive values and zero for negative values. It helps mitigate the vanishing gradient problem.\nMathematical Expression:\n\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nTanh (Hyperbolic Tangent) Activation Function:\n\nDescription: Tanh is another common activation function that maps input values to the range ((-1, 1)). It provides zero-centered output, which can help training converge faster.\nMathematical Expression:\n\n\\[\n\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nLeaky ReLU Activation Function:\n\nDescription: Leaky ReLU is a variation of ReLU that allows a small gradient when the input is negative. It addresses the “dying ReLU” problem by preventing neurons from becoming inactive.\nMathematical Expression:\n\n\\[\n\\text{Leaky ReLU}(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\ \\alpha x, & \\text{if } x &lt; 0 \\end{cases}\n\\]\n\nwhere \\(\\alpha\\) is a small constant.\nUse these activation functions and their mathematical expressions to understand and describe their behavior in neural networks.\n\n\nCode\n# Create a data frame with x values\nx &lt;- seq(-3, 3, length.out = 1000)\ndf &lt;- data.frame(x = x)\n\n# Sigmoid Activation Function\ndf$sigmoid &lt;- 1 / (1 + exp(-x))\n\n# ReLU (Rectified Linear Unit) Activation Function\ndf$relu &lt;- pmax(0, x)\n\n# Tanh (Hyperbolic Tangent) Activation Function\ndf$tanh &lt;- (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\n# Leaky ReLU Activation Function\nleaky_relu &lt;- function(x, alpha = 0.01) {\n  ifelse(x &gt;= 0, x, alpha * x)\n}\ndf$leaky_relu &lt;- leaky_relu(x)\n\n# Plot the activation functions\nggplot(df, aes(x)) +\n  geom_line(aes(y = sigmoid, color = \"Sigmoid\"), linewidth = 1) +\n  geom_line(aes(y = relu, color = \"ReLU\"), linewidth = 1) +\n  geom_line(aes(y = tanh, color = \"Tanh\"), linewidth = 1) +\n  geom_line(aes(y = leaky_relu, color = \"Leaky ReLU\"), linewidth = 1) +\n  labs(\n    title = \"Common Activation Functions for Neural Networks\",\n    x = \"Input (x)\",\n    y = \"Output\",\n    color = \"Activation Function\"\n  ) +\n  scale_color_manual(values = c(\"Sigmoid\" = \"#E69F00\", \"ReLU\" = \"#56B4E9\", \"Tanh\" = \"#009E73\", \"Leaky ReLU\" = \"#CC79A7\")) +\n  theme_minimal()\n\n\n\n\n\nFigure 4: Activation functions used in neural networks.\n\n\n\n\n\nSingle Layer Networks\n\n\nMultilayer networks\n\n\nConvolutional and Recurrent Networks"
  },
  {
    "objectID": "lesson07/index.html#convolutional-and-recurrent-networks",
    "href": "lesson07/index.html#convolutional-and-recurrent-networks",
    "title": "Lesson 7",
    "section": "Convolutional and Recurrent Networks",
    "text": "Convolutional and Recurrent Networks\n\nOverview of Neural Network Types\nThus far, we have been discussing what are known as feedforward neural networks. However, there are variations on this that are worth describing as different kinds of neural networks are used for different kinds of tasks, including image recognition, natural language processing, and time series analysis. There are three main types of neural networks: feedforward, convolutional, and recurrent networks.\n\nFeedforward Neural Networks (FNNs)\n\nDescription: Feedforward neural networks are the simplest type of neural network. They consist of an input layer, one or more hidden layers, and an output layer. Information flows in one direction, from input to output, without cycles or loops.\nUse Cases: FNNs are commonly used for tasks like classification and regression.\nAdvantages: Easy to implement, work well for structured data.\nDisadvantages: Limited for tasks involving sequences or spatial data.\n\n\n\nConvolutional Neural Networks (CNNs)\n\nDescription: Convolutional neural networks are designed for tasks involving grid-like data, such as images. They use convolutional layers to automatically learn and detect features at different spatial hierarchies.\nUse Cases: CNNs excel at image classification, object detection, and segmentation.\nAdvantages: Hierarchical feature learning, translational invariance.\nDisadvantages: May require substantial data, computationally intensive.\n\n\n\nRecurrent Neural Networks (RNNs)\n\nDescription: Recurrent neural networks are specialized for sequence data. They use recurrent connections to maintain memory of previous inputs. This enables tasks that depend on sequence context.\nUse Cases: RNNs are used for tasks like time series prediction, speech recognition, and natural language processing.\nAdvantages: Sequence modeling, dynamic context understanding.\nDisadvantages: Vulnerable to vanishing gradients, limited memory.\n\n\n\nComparison and Contrast\n\nData Type: FNNs work well with structured data, while CNNs are designed for grid-like data (e.g., images), and RNNs are for sequential data.\nArchitecture: FNNs have no internal memory or feedback loops, while RNNs maintain memory through recurrent connections. CNNs have convolutional layers for feature extraction.\nApplications: FNNs are suitable for tabular data, CNNs for image-related tasks, and RNNs for sequences (text, time series).\nTraining: FNNs are typically trained via backpropagation, CNNs leverage convolutional filters, and RNNs use backpropagation through time (BPTT).\n\nEach type of neural network is tailored to specific data types and tasks. The choice of network architecture depends on the problem at hand, and in some cases, hybrid models may be used to leverage the strengths of multiple network types.\n\n\n\nConvolutional Neural Networks (CNNs)\nConvolutional Neural Networks (CNNs) are a type of neural network designed for processing grid-like data, with a primary focus on tasks related to images and other spatial data. CNNs have been instrumental in revolutionizing computer vision and are widely used for tasks like image classification, object detection, and segmentation.\n\nArchitecture of CNNs\nCNNs are characterized by a unique architecture that is well-suited for extracting features from grid-like data:\n\nConvolutional Layers: These layers are responsible for feature extraction. They consist of learnable filters (kernels) that scan the input data, capturing local patterns. Convolutional layers create feature maps that highlight relevant spatial features.\nPooling Layers: Pooling layers reduce the spatial dimensions of the feature maps, decreasing the computational load. Max pooling is a common approach where the maximum value in a local region is retained.\nFully Connected Layers: After feature extraction, CNNs often have one or more fully connected layers similar to those in feedforward networks. These layers combine the features to make predictions.\n\nFigure 5 provides an illustration of the CNN architecture. For further perspective on CNNs, view this article.\n\n\n\nFigure 5: Architecture of CNNs.\n\n\n\n\nTraining Process for CNNs\nThe training process for CNNs involves the following key steps:\n\nInitialization: Weights and biases in the network are initialized, typically with small random values.\nForward Propagation: During forward propagation, input data is passed through the network. Convolutional and pooling layers extract features, while fully connected layers produce predictions.\nLoss Calculation: A loss function is used to measure the difference between the predicted values and the ground truth. Common loss functions include cross-entropy for classification tasks.\nBackpropagation: During backpropagation, gradients are computed with respect to the loss. Gradients are used to adjust the network’s weights and biases in a direction that minimizes the loss.\nOptimization: Various optimization algorithms, like stochastic gradient descent (SGD) or its variants (e.g., Adam), are used to update the network’s parameters. The learning rate determines the size of weight updates.\nTraining Loop: Steps 2 to 5 are repeated iteratively for a fixed number of epochs or until convergence. Mini-batch training is common, where the data is divided into small batches for more efficient training.\n\n\n\nCNNs in Practice\nIn practice, CNNs are often pre-trained on large datasets (e.g., ImageNet) to learn useful feature representations. Transfer learning allows fine-tuning these pre-trained models for specific tasks, reducing the need for large datasets.\nThe success of CNNs can be attributed to their ability to automatically learn hierarchical features from raw data. They excel at capturing patterns in different spatial hierarchies, making them suitable for a wide range of computer vision tasks.\n\n\n\nRecurrent Neural Networks (RNNs)\nRecurrent Neural Networks (RNNs) are a type of neural network specially designed for sequential data and tasks that depend on maintaining memory of previous inputs. RNNs have been widely used in natural language processing, time series analysis, speech recognition, and more.\n\nArchitecture of RNNs\nRNNs are characterized by their recurrent connections, which enable the network to maintain memory across time steps. The basic architecture of an RNN consists of the following components:\n\nHidden State: At each time step, an RNN maintains a hidden state that serves as a memory of past inputs and computations.\nRecurrent Connection: The hidden state at the current time step is computed based on the input at the current time step and the hidden state at the previous time step. This recurrent connection allows RNNs to capture dependencies across sequential data.\nOutput Layer: RNNs can have an output layer, which produces predictions or features based on the current hidden state. The output can be used for various tasks, such as sequence classification or prediction.\n\nFigure 6 provides an illustration of the RNN architecture. For further perspective on CNNs, view this article.\n\n\n\nFigure 6: Architecture of an RNN.\n\n\n\n\nTraining Process for RNNs\nThe training process for RNNs involves the following key steps:\n\nInitialization: Weights and biases in the network are initialized, typically with small random values.\nForward Propagation: During forward propagation, input data is passed through the network one time step at a time. The hidden state is updated at each time step based on the input and the previous hidden state.\nLoss Calculation: A loss function is used to measure the difference between the predicted values and the ground truth. Common loss functions include mean squared error for regression tasks and cross-entropy for classification tasks.\nBackpropagation Through Time (BPTT): BPTT is a variant of backpropagation used to compute gradients through time. It involves calculating gradients of the loss with respect to the network’s parameters, considering all time steps.\nOptimization: Gradients are used to adjust the network’s weights and biases using optimization algorithms like stochastic gradient descent (SGD) or its variants.\nTraining Loop: Steps 2 to 5 are repeated iteratively for a fixed number of time steps or until convergence. Mini-batch training is common, where the data is divided into small batches for more efficient training.\n\n\n\nRNNs in Practice\nIn practice, RNNs may face challenges like vanishing gradients, where gradients become extremely small and hinder the training process. To address this, variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been developed with more sophisticated gating mechanisms.\nRNNs are effective for tasks that involve sequential data, as they can capture dependencies and temporal patterns. They have been widely used for natural language processing, including tasks like language modeling, machine translation, and sentiment analysis.\nDespite their effectiveness, RNNs may be computationally expensive and may not scale well to very long sequences. In such cases, alternatives like attention mechanisms or Transformers have gained popularity."
  },
  {
    "objectID": "lesson07/index.html#multilayer-networks",
    "href": "lesson07/index.html#multilayer-networks",
    "title": "Lesson 7",
    "section": "Multilayer networks",
    "text": "Multilayer networks\nIn principle, a single layer neural network can approximate most functions of interest in machine learning. This is a consequence of a family of results known as universal approximation theorems. However, in practice, deep learning is improved by using multiple layers. A neural network with multiple layers is known as a multilayer neural network. Let’s examine the training of a multilayer neural network in more detail.\n\nBackpropagation\nThe implementation of gradient descent or its variant in deep learning typically requires us to compute the gradient of chains of functions like\n\\[\n{\\bf y} = (f_{K} \\circ f_{K-1} \\circ \\cdots \\circ f_{1})({\\bf x})\n\\] where \\({\\bf x}\\) are the inputs, \\({\\bf y}\\) are the observations, and every \\(f_{i}\\) has its own parameters. In neural networks,\n\\[\nf_{i}({\\bf x}_{i-1}) = \\sigma({\\bf W}_{i-1}{\\bf x}_{i-1} + {\\bf b}_{i-1})\n\\] is the activation function for the \\(i\\)-th layer, where \\({\\bf W}_{i-1}\\) is the weight matrix and \\({\\bf b}_{i-1}\\) is the bias vector. Here, \\({\\bf x}_{i-1}\\) is the output from layer \\(i-1\\). To train such a model, we need to compute the gradient of the loss function with respect to the parameters \\({\\bf W}_{i}\\) and \\({\\bf b}_{i}\\) for each layer \\(i\\). This is done via the backpropagation algorithm. We will now explain how this works. For a reference, see section 5.6.1 from (Deisenroth, Faisal, and Ong 2020). See also these online notes."
  }
]