---
title: "Lesson 8"
subtitle: "Support Vector Machines"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson08.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(caret)
library(ISLR2)
library(vip)
library(pdp)
library(kernlab)
library(svmpath)
library(grid)
library(lattice)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))

# Colors
dark2 <- RColorBrewer::brewer.pal(8, "Dark2")
set1 <- RColorBrewer::brewer.pal(9, "Set1")

# Plotting function; modified from svmpath::svmpath()
plot_svmpath <- function(x, step = max(x$Step), main = "") {
  
  # Extract model info
  object <- x
  f <- predict(object, lambda = object$lambda[step], type = "function")
  x <- object$x
  y <- object$y
  Elbow <- object$Elbow[[step]]
  alpha <- object$alpha[, step]
  alpha0 <- object$alpha0[step]
  lambda <- object$lambda[step]
  df <- as.data.frame(x[, 1L:2L])
  names(df) <- c("x1", "x2")
  df$y <- norm2d$y
  beta <- (alpha * y) %*% x

  # Construct plot
  ggplot(df, aes(x = x1, y = x2)) +
    geom_point(aes(shape = y, color = y), size = 3, alpha = 0.75) +
    xlab("Income (standardized)") +
    ylab("Lot size (standardized)") +
    xlim(-6, 6) +
    ylim(-6, 6) +
    coord_fixed() +
    theme(legend.position = "none") +
    theme_bw() +
    scale_shape_discrete(
      name = "Owns a riding\nmower?",
      breaks = c(1, 2),
      labels = c("Yes", "No")
    ) +
    scale_color_brewer(
      name = "Owns a riding\nmower?",
      palette = "Dark2",
      breaks = c(1, 2),
      labels = c("Yes", "No")
    ) +
    geom_abline(intercept = -alpha0/beta[2], slope = -beta[1]/beta[2], 
                color = "black") +
    geom_abline(intercept = lambda/beta[2] - alpha0/beta[2], 
                slope = -beta[1]/beta[2], 
                color = "black", linetype = 2) +
    geom_abline(intercept = -lambda/beta[2] - alpha0/beta[2], 
                slope = -beta[1]/beta[2], 
                color = "black", linetype = 2) +
    geom_point(data = df[Elbow, ], size = 3) +
    ggtitle(main)
    
}

# Load attrition data
df <- attrition %>% 
  mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = 0.7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the support vector machine (SVM) approach to classification. 

- Understand the role of kernels in SVM.

- Use the `tidymodels` workflow to fit and tune various SVM classification models. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 9 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 14 of [Hands-On Machine Learning with R*](https://bradleyboehmke.github.io/HOML/) [@boehmke2019hands].

- [View Support Vector Classifier video on YouTube](https://youtu.be/pjvnCEfAswc?si=5I-xmhB3Y006RP0Y).

```{r}
#| echo: false

vembedr::embed_youtube(id="pjvnCEfAswc?si=5I-xmhB3Y006RP0Y",height=450) %>%
  vembedr::use_align("center")
```

- [View Support Vector Classifiers in R video on YouTube](https://youtu.be/WCRwbrNWrpw?si=rkEz30HgpeBK9VTD).
    
```{r}
#| echo: false

vembedr::embed_youtube(id="WCRwbrNWrpw?si=rkEz30HgpeBK9VTD",height=450) %>%
  vembedr::use_align("center")
```

    
## Overview    

[Support Vector Machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) are a class of powerful machine learning algorithms commonly used for classification tasks but can also be used for regression. The fundamental principle behind SVM is to find a hyperplane that maximizes the so-called margin between different classes in the data. This hyperplane serves as the decision boundary, where data points are classified into one of two or more classes based on their position relative to the hyperplane. SVM is particularly effective in scenarios where the classes are well-separated and works well in high-dimensional spaces. The choice of the kernel function in SVM allows for handling nonlinear data by mapping it to a higher-dimensional space, making it versatile for a wide range of problems.

Variations of SVM include:
- [Support Vector Machine with a Linear Kernel (Linear SVM)](https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM)
- [Support Vector Machine with a Polynomial Kernel (Polynomial SVM)](https://en.wikipedia.org/wiki/Support_vector_machine#Polynomial_kernel)
- [Support Vector Machine with a Radial Basis Function Kernel (RBF SVM)](https://en.wikipedia.org/wiki/Support_vector_machine#RBF_kernel)

These variations offer flexibility to adapt to different data distributions. Additionally, SVM can be extended to multiclass classification by using techniques like one-vs-all. Despite its effectiveness, SVM's performance can be sensitive to the choice of kernel and hyperparameters, and it may be computationally expensive for large data sets. However, it remains a valuable tool in the machine learning toolkit, especially when dealing with well-defined classes and both linear and non-linear data.

## Hyperplanes and Margins

Recall that a linear equataion like

$$
f({\bf x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p = 0
$$

determines a hyperplane in $p$-dimensional space. @fig-hyperplanes shows examples of hyperplanes in 2-D and 3-D feature space.

```{r}
#| code-fold: true
#| label: fig-hyperplanes
#| fig-cap: Examples of hyperplanes in 2-D and 3-D feature space.

# Construct data for plotting
x1 <- x2 <- seq(from = 0, to = 1, length = 100)
xgrid <- expand.grid(x1 = x1, x2 = x2)
y1 <- 1 + 2 * x1
y2 <- 1 + 2 * xgrid$x1 + 3 * xgrid$x2

# Hyperplane: p = 2
p1 <- lattice::xyplot(
  x = y1 ~ x1, 
  type = "l", 
  col = "black", 
  xlab = expression(X[1]), 
  ylab = expression(X[2]),
  main = expression({f(X)==1+2*X[1]-X[2]}==0),
  scales = list(tck = c(1, 0))
)

# Hyperplane: p = 3
p2 <- lattice::wireframe(
  x = y2 ~ xgrid$x1 * xgrid$x2, 
  xlab = expression(X[1]), 
  ylab = expression(X[2]),
  zlab = expression(X[3]),
  main = expression({f(X)==1+2*X[1]+3*X[2]-X[3]}==0),
  drape = TRUE,
  colorkey = FALSE,
  col = dark2[1],
  scales = list(arrows = FALSE)
  # par.settings = list(axis.line = list(col = "transparent"))
)

# Display plots side by side
gridExtra::grid.arrange(p1, p2, nrow = 1)

```


Further, a hyperplane separates the feature space into two half-spaces. Namely, $f({\bf x}) > 0$ and $f({\bf x}) < 0$. We can try to use this geometric fact to classify observations into two classes. For example, we can classify an observation ${\bf x}_i$ as belonging to class $y_{i} = 1$ if $f({\bf x}_i) > 0$ and as belonging to class $y_{i} = -1$ if $f({\bf x}_i) < 0$. Note that this can be summarized as:  

> ${\bf x}_i$ belongs to class $y_{i}$ if $y_i \times f({\bf x}_i) > 0$


In such a case, we refer to the hyperplane as a **decision boundary**. If a hyperplane separates the feature space into two half-spaces in such a way as to distinguish binary classes, then we call that hyperplane a **separating hyperplane**. The problem is that in general there can be an infinite number of separating hyperplanes for a given data set. For example, @fig-svm-separating-hyperplanes shows multiple separating hyperplanes for a simulated data set with two classes.


```{r}
#| code-fold: true
#| label: fig-svm-separating-hyperplanes
#| fig-cap: Simulated binary classification data with two separable classes.
#| warning: false
#| message: false

# Simulate data
set.seed(805)
norm2d <- as.data.frame(mlbench::mlbench.2dnormals(
  n = 100,
  cl = 2,
  r = 4,
  sd = 1
))
names(norm2d) <- c("x1", "x2", "y")  # rename columns

# Scatterplot
p1 <- ggplot(norm2d, aes(x = x1, y = x2)) +
  geom_point(aes(shape = y, color = y), size = 3, alpha = 0.75) +
  xlab("Income (standardized)") +
  ylab("Lot size (standardized)") +
  xlim(-6, 6) +
  ylim(-6, 6) +
  coord_fixed() +
  theme(legend.position = "none") +
  theme_bw() +
  scale_shape_discrete(
    name = "Owns a riding\nmower?",
    breaks = c(1, 2),
    labels = c("Yes", "No")
  ) +
  scale_color_brewer(
    name = "Owns a riding\nmower?",
    palette = "Dark2",
    breaks = c(1, 2),
    labels = c("Yes", "No")
  )

# Fit a Logistic regression, linear discriminant analysis (LDA), and optimal
# separating hyperplane (OSH). Note: we sometimes refer to the OSH as the hard 
# margin classifier
fit_glm <- glm(as.factor(y) ~ ., data = norm2d, family = binomial)
fit_lda <- MASS::lda(as.factor(y) ~ ., data = norm2d)
invisible(capture.output(fit_hmc <- ksvm(  # use ksvm() to find the OSH
  x = data.matrix(norm2d[c("x1", "x2")]),
  y = as.factor(norm2d$y), 
  kernel = "vanilladot",  # no fancy kernel, just ordinary dot product
  C = Inf,                # to approximate hard margin classifier
  prob.model = TRUE       # needed to obtain predicted probabilities
)))

# Grid over which to evaluate decision boundaries
npts <- 500
xgrid <- expand.grid(
  x1 = seq(from = -6, 6, length = npts),
  x2 = seq(from = -6, 6, length = npts)
)

# Predicted probabilities (as a two-column matrix)
prob_glm <- predict(fit_glm, newdata = xgrid, type = "response")
prob_glm <- cbind("1" = 1 - prob_glm, "2" = prob_glm)
prob_lda <- predict(fit_lda, newdata = xgrid)$posterior
prob_hmc <- predict(fit_hmc, newdata = xgrid, type = "probabilities")

# Add predicted class probabilities
xgrid2 <- xgrid %>%
  cbind("GLM" = prob_glm[, 1L], 
        "LDA" = prob_lda[, 1L], 
        "HMC" = prob_hmc[, 1L]) %>%
  tidyr::gather(Model, Prob, -x1, -x2)

# Scatterplot with decision boundaries
p2 <- p1 + 
  stat_contour(data = xgrid2, aes(x = x1, y = x2, z = Prob, linetype = Model), 
               breaks = 0.5, color = "black")

# Display plots side by side
p2
```
    
Thus, our problem is to find a separating hyperplane that is "best" in some particular sense. Of course, whatever "best" means it should be with respect to performance on a test set and not with respect to performance on the training set. Otherwise, we will overfit the training data. The intuitive idea is to find a separating hyperplane which as a decision boundary  provides the maximum separation between the two classes. 
    
### Maximal Marginal Classifier

This is illustrated in @fig-max-margin. The solid line is the decision boundary and the dotted lines are the margins. The margin is the distance between the decision boundary and the closest observation from either class. The observations that are closest to the decision boundary are called **support vectors**. The decision boundary is called the **maximal margin classifier**. 


![The solid lines shows the maximal margin hyperplane. The margin is the distance between the two dotted lines.](https://www.dropbox.com/scl/fi/z6bfo6dl85hddo7kceqf9/9_3.jpg?rlkey=erj5245hguu6zfn6yx8y39u4p&raw=1){#fig-max-margin width=75% height=75%}    


The maximal margin classifier is constructed as a solution to the following optimization problem:

$$
\text{maximize}_{\beta_0,\beta_{1},\ldots, \beta_{p}} M 
$$

subject to 


$$
\begin{align*}
& \sum_{j=1}^{p} \beta_{j}^{2} = 1 \\
& y_{i}\left(\beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij}\right) \geq M, 
\end{align*}
$$

for all $i = 1, 2, \ldots, n$. Put differently, the maximal margin classifier finds the separating hyperplane that provides the largest margin/gap between the two classes. The width of both margin boundaries is  
$M$.
 
What if it is not possible to perfectly separate the two classes? That is, what if there is no separating hyperplane? In this situation, we can loosen the constraints (or soften the margin) by allowing some points to be on the wrong side of the margin; this is referred to as the the soft margin classifier (SMC) or **support vector classifier**. 

## Support Vector Classifier

The soft margin classifier is constructed as a solution to an optimization problem modifying that of the maximal margin classifier:

$$
\text{maximize}_{\beta_0,\beta_{1},\ldots, \beta_{p}, \epsilon_{1},\ldots, \epsilon_{n}} M 
$$

subject to 


$$
\begin{align*}
& \sum_{j=1}^{p} \beta_{j}^{2} = 1 \\
& y_{i}\left(\beta_{0} + \sum_{j=1}^{p} \beta_{j} x_{ij}\right) \geq M(1 - \epsilon_{i}), \\
& \epsilon_{i} \geq 0, \quad \sum_{i=1}^{n} \epsilon_{i} \leq C,
\end{align*}
$$
    
for all $i = 1, 2, \ldots, n$. The hyperparameter $C$ is a non-negative tuning parameter that controls the amount of softening. The larger the value of $C$, the more points are allowed to be on the wrong side of the margin. The parameter $C$ is often referred to as the **budget**. The larger the budget, the more we are willing to pay for misclassification. The hyperparameter $C$ is often chosen via cross-validation. This is illustrated in @fig-smc. The left panel shows the maximal margin classifier. The right panel shows the soft margin classifier with a budget of $C = \infty$. The solid black points represent the support vectors that define the margin boundaries.   
    
![Soft margin classifier. (Left) Zero budget for overlap (i.e., the maximal margin). (Right) Maximumn allowable overlap. The solid black points represent the support vectors that define the margin boundaries.](https://bradleyboehmke.github.io/HOML/11b-svm_files/figure-html/smc-1.png){#fig-smc}

```{r}
#| include: false
#| code-fold: true
#| message: false
#| warning: false
#| label: fig-smc
#| fig-cap: Soft margin classifier. (Left) Zero budget for overlap (i.e., the maximal margin). (Right) Maximumn allowable overlap. The solid black points represent the support vectors that define the margin boundaries.
#| fig-asp: 0.5
#| fig-width: 8
#| fig-out.width: 100%

# Fit the entire regularization path
fit_smc <- svmpath(
  x = data.matrix(norm2d[c("x1", "x2")]), 
  y = ifelse(norm2d$y == 1, 1, -1)
)
# Plot both extremes
p1 <- plot_svmpath(fit_smc, step = max(fit_smc$Step), main = expression(C == 0))
p2 <- plot_svmpath(fit_smc, step = min(fit_smc$Step), main = expression(C == infinity))
gridExtra::grid.arrange(p1, p2, nrow = 1)
```



## Support Vector Machines

How should we handle situations where there is clearly a need for a nonlinear decision boundary? One approach is to use the so-called kernel trick. The idea is to enlarge the feature space by adding nonlinear transformations of the original features. This is illustrated in @fig-svm-circle. The left panel shows the two classes in the original feature space. The middle panel shows the two classes in the enlarged feature space. The right panel shows the decision boundary from the maximal margin in the enlarged feature space projected back into the original feature space. The decision boundary in the original feature space is nonlinear.

```{r}
#| code-fold: true
#| message: false
#| warning: false
#| label: fig-svm-circle
#| fig-cap: Simulated nested circle data. (Left) The two classes in the original (2-D) feature space. (Middle) The two classes in the enlarged (3-D) feature space. (Right) The decision boundary from the maximal margin in the enlarged feature space projected back into the original feature space.
#| fig-asp: 0.33
#| fig-width: 12
#| fig-out.width: 100%

# Simulate data
set.seed(1432)
circle <- as.data.frame(mlbench::mlbench.circle(
  n = 200,
  d = 2
))
names(circle) <- c("x1", "x2", "y")  # rename columns

# Fit a support vector machine (SVM)
fit_svm_poly <- ksvm( 
  x = data.matrix(circle[c("x1", "x2")]),
  y = as.factor(circle$y), 
  kernel = "polydot",       # polynomial kernel
  kpar = list(degree = 2),  # kernel parameters
  C = Inf,                  # to approximate maximal margin classifier
  prob.model = TRUE         # needed to obtain predicted probabilities
)

# Grid over which to evaluate decision boundaries
npts <- 500
xgrid <- expand.grid(
  x1 = seq(from = -1.25, 1.25, length = npts),
  x2 = seq(from = -1.25, 1.25, length = npts)
)

# Predicted probabilities (as a two-column matrix)
prob_svm_poly <- predict(fit_svm_poly, newdata = xgrid, type = "probabilities")

# Scatterplot
p1 <- contourplot(
  x = prob_svm_poly[, 1] ~ x1 * x2, 
  data = xgrid, 
  at = 0, 
  labels = FALSE,
  scales = list(tck = c(1, 0)),
  xlab = "x1",
  ylab = "x2",
  main = "Original feature space",
  panel = function(x, y, z, ...) {
    panel.contourplot(x, y, z, ...)
    panel.xyplot(
      x = circle$x1, 
      y = circle$x2, 
      groups = circle$y, 
      pch = 19, 
      cex = 1,
      col = adjustcolor(dark2[1L:2L], alpha.f = 0.5),
      ...
    )
  }
)

# Enlarge feature space
circle_3d <- circle
circle_3d$x3 <- circle_3d$x1^2 + circle_3d$x2^2

# 3-D scatterplot
p2 <- cloud(
  x = x3 ~ x1 * x2, 
  data = circle_3d, 
  groups = y,
  main = "Enlarged feature space",
  par.settings = list(
    superpose.symbol = list(
      pch = 19,
      cex = 1,
      col = adjustcolor(dark2[1L:2L], alpha.f = 0.5)
    )
  )
) 

# Scatterplot with decision boundary
p3 <- contourplot(
  x = prob_svm_poly[, 1] ~ x1 * x2, 
  data = xgrid, 
  at = 0.5, 
  labels = FALSE,
  scales = list(tck = c(1, 0)),
  xlab = "x1",
  ylab = "x2",
  main = "Non-linear decision boundary",
  panel = function(x, y, z, ...) {
    panel.contourplot(x, y, z, ...)
    panel.xyplot(
      x = circle$x1, 
      y = circle$x2, 
      groups = circle$y, 
      pch = 19, 
      cex = 1,
      col = adjustcolor(dark2[1L:2L], alpha.f = 0.5),
      ...
    )
  }
) 

# Combine plots
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

Support vector machines (SVMs) are methods that automate using kernels to enlarge the feature space. The idea is to find a hyperplane that separates the classes in the enlarged feature space. The hyperplane is then projected back into the original feature space. The decision boundary in the original feature space is nonlinear. The hyperplane is chosen to maximize the margin between the classes. The support vectors are the points that are closest to the hyperplane. The support vectors are the only points that influence the hyperplane. 

We will not go into the full mathematical detail of SVMs here but take a moment to describe some of the different kernel options that are used frequently in practice. First, recall the notation for an inner product:

$$
\langle {\bf x}_{i}, {\bf x}_{i'} \rangle = \sum_{j=1}^{p} x_{ij} x_{i'j}
$$
An inner product is a type of **kernel** and we generally write $K({\bf x}_{i}, {\bf x}_{i'})$ to denote a kernel function. Then, with some kernel we consider functions of the form

$$
f({\bf x}) = \beta_{0} + \sum_{i \in \mathcal{S}} \alpha_{i} K({\bf x}_{i}, {\bf x})
$$
where $\mathcal{S}$ is the set of support vectors. The most common kernels are:

- **Linear kernel**: $K({\bf x}_{i}, {\bf x}_{i'}) = \langle {\bf x}_{i}, {\bf x}_{i'} \rangle$

- **Polynomial kernel**: $K({\bf x}_{i}, {\bf x}_{i'}) = (1 + \langle {\bf x}_{i}, {\bf x}_{i'} \rangle)^{d}$

- **Radial basis function kernel**: $K({\bf x}_{i}, {\bf x}_{i'}) = \exp(-\gamma \langle {\bf x}_{i} - {\bf x}_{i'}, {\bf x}_{i} - {\bf x}_{i'} \rangle)$


### More Than Two Classes

SVMs can be extended to more than two classes using the **one-versus-one** approach. In this approach, we fit a separate SVM for each pair of classes. Then, to classify a new observation, we use a majority vote approach.

### Support Vector Regression

SVMs can also be used for regression. The idea is to find a hyperplane that is as close as possible to the data points. The hyperplane is chosen to minimize the sum of the distances between the hyperplane and the data points. The support vectors are the points that are closest to the hyperplane. The support vectors are the only points that influence the hyperplane.


## Summary    

Support Vector Machines (SVM) are a powerful and versatile class of supervised machine learning algorithms for classification. SVMs are based on the concept of decision planes that define decision boundaries. A decision plane is one that separates between a set of objects having different class memberships. SVMs are used for both classification and regression tasks. SVMs are particularly well suited for classification of complex but small- or medium-sized datasets.

### Key Concepts

1. **Maximizing Margin**: SVM's primary objective is to find a decision boundary that maximizes the margin between different classes of data points. This boundary is called the "hyperplane."

2. **Linear Separability**: SVM works well when data is linearly separable, meaning that it can be separated by a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions).

3. **Support Vectors**: The data points that are closest to the hyperplane and influence its position are called "support vectors." These support vectors play a crucial role in defining the decision boundary.

4. **Kernel Tricks**: SVM can handle non-linear data by applying kernel functions (e.g., polynomial, radial basis function) that transform the data into a higher-dimensional space, where it becomes linearly separable.

### SVM Classification Process

1. **Data Preparation**: SVM begins with labeled training data, where each data point is associated with a class label.

2. **Model Training**: The SVM algorithm learns the optimal hyperplane that best separates the classes while maximizing the margin. The hyperplane equation can be expressed as $f(x) = \mathbf{w} \cdot \mathbf{x} + b$, where $\mathbf{w}$ is the weight vector and $b$ is the bias term.

3. **Margin Calculation**: The margin is determined by the distance between the hyperplane and the nearest support vectors from each class.

4. **Classification**: To predict the class of a new data point, SVM evaluates $f(x)$. If $f(x) > 0$, the point is classified into one class; if $f(x) < 0$, it's classified into the other class.

### Hyperparameter Tuning

- **C Parameter**: It controls the trade-off between maximizing the margin and minimizing the classification error. Smaller values of C create a wider margin but may misclassify some points, while larger values of C lead to a narrower margin but fewer misclassifications.

- **Kernel Type**: The choice of kernel function, such as linear, polynomial, or radial basis function, can significantly impact the model's ability to handle non-linear data.

### Advantages of SVM

- Effective in high-dimensional spaces.
- Robust against overfitting.
- Suitable for small to large datasets.
- Works well with both linear and non-linear data.

### Limitations

- Computationally expensive for large datasets.
- Sensitivity to the choice of kernel and hyperparameters.
- Can be challenging to interpret in high-dimensional spaces.

SVM is a versatile algorithm widely used for classification tasks, especially when dealing with well-defined classes and both linear and non-linear data. It's essential to understand its hyperparameters and the choice of kernel functions to maximize its effectiveness in different scenarios.



## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)