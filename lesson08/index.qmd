---
title: "Lesson 8"
subtitle: "Support Vector Machines"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson08.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(caret)
library(ISLR2)
library(vip)
library(pdp)
library(kernlab)
library(svmpath)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))

# Colors
dark2 <- RColorBrewer::brewer.pal(8, "Dark2")
set1 <- RColorBrewer::brewer.pal(9, "Set1")

# Plotting function; modified from svmpath::svmpath()
plot_svmpath <- function(x, step = max(x$Step), main = "") {
  
  # Extract model info
  object <- x
  f <- predict(object, lambda = object$lambda[step], type = "function")
  x <- object$x
  y <- object$y
  Elbow <- object$Elbow[[step]]
  alpha <- object$alpha[, step]
  alpha0 <- object$alpha0[step]
  lambda <- object$lambda[step]
  df <- as.data.frame(x[, 1L:2L])
  names(df) <- c("x1", "x2")
  df$y <- norm2d$y
  beta <- (alpha * y) %*% x

  # Construct plot
  ggplot(df, aes(x = x1, y = x2)) +
    geom_point(aes(shape = y, color = y), size = 3, alpha = 0.75) +
    xlab("Income (standardized)") +
    ylab("Lot size (standardized)") +
    xlim(-6, 6) +
    ylim(-6, 6) +
    coord_fixed() +
    theme(legend.position = "none") +
    theme_bw() +
    scale_shape_discrete(
      name = "Owns a riding\nmower?",
      breaks = c(1, 2),
      labels = c("Yes", "No")
    ) +
    scale_color_brewer(
      name = "Owns a riding\nmower?",
      palette = "Dark2",
      breaks = c(1, 2),
      labels = c("Yes", "No")
    ) +
    geom_abline(intercept = -alpha0/beta[2], slope = -beta[1]/beta[2], 
                color = "black") +
    geom_abline(intercept = lambda/beta[2] - alpha0/beta[2], 
                slope = -beta[1]/beta[2], 
                color = "black", linetype = 2) +
    geom_abline(intercept = -lambda/beta[2] - alpha0/beta[2], 
                slope = -beta[1]/beta[2], 
                color = "black", linetype = 2) +
    geom_point(data = df[Elbow, ], size = 3) +
    ggtitle(main)
    
}

# Load attrition data
df <- attrition %>% 
  mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = 0.7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the support vector machine (SVM) approach to classification. 

- Use the `tidymodels` workflow to fit and tune various SVM classification models. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 9 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 9 of *Statistical Learning with Math and R* [@suzuki2020statistical].

- [View Support Vector Classifier video on YouTube](https://youtu.be/pjvnCEfAswc?si=5I-xmhB3Y006RP0Y).

```{r}
#| echo: false

vembedr::embed_youtube(id="pjvnCEfAswc?si=5I-xmhB3Y006RP0Y",height=450) %>%
  vembedr::use_align("center")
```

- [View Support Vector Classifiers in R video on YouTube](https://youtu.be/WCRwbrNWrpw?si=rkEz30HgpeBK9VTD).
    
```{r}
#| echo: false

vembedr::embed_youtube(id="WCRwbrNWrpw?si=rkEz30HgpeBK9VTD",height=450) %>%
  vembedr::use_align("center")
```

    
## Overview    

[Support Vector Machines (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) are a class of powerful machine learning algorithms commonly used for classification tasks. The fundamental principle behind SVM is to find a hyperplane that maximizes the margin between different classes in the data. This hyperplane serves as the decision boundary, where data points are classified into one of two or more classes based on their position relative to the hyperplane. SVM is particularly effective in scenarios where the classes are well-separated and works well in high-dimensional spaces. The choice of the kernel function in SVM allows it to handle non-linear data by mapping it to a higher-dimensional space, making it versatile for a wide range of classification problems.

Variations of SVM include:
- [Support Vector Machine with a Linear Kernel (Linear SVM)](https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM)
- [Support Vector Machine with a Polynomial Kernel (Polynomial SVM)](https://en.wikipedia.org/wiki/Support_vector_machine#Polynomial_kernel)
- [Support Vector Machine with a Radial Basis Function Kernel (RBF SVM)](https://en.wikipedia.org/wiki/Support_vector_machine#RBF_kernel)

These variations offer flexibility to adapt to different data distributions. Additionally, SVM can be extended to multiclass classification by using techniques like one-vs-all. Despite its effectiveness, SVM's performance can be sensitive to the choice of kernel and hyperparameters, and it may be computationally expensive for large datasets. However, it remains a valuable tool in the machine learning toolkit, especially when dealing with well-defined classes and both linear and non-linear data.


```{r}
#| code-fold: true
#| label: fig-hyperplanes
#| fig-cap: Examples of hyperplanes in 2-D and 3-D feature space.

# Construct data for plotting
x1 <- x2 <- seq(from = 0, to = 1, length = 100)
xgrid <- expand.grid(x1 = x1, x2 = x2)
y1 <- 1 + 2 * x1
y2 <- 1 + 2 * xgrid$x1 + 3 * xgrid$x2

# Hyperplane: p = 2
p1 <- lattice::xyplot(
  x = y1 ~ x1, 
  type = "l", 
  col = "black", 
  xlab = expression(X[1]), 
  ylab = expression(X[2]),
  main = expression({f(X)==1+2*X[1]-X[2]}==0),
  scales = list(tck = c(1, 0))
)

# Hyperplane: p = 3
p2 <- lattice::wireframe(
  x = y2 ~ xgrid$x1 * xgrid$x2, 
  xlab = expression(X[1]), 
  ylab = expression(X[2]),
  zlab = expression(X[3]),
  main = expression({f(X)==1+2*X[1]+3*X[2]-X[3]}==0),
  drape = TRUE,
  colorkey = FALSE,
  col = dark2[1],
  scales = list(arrows = FALSE)
  # par.settings = list(axis.line = list(col = "transparent"))
)

# Display plots side by side
gridExtra::grid.arrange(p1, p2, nrow = 1)

```


  

## Support Vector Machine Approaches
    
    
### Maximal Marginal Classifier

![Left: Separating hyperplanes for binary data. Right: The decision rule based on a separating hyperplane.](https://www.dropbox.com/scl/fi/zzqb6rbbkput1wegmuaj7/9_2.jpg?rlkey=ke0owlerrfm7xyk2momwo8mv5&raw=1){#fig-sep-hyperplane}



![The solid lines shows the maximal margin hyperplane. The margin is the distance between the two dotted lines.](https://www.dropbox.com/scl/fi/z6bfo6dl85hddo7kceqf9/9_3.jpg?rlkey=erj5245hguu6zfn6yx8y39u4p&raw=1){#fig-max-margin width=75% height=75%}    






    
    

## Summary    

Support Vector Machines (SVM) are a powerful and versatile class of supervised machine learning algorithms for classification.

### Key Concepts

1. **Maximizing Margin**: SVM's primary objective is to find a decision boundary that maximizes the margin between different classes of data points. This boundary is called the "hyperplane."

2. **Linear Separability**: SVM works well when data is linearly separable, meaning that it can be separated by a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions).

3. **Support Vectors**: The data points that are closest to the hyperplane and influence its position are called "support vectors." These support vectors play a crucial role in defining the decision boundary.

4. **Kernel Tricks**: SVM can handle non-linear data by applying kernel functions (e.g., polynomial, radial basis function) that transform the data into a higher-dimensional space, where it becomes linearly separable.

### SVM Classification Process

1. **Data Preparation**: SVM begins with labeled training data, where each data point is associated with a class label.

2. **Model Training**: The SVM algorithm learns the optimal hyperplane that best separates the classes while maximizing the margin. The hyperplane equation can be expressed as $f(x) = \mathbf{w} \cdot \mathbf{x} + b$, where $\mathbf{w}$ is the weight vector and $b$ is the bias term.

3. **Margin Calculation**: The margin is determined by the distance between the hyperplane and the nearest support vectors from each class.

4. **Classification**: To predict the class of a new data point, SVM evaluates $f(x)$. If $f(x) > 0$, the point is classified into one class; if $f(x) < 0$, it's classified into the other class.

### Hyperparameter Tuning

- **C Parameter**: It controls the trade-off between maximizing the margin and minimizing the classification error. Smaller values of C create a wider margin but may misclassify some points, while larger values of C lead to a narrower margin but fewer misclassifications.

- **Kernel Type**: The choice of kernel function, such as linear, polynomial, or radial basis function, can significantly impact the model's ability to handle non-linear data.

### Advantages of SVM

- Effective in high-dimensional spaces.
- Robust against overfitting.
- Suitable for small to large datasets.
- Works well with both linear and non-linear data.

### Limitations

- Computationally expensive for large datasets.
- Sensitivity to the choice of kernel and hyperparameters.
- Can be challenging to interpret in high-dimensional spaces.

SVM is a versatile algorithm widely used for classification tasks, especially when dealing with well-defined classes and both linear and non-linear data. It's essential to understand its hyperparameters and the choice of kernel functions to maximize its effectiveness in different scenarios.



## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)