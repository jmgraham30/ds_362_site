---
title: "Lesson 4"
subtitle: "Introduction to Classification"
author: "JMG"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson04.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| message: false
#| warning: false
#| echo: false

# load packages used in document
library(tidyverse)
library(tidymodels)
library(tidytuesdayR)
library(ISLR2)
library(broom)
library(kableExtra)
library(patchwork)
library(pROC)
library(palmerpenguins)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 13))
```

## Learning Objectives

After this lesson, students will be able to:

* Define the problem of classification. 

* Fit a logistic regression model.

* Define the k-Nearest Neighbor algorithm.

* Create and interpret an ROC curve. 

## Readings, etc.

For this lesson:

- Read chapter 3 of *Statistical Learning with Math and R* [@suzuki2020statistical]. You may also want to read chapter 4 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction].

-   Watch the corresponding video lecture on classification. [View on YouTube](https://youtu.be/BMJQ3LQ_QKU?si=XvFgGO2jJ5OyR-v4).

```{r}
#| echo: false

vembedr::embed_youtube(id="BMJQ3LQ_QKU?si=XvFgGO2jJ5OyR-v4",height=450) %>%
  vembedr::use_align("center")
```

- The video on logistic regression might also be helpful. [View on YouTube](https://youtu.be/kr_Be9NVXOM?si=0OtVihMY3zmbY0pa).

```{r}
#| echo: false

vembedr::embed_youtube(id="kr_Be9NVXOM?si=0OtVihMY3zmbY0pa",height=450) %>%
  vembedr::use_align("center")
```

## Overview

In a classification problem, we assume a functional relationship of the form

$$
y = f({\bf x}) + \epsilon
$$

where the response variable $y$ takes values in a finite set. The $y$ values will often correspond to classes or categories and this is why we call this problem classification. Classification is a common problem for machine learning, perhaps even more common than regression. There are two general approaches to classification:

1. predict the values of $y$, that is, the classes directly, or

2. predict the probabilities for $y$ to be in each of the relevant classes. 

The approach in 2 has the benefit that we can think of classification as a regression problem since probabilities are numerical quantities. On the other hand, any regression problem can be recast as a classification by binning or otherwise discretizing the response variable. 

A special case of classification is **binary** classification which is the situation where the response $y$ can take on but two distinct values. We will begin out study of classification by looking at binary classification and logistic regression which is a common approach to binary classification. 

## Logistic Regression 

Suppose that we have labelled data $({\bf x}_{1},y_{1}), ({\bf x}_{2},y_{2}), \ldots , ({\bf x}_{n},y_{n})$ such that for each $i$, $y_{i} \in \{0,1\}$. That is, the response variables can take on only two distinct values. Our goal is to build a model that can predict

$$
p({\bf x}) := P(y = 1 | {\bf x})
$$
The method of **logistic regression** is to build a model of the form

$$
p({\bf x}) = \frac{e^{\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{p}x_{p}}}{1 + e^{\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{p}x_{p}}}
$$

From which we can derive the expression

$$
\log\left( \frac{p({\bf x})}{1 - p({\bf x})} \right) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{p}x_{p}
$$
We call the expression

$$
\log\left( \frac{p({\bf x})}{1 - p({\bf x})} \right)
$$

the *log odds* or *logit*.

**Note:** While the probability $P(Y = 1| {\bf x})$ is **not** a linear function of the predictor variables, the log odds **is** a linear function of the predictor.  

We refer to a function of the form

$$
f(x) = \frac{e^x}{1 + e^x}
$$

as a **logistic function**.

Let's develop some motivation and intuition for logistic regression. Consider the `Default` data set from the `ISLR2` package. The first few rows of the data are shown below:

```{r}
#| echo: false

Default %>%
  head() %>%
  kable()
```


Our goal is to use one or more of the variables `student`, `balance`, or `income` to predict whether an individual is likely to default on their loan. Here we will think of the values for the response `default` as $\text{No} = 0$ and $\text{Yes} = 1$. To do this, we create a 0-1 version of the response:

```{r}
Default <- Default %>%
  mutate(default_01 = ifelse(default == "No",0,1))

glimpse(Default)
```

Let's begin by focusing on just `balance` as a predictor. We can plot the `default` variable versus the `balance`:

```{r}
#| label: fig-default
#| fig-cap: Plot of defaults versus loan balance for the Default data from the ISLR2 package. 


Default %>%
  ggplot(aes(x=balance,y=default_01)) + 
  geom_point(alpha=0.2) + 
  labs(x="Loan balance",y="Default, No=0, Yes=1") 
```

**Question:** What does the plot in @fig-default suggest? 

The goal of logistic regression is to fit the in some sense best logistic function to the data. Let's compare a couple of different logistic functions as candidate models for the `Default` data. @fig-logistics shows two different logistic curves plotted over the `Default` data.

```{r}
#| label: fig-logistics
#| fig-cap: Comparison of two different logistics curves plotted over the defaults versus loan balance for the Default data from the ISLR2 package. 

f_1 <- function(x){ exp(-5 + 0.01 * x) / (1 + exp(-5 + 0.01 * x))}
f_2 <- function(x){ exp(-11 + 0.008 * x) / (1 + exp(-11 + 0.008 * x))}

p_1 <- Default %>%
  ggplot(aes(x=balance,y=default_01)) + 
  geom_point(alpha=0.2) + 
  stat_function(fun = f_1,color="orange",linewidth=1.1) + 
  labs(x="Loan balance",y="Default, No=0, Yes=1") 

p_2 <- Default %>%
  ggplot(aes(x=balance,y=default_01)) + 
  geom_point(alpha=0.2) + 
  stat_function(fun = f_2, color="darkgreen",linewidth=1.1) + 
  labs(x="Loan balance",y="Default, No=0, Yes=1") 

p_1 + p_2
```

**Question:** Which of the two logistic curves in @fig-logistics  do you think is a better fit for the `Default` data? Explain your answer. 

The problem for us now is, how do we define and determine the "best-fit" logistic function for binary classification? Minimizing the squared error for the residuals is not going to work well in this situation (**why not?**). 

### Likelihood Optimization

The **likelihood function** is the probability of the observed data viewed as a function of the model parameters. Then, we define the best-fit logistic curve to be the one that *maximizes* the likelihood. 

**Note:** We point out again that machine learning methods often involve solving some kind of optimization problem and we see that this is the case with logistic regression. 

How should we maximize the likelihood function which is a multi-variable function? One approach is or course to find critical points by computing and setting the partial derivatives to zero and then use the Hessian criteria to confirm there is a maximum. There is a trick that makes this a little easier, that is, rather than maximize the likelihood function, it's typical to maximize the log-likelihood. Because the $\log$ function converts products into sums, this makes it easier to compute partial derivatives.

Luckily for use, the maximum likelihood method is already implemented in R through the `glm` function, where glm stands for **generalized linear model**. For example,

```{r}
#| code-fold: false
default_fit_1 <- glm(default_01 ~ balance, data=Default, family="binomial")
coef(default_fit_1)
```

gives the maximum likelihood estimated values for $\beta_{0}$ and $\beta_{1}$. Let's plot the corresponding logistic function:

```{r}
#| label: fig-best-logistic
#| fig-cap:  Maximum likelihood best fit logistic curve plotted over the defaults versus loan balance for the Default data from the ISLR2 package. 

f_best <- function(x){ exp(-10.65 + 0.0055 * x) / (1 + exp(-10.65 + 0.0055 * x))}


Default %>%
  ggplot(aes(x=balance,y=default_01)) + 
  geom_point(alpha=0.2) + 
  stat_function(fun = f_best, color="purple",linewidth=1.1) + 
  labs(x="Loan balance",y="Default, No=0, Yes=1") 

```

**Question:** How does the maximum likelihood estimated logistic curve compare with the two from @fig-logistics?

We can use our logistic regression model to make predictions. For example, suppose our friend has a balance of $800, are the likelihood to default on their load? We can answer this as follows:

```{r}
#| code-fold: false

(default_800_odds <- predict(default_fit_1,newdata=tibble(balance=800)))
(default_800_prob <- predict(default_fit_1,newdata=tibble(balance=800),type="response"))
```

We see that the log-odds is very small which corresponds to a very small probability of default. Notice the relationship between the probability and the log-odds:

```{r}
#| code-fold: false

log(default_800_prob / (1 - default_800_prob))
```

On the other hand, if our friend has a balance of $2,200 then

```{r}
#| code-fold: false

(default_2200_odds <- predict(default_fit_1,newdata=tibble(balance=2200)))
(default_2200_prob <- predict(default_fit_1,newdata=tibble(balance=2200),type="response"))
```

and we see that the predicted probability of default is high. A question we will return to soon is, how does the probability of default relate to classifying defaults as "no" or "yes"?

We can just as easily fit a logistic regression model with more than one predictor. For example,

```{r}
#| code-fold: false
default_fit_2 <- glm(default_01 ~ balance + income + student, data=Default, family="binomial")
coef(default_fit_2)
```


Which of the two models is better remains an open question and we will address this and other topics related to logistic regression later. 

## Classification Errors

Suppose we work for a loan company and our boss isn't interested in the probability that a borrower will default and just wants to know if someone if likely to default or not. One way to deal with this is to choose a cutoff value for the probability of default and classify borrowers with probability of default greater than the threshold value as likely to default. For example, suppose we classify any borrower with a probability of defaulting greater than 50% as likely to default. This will inevitably lead to some amount of misclassification. We can calculate this for our data and model:

```{r}
#| code-fold: false


default_fit_1_preds <- predict(default_fit_1,type="response")
default_fit_1_class <- (default_fit_1_preds > 0.5) %>% as.numeric()

table(Default$default_01,default_fit_1_class)
```

This says that there were 42 loans that did not default but the model classified as defaults. That is, the model produced 42 *false positives*. On the other hand, the model classified 233 loans that did default as non-defaults. That is, the model produced 233 *false negatives*. These numbers will change if we change the threshold value for classification. For example, what if we set our threshold to 45%?

```{r}
#| code-fold: false


default_fit_1_preds <- predict(default_fit_1,type="response")
default_fit_1_class <- (default_fit_1_preds > 0.45) %>% as.numeric()

table(Default$default_01,default_fit_1_class)
```

For later purposes, let's define the following quantities:

$$
\begin{align*}
\text{sensitivity or true positive rate} &= \frac{\text{true positive}}{\text{true positive} + \text{false negative}} \\
\text{specificity or false positive rate} &= \frac{\text{true negative}}{\text{true negative} + \text{false positive}}
\end{align*}
$$

Then, in our example with the 50% threshold value, the sensitivity is `r 100 / (100 + 233)` and the specificity is `r  9625 / (9625 + 42)`. 

Notice that the sensitivity and specificity are both functions of the probability threshold value used to make the decision about how to classify predicted response values. Let's write an R function that computes the sensitivity and specificity for any threshold value. 

```{r}
#| code-fold: false

spec_sens <- function(threshold_val){

  default_fit_1_class <- (default_fit_1_preds > threshold_val) %>% as.numeric()

  conf_mat <- table(Default$default_01,default_fit_1_class)
  
  sensitivity <- conf_mat[2,2] / (sum(conf_mat[2, ]))
  specificity <- conf_mat[1,1] / (sum(conf_mat[1, ]))
  
  tibble(sensitivity = sensitivity,specificity = specificity)
  
}
```

Now, let's test our function

```{r}
#| code-fold: false

spec_sens(0.5)
```

### ROC Curves

Let's use our function to compute and plot the sensitivity and specificity values for a range of threshold values. 

```{r}
thresh_vals <- seq(0.001,0.981,by=0.001)
spec_sens_df <- map_df(thresh_vals,spec_sens) %>%
  mutate(thresh_vals = thresh_vals)

spec_sens_df %>%
  ggplot(aes(x=specificity,y=sensitivity,color=thresh_vals)) + 
  geom_path(linewidth=1.1) + 
  scale_x_reverse() + 
  xlim(c(1,0)) + 
  ylim(c(0,1))

```

This creates what is known as a **receiver operating characteristic (ROC)** curve which is is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. 

The package `pROC` contains functions that will create an ROC curve for us. For example, 

```{r}
#| label: fig-proc
#| fig-cap: An ROC curve for our logistic regression model of defaults as predicted by balance. The curve is created usign functions from the pROC package.
#| code-fold: false
#| warning: false
#| message: false
ggroc(roc(Default$default_01,predict(default_fit_1,type="response")),size=2)
```

The **area under the ROC curve (AUC)** is a widely used measure of overall performance of a binary classifier because it summarizes the performance over a range of threshold values. Again, the `pROC` package contains functions that allows us to calculate the AUC:

```{r}
#| code-fold: false

auc(Default$default_01,predict(default_fit_1,type="response"))
```

Note that the largest possible value for AUC is 1. Further, we expect a classifies that performs no better than change to have an AUC of 0.5. We see that as a classifier on the `Deafault` data logistic regression appears to perform well. However, we only computed the AUC for the training data and ultimately what we are interested in is how the model performs on test data. We will return to this issue later. 

An important application of ROC curves and AUC is in comparing different types of classifiers. So, we will spend some time learning about other types of classifiers. 

## KNN

Another approach to classification (and also regression) is to use distance-based methods such as **K Nearest Neighbors** (KNN). The basic model assumption is that data points that are closer together are more likely to belong to the same class. To implement such models, we need to address two points:

*  How do we measure distance between data points?

* What is our decision rule for classifying a data point when it is close to multiple points from different classes. 

Given a particular measure of distance, the KNN algorithm proceeds as follows:

1. Choose a value $k$ that is less than or equal to, but typically much less than the total size $n$ of the observed data.

2. For a new value ${\bf x}$, find the $k$ data points that are closest to ${\bf x}$. 

3. Assign to ${\bf x}$ the majority class among its $k$ nearest neighbors. Note that it may also be necessary to establish a rule for breaking ties.   

There is a [KNN demo](https://codepen.io/gangtao/pen/PPoqMW) associated with the textbook [@dinov2018data]. [View the KNN demo](https://codepen.io/gangtao/pen/PPoqMW).


Let's work through some KNN examples together in a new R project. 



**Note:** An important application of KNN is to imputation of missing data.  

There is one major issue with distance-based classification (or regression) methods. This is an issue known as the [**curse of dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The issue is basically that as the dimension of the space that data points belong to increases, the more sparse the data points are within the space. You will be asked to explore this issue further in the homework exercises. 

## Multi-class Classifiers




## Preparation for the next lesson

For the next lesson we will cover resampling methods. To prepare for the next lesson, please do the following:

-   Read chapter 5 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 4 of *Statistical Learning with Math and R* [@suzuki2020statistical].

-   Watch the corresponding video lecture on cross validation. [View on YouTube](https://youtu.be/6eWODQJrMKs?si=tPPLUz9g9TMCt6EL).

```{r}
#| echo: false

vembedr::embed_youtube(id="6eWODQJrMKs?si=tPPLUz9g9TMCt6EL",height=450) %>%
  vembedr::use_align("center")
```



## References

::: {#refs}
:::

::: {.callout-tip collapse="true"}
## Expand for Session Info

```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```
:::

[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width="15%"}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)

