---
title: "Lesson 3"
subtitle: "Linear Regression"
author: "JMG"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson03.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| message: false
#| warning: false
#| echo: false

# load packages used in document
library(tidyverse)
library(tidytuesdayR)
library(ISLR2)
library(broom)

theme_set(theme_minimal(base_size = 13))
```

## Learning Objectives

After this lesson, students will be able to:

* Define linear regression and appreciate it from the perspective of machine learning.

* Define the norm of a vector. 

* Understand the role of systems of linear equations, matrices, and $QR$ factorization in linear regression. 

* Use R to compute the $QR$ factorization of a matrix. 

* Use the $QR$ factorization to compute the coefficients for linear regression. 


## Readings, etc.

For this lesson:

-   Read chapter 3 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 2 of *Statistical Learning with Math and R* [@suzuki2020statistical].

-   Watch the corresponding video lecture on regression. [View on YouTube](https://youtu.be/ox0cKk7h4o0).

```{r}
#| echo: false

vembedr::embed_youtube(id="ox0cKk7h4o0",height=450) %>%
  vembedr::use_align("center")
```

## Motivation for Linear Regression

Recall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:

$$
y = f({\bf x}) + \epsilon
$$
and then we seek to find a function $\hat{f}$ from a predetermined class of functions that does a good job in approximating $f$. Let's study this problem in more detail but in a very simplest setting. Specifically, we will assume that ${\bf x}$ and $y$ are both single numerical variables and that $f$ is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers $\beta_{0}$ and $\beta_{1}$ such that

$$
y = \beta_{0} + \beta_{1} x + \epsilon
$$
for all values of $x$ and $y$. Recall that we are assuming that $\text{E}[\epsilon] = 0$ so $\epsilon$ is a random variable with expected value (or mean) equal to zero. 

If we restrict ourselves to the class of single-variable linear functions, then finding an approximation to $f(x) = \beta_{0} + \beta_{1} x$ is equivalent to finding values $\hat{\beta}_0$ and $\hat{\beta}_{1}$ so that

$$
\hat{f}(x) = \hat{\beta}_{0} + \hat{\beta}_{1} x \approx f(x) = \beta_{0} + \beta_{1} x
$$
Thus, this would be a **parametric** model since any candidate approximating function is uniquely specified by specifying the values for the parameters $\hat{\beta}_0$ and $\hat{\beta}_{1}$. 

@fig-slr shows the plot of data that has been generated by a relationship of the form $y = \beta_{0} + \beta_{1} x + \epsilon$. You should examine the code used to create or simulate the data in this example and see how it relates to the expression $y = \beta_{0} + \beta_{1} x + \epsilon$.

```{r}
#| label: fig-slr
#| fig-cap: A data set with two numerical variables $x$ and $y$ generated by an underlying linear function so that $y = \beta_{0} + \beta_{1}x + \epsilon$.

set.seed(1287)
N <- 25
x <- rnorm(N,mean=72,sd=12)
y <- 1.2 + 0.75 * x + rnorm(N,sd=2)
xy_data <- tibble(x=x,y=y)

xy_data %>%
  ggplot(aes(x=x,y=y)) + 
  geom_point()
```

From a (supervised) machine learning perspective, fitting a line to such data means "learning" the values of $\hat{\beta}_0$ and $\hat{\beta}_{1}$ from the data. How do we learn $\hat{\beta}_0$ and $\hat{\beta}_{1}$? @fig-a-resid shows the same data as in @fig-slr but where we have added a best fit line as well as a single residual value. 

```{r}
#| code-fold: true
#| message: false
#| label: fig-a-resid
#| fig-cap: The same data as shown in @fig-slr but with a best fit line as well as a single residual also shown.

fitted_linear_model <- lm(y ~ x, data=xy_data) %>%
  augment()

a_point <- fitted_linear_model[1,1:3] %>% as.numeric()

xy_data %>%
  ggplot(aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(method="lm",se = FALSE) + 
  geom_point(data=NULL,aes(x=a_point[2],y=a_point[3]),color="purple",size=3) + 
  geom_segment(aes(x = a_point[2], y = a_point[1], xend = a_point[2], yend = a_point[3]), 
               data = NULL,
               color="red",lwd=1)
```

@fig-resids shows the same data as in @fig-slr but where we have added a best fit line as well as all the residual values.  One way to learn the values for $\hat{\beta}_0$ and $\hat{\beta}_{1}$ is to minimize the squared error for the residuals. 

```{r}
#| code-fold: true
#| message: false
#| label: fig-resids
#| fig-cap: The same data as shown in @fig-slr but with a best fit line as well as all residuals also shown.

fitted_linear_model %>%
  ggplot(aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(method="lm",se = FALSE) +
  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), 
               color="red",lwd=1)
```


Notice that we can write the squared error for the residuals as a function of two variables $L(\beta_{0},\beta_{1})$ defined by

$$
L(\beta_{0},\beta_{1}) = \sum_{i=1}^{n}(y_{i} - \beta_{0} - \beta_{1}x_{i})^2
$$

Then, in order to minimize this function we need to find the critical values for the function $L$ by computing partial derivatives and solving

$$
\begin{align*}
\frac{\partial L}{\partial \beta_{0}} &= 0 \\
\frac{\partial L}{\partial \beta_{1}} &= 0
\end{align*}
$$

However, there is an alternative approach that uses tools from linear algebra such as matrices and we will examine this approach for a few reasons:

1. It motivates the use of linear algebra and matrices in machine learning. 

2. It helps provide a geometric perspective to machine learning. 

3. It generalizes well to the situation when we have more than one predictor variable. 

The next section treats the linear algebra tools we will use for linear regression and the section after that applies linear algebra to do linear regression. 

**Note:** Linear regression can and often is used even in situations where we do not know *a priori* that $f$ in the relation $y = f(x) + \epsilon$ is linear. 

**Question:** What do you think some of the pros and cons of using linear regression for supervised learning even if the function $f$ in the relationship $y = f(x) + \epsilon$ might not be linear?

## Overdetermined Linear Systems and $QR$ Factorization

Recall that a system of linear equations is an expression of the form

$$
\begin{align*}
a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1p} x_{p}  &= b_{1} \\
a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2p} x_{p} &= b_{2} \\
 &\vdots \\
a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{np} x_{p} &= b_{n} 
\end{align*}
$$

where there are $p$ unknowns $x_{j}$, $n \times p$ coefficients $a_{ij}$, and $n$ given values $b_{i}$. Such a system can be rewritten using matrix notation as

$$
\left[\begin{array}{cccc} a_{11} & a_{12} & \cdots & a_{1p} \\ a_{21} & a_{22} & \cdots & a_{2p} \\ \vdots & \ddots & \cdots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{np} \end{array}\right] \left[\begin{array}{c} x_{1} \\ x_{2} \\ \vdots \\ x_{p} \end{array} \right] = \left[\begin{array}{c} b_{1} \\ b_{2} \\ \vdots \\ b_{n} \end{array} \right]
$$

or even more concisely as 

$$
A {\bf x} = {\bf b}
$$

and we say that $A$ is an $n \times p$ matrix. 

**Definition:** A linear system $A {\bf x} = {\bf b}$ is said to be **overdetermined** if there are more equations that unknowns. That is, if $n > p$. 

We will soon see that the problem of linear regression typically corresponds to "solving" an overdetermined linear system. There's a problem though, overdetermined linear systems do not usually have a solution. For example, consider the following linear system:

$$
\begin{align*}
x &= 1 \\
x &= 2
\end{align*}
$$
which is an overdetermined linear system since it has two equations in one unknown. This system clearly does not possess a solution. 

Given an $n \times p$ matrix $A$ and an  $n$-vector ${\bf b}$, for any vector $p$-vector ${\bf x}$ we can always form the residual ${\bf r}$ defined by 

$$
{\bf r} = {\bf b} - A{\bf x}
$$
Note that ${\bf x}$ is a solution to the linear system $A{\bf x} = {\bf b}$ is and only if the residual ${\bf r}$ is the zero vector.

Now, based on the last example we cannot generally make the the residual zero for an overdetermined system. However, we could instead search for a vector ${\bf x}$ that makes the residual as small as possible. In order to do so, we need a way to measure the size of a residual. Vector norms are a mathematical object that allow us to measure the size of residuals. 


### Norms: A Technical Tool

**Definition:** A vector norm, denoted by $\|\cdot \|$ is an object that takes as input a vector and returns a real number while satisfying the following properties:

1. Positivity: For any vector $\|{\bf v}\| \geq 0$ and $\|{\bf v}\| = 0$ if and only if ${\bf v}$ is the zero vector. 

2. Homogeneity: If $\alpha$ is a number and ${\bf v}$ is a vector, then $\|\alpha {\bf v}\| = |\alpha| \|{\bf v}\|$. 

3. Triangle inequality: For any vectors ${\bf u}$ and ${\bf v}$, we have

$$
\|{\bf u} + {\bf v} \| \leq \|{\bf u}\| + \|{\bf v}\|
$$
**Example:** The most relevant example for us, at least in the setting of linear regression is the 2-norm $\|\cdot\|_{2}$ which for a vector ${\bf v} = [\begin{array}{cccc} v_{1} & v_{2} & \cdots & v_{n} \end{array}]^{T}$ is defined by

$$
\|{\bf v}\|_{2} = \sqrt{v_{1}^2 + v_{2}^{2} + \cdots + v_{n}^{2}}
$$

**Defintion:** Given a vector norm $\|\cdot\|$, the **distance between two vectors** ${\bf u}$ and ${\bf v}$ is defined to be $\|{\bf u} - {\bf v}\|$.

**Note:** It is possible to generalize vector norms to norms on sets of functions. This is also useful in machine learning since this allows us to define a notion of distance between functions.


### Linear Regression and Linear Least Squares

The 2-norm allows us to define the **linear least squares problem:**

> Given an $n \times p$ matrix $A$ and a $n$-vector ${\bf b}$, find a vector ${\bf x}$ that satifies

$$
\|{\bf b} - A{\bf x}\|_{2} \leq \|{\bf b} - A{\bf \xi}\|_{2} , \ \ \text{for all vectors } {\bf \xi}
$$
That is, the linear least squares problem is to find a vector that minimizes the corresponding residual with respect to the 2-norm. 

**Example:** Consider again the linear  system

$$
\begin{align*}
x &= 1 \\
x &= 2
\end{align*}
$$

Then the linear least squares problem in this case is to find the value $x$ such that

$$
\sqrt{(x-1)^2 + (x-2)^2}
$$

is as small as possible. Note that this is equivalent to minimizing

$$
(x-1)^2 + (x-2)^2 = 2x^2 - 6x + 5
$$

In general, minimizing the residual in the 2-norm corresponds to minimizing a multi-variable quadratic function. However, we can use linear algebra instead of calculus to solve the linear least squares problem. The main tool we need is what is known as the $QR$ factorization or decomposition of a matrix.   

### $QR$ Factorization

Consider the following example that can easily be checked by hand.

$$
\left[\begin{array}{cc} 3 & 1 \\ 4 & 2 \end{array}\right] = \left[\begin{array}{cc} \frac{3}{5} & -\frac{4}{5} \\ \frac{4}{5} & \frac{3}{5} \end{array}\right]\left[\begin{array}{cc} 5 & \frac{11}{5} \\ 0 & \frac{2}{5} \end{array}\right]
$$

This has the form $A = QR$ and what is really interesting here is that

1. The matrix $Q = \left[\begin{array}{cc} \frac{3}{5} & -\frac{4}{5} \\ \frac{4}{5} & \frac{3}{5} \end{array}\right]$ satisfies that $QQ^{T} = I$ and $Q^{T}Q = I$. We refer to any matrix that satisfies such properties as an **orthogonal** matrix. 

2. The matrix $R = \left[\begin{array}{cc} 5 & \frac{11}{5} \\ 0 & \frac{2}{5} \end{array}\right]$ is **upper triangular** since all the entries below the main diagonal are zero. 

Every matrix has a $QR$ factorization and it is unique if $n \geq p$ and $A$ is of full rank. Further, the $QR$ factorization can be used to solve the linear least squares problem as follows:

1. Compute $A = QR$, the $QR$ factorization [^1]. There are efficient algorithms for doing this.

2. Form the vector $Q^{T}{\bf b}$.

3. Solve the linear system $R{\bf x} = Q^{T}{\bf b}$.

The system in point 3 is relatively easy to solve. Since $R$ is upper triangular, the system $R{\bf x} = Q^{T}{\bf b}$ can be solved by a simple algorithm known as backward substitution which is implemented in R by the function `backsolve`. 

[^1]: Technically we are assuming that $A = QR$ is the *reduced* $QR$ factorization meaning that $R$ has size $p \times p$. 

#### Computing the $QR$ Factorization with R

The following R code shows how the create a matrix, compute its $QR$ factorization with the function `qr`, and then extract the matrices $Q$ and $R$:

```{r}
#| code-fold: false

A <- matrix(c(3,1,4,2),2,2,byrow = TRUE)
A_qr <- qr(A)
Q <- qr.Q(A_qr)
R <- qr.R(A_qr)
```

Let's check that $QR = A$:

```{r}
#| code-fold: false

Q %*% R
```


Let's check that $Q$ is an orthogonal matrix:

```{r}
#| code-fold: false

round(Q %*% t(Q),2)
```




```{r}
#| code-fold: false

round(t(Q) %*% Q,2)
```


## Simple Linear Regression

Return to the data shown in @fig-slr and @fig-a-resid. Fitting a line to this data means finding values for the intercept $\hat{\beta}_{0}$ and slope $\hat{\beta}_{1}$ so that for each data point $(x_{i},y_{i})$ we have that 

$$
\hat{\beta}_{0} + \hat{\beta}_{1} x_{i} = \hat{\beta}_{0} \cdot 1 + \hat{\beta}_{1} x_{i}
$$

is as close as possible to $y_{i}$. We can rewrite the last expression as a matrix vector product:

$$
X {\bf \beta} =  \left[\begin{array}{cc} 1 & x_{1} \\ 1 & x_{2} \\ \vdots & \vdots \\ 1 & x_{n} \end{array}\right]\left[\begin{array}{c} \beta_{0} \\ \beta_{1} \end{array}\right]
$$

In order to account for the intercept coefficient, we have to add the column of ones. We call the matrix $X$ so formed the **data matrix** and the vector ${\bf \beta}$ the parameter vector. Thus, we can write the linear regression problem as a linear least squares problem to minimize the residual ${\bf r} = {\bf y} - X{\bf \beta}$. Minimizing this is the 2-norm is the same as minimizing the squared error. Let's see this worked out in a computational example. 

### An Example

The following R code constructs the data matrix $X$, computes the $QR$ factorization for $X$, and then applies the backward substitution algorithm to solve $R{\bf \beta} = Q^{T}{\bf y}$:



```{r}
#| code-fold: false

y_vect <- xy_data %>% pull(y)
y_vect <- matrix(y_vect,ncol = 1)
X_mat <- xy_data %>% 
  mutate(int_ones = rep(1,nrow(xy_data))) %>% 
  select(int_ones,x) %>% 
  as.matrix()

X_qr <- qr(X_mat)
R_mat <- qr.R(X_qr) 
Q_mat <- qr.Q(X_qr)
(beta_vals <- backsolve(R_mat,t(Q_mat) %*% y_vect))
```

Now that we have estimated model coefficients, we can use that information to make predictions with the model. For example,

```{r}
#| code-fold: false

beta_vals_vect <- matrix(beta_vals,ncol=1,byrow = TRUE)
X_new <- matrix(c(1,67),1,2,byrow=TRUE)
(y_pred <- X_new %*% beta_vals_vect)
```

### Fitting Linear Models with R

There are many functions in base R and other packages that can be used to fit not only linear models but a variety of many different types of models. In base R, we have a function `lm` that can be used to fit a linear regression model. Let's examine the documentation for `lm`.  We see that the first argument for `lm` is a formula. Many modeling functions in base R and other packages accept utilize a formula to represent the model specification. The easiest way to understand this is to see some examples. 

Let's compare what R does when we use the function `lm` to fit a linear model with our earlier approach of using $QR$ factorization.

```{r}
#| code-fold: false

lm_fit <- lm(y ~ x,data=xy_data)

coefficients(lm_fit)
```

Under the hood, the `lm` function in R is using the $QR$ factorization to compute the slope and intercept for the line in figures like @fig-a-resid and @fig-resids. 

We can also make predictions on new data with `lm` models:

```{r}
#| code-fold: false

predict(lm_fit,newdata=tibble(x=67))
```

## Multiple Linear Regression

Suppose that we have data of the form $(y_{i},{\bf x}_{i}) = (y_{i},x_{i1},x_{i2},\ldots , x_{ip})$ so that there are $p$ predictor variables. The multiple linear regression model takes the form

$$
y = \beta_{0} + \beta_{1}{\bf x}_{1} + \beta_{2}{\bf x}_{2} + \cdots + \beta_{p}{\bf x}_{p} + \epsilon
$$

Taking into account the column of ones we can form a $n \times (p + 1)$ sized data matrix and again use $QR$ factorization to solve the corresponding linear least squares problem for the residual ${\bf r} = {\bf y} - X{\bf \beta}$. Now, our coefficient vector ${\bf \beta}$ will have length $p+1$.  

Multiple linear regression is a significant generalization of simple linear regression because it not only allows us to account for multiple predictor variables, but also allows us to account for certain types of nonlinearity and also predictor variables that are categorical. This is because:

1. The "linear" part of linear regression refers to linearity with respect to the coefficients ${\bf \beta}$.

2. We can use dummy variables to represent categorical predictor variables. 

The point is, as long as our data can be represented by a data matrix $X$, then we can try to use $QR$ factorization to solve the linear least squares problem to minimize the residuals ${\bf r} = {\bf y} - X{\bf \beta}$. 

We can use the `lm` function to easily fit multiple linear regression models.  Let's work through some examples together. 

## Linear Regression in Machine Learning

Through our study of linear regression, we have derived and implemented our first supervised machine learning algorithm. We have also seen in our worked examples how to use `tidymodels` to apply linear regression as a machine learning algorithm. In particular, we have seen how to: 

* Account for certain types of nonlinearity. 

* Account for categorical predictors. 

* Separate data into a training set and a test set.

* Set up and fit a model.

* Use a model to make predictions.

* Assess model accuracy by computing the root mean square error. 

However, there are several additional considerations we still need to address. For example,

* Assessing model uncertainty.

* Choosing which predictors to include or not in a model. 

* Deciding between different classes of models. 

We will take up these issues soon but before doing so, we will first look at a learning algorithm for a classification problem. 

## Preparation for the next lesson

For the next lesson:

-   Read chapter 4 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 3 of *Statistical Learning with Math and R* [@suzuki2020statistical].

-   Watch the corresponding video lecture on classification. [View on YouTube](https://youtu.be/BMJQ3LQ_QKU?si=XvFgGO2jJ5OyR-v4).

```{r}
#| echo: false

vembedr::embed_youtube(id="BMJQ3LQ_QKU?si=XvFgGO2jJ5OyR-v4",height=450) %>%
  vembedr::use_align("center")
```

## References

::: {#refs}
:::

::: {.callout-tip collapse="true"}
## Expand for Session Info

```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```
:::

[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width="15%"}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)

