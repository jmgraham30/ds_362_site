---
title: "Lesson 3"
subtitle: "Linear Regression"
author: "JMG"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson03.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| message: false
#| warning: false
#| echo: false

# load packages used in document
library(tidyverse)
library(tidytuesdayR)
library(ISLR2)
library(broom)

theme_set(theme_minimal(base_size = 13))
```

## Learning Objectives

After this lesson, students will be able to:

* Define linear regression and appreciate from the perspective of machine learning.

* Define the norm of a vector. 

* Understand the role of systems of linear equations, matrices, and $QR$ factorization in linear regression. 

* Use R to compute the $QR$ factorization of a matrix. 

* Use the $QR$ factorization to compute the coefficients for linear regression. 


## Readings, etc.

For this next lesson:

-   Read chapter 3 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 2 of *Statistical Learning with Math and R* [@suzuki2020statistical].

-   Watch the corresponding video lecture on regression. [View on YouTube](https://youtu.be/ox0cKk7h4o0).

```{r}
#| echo: false

vembedr::embed_youtube(id="ox0cKk7h4o0",height=450) %>%
  vembedr::use_align("center")
```

## Motivation for Linear Regression

Recall that in a supervised learning problem, we assume that there is a relationship between the predictor and response variables of the form:

$$
y = f({\bf x}) + \epsilon
$$
and then we seek to find a function $\hat{f}$ from a predetermined class of functions that does a good job in approximating $f$. Let's study this problem in more detail but in a very simplest setting. Specifically, we will assume that ${\bf x}$ and $y$ are both single numerical variables and that $f$ is linear. Then writing everything out in detail, we assume that there are (true but unknown) numbers $\beta_{0}$ and $\beta_{1}$ such that

$$
y = \beta_{0} + \beta_{1} x + \epsilon
$$
for all values of $x$ and $y$. Recall that we are assuming that $\text{E}[\epsilon] = 0$ so $\epsilon$ is a random variable with expected value (or mean) equal to zero. 

If we restrict ourselves to the class of single-variable linear functions, then finding an approximation to $f(x) = \beta_{0} + \beta_{1} x$ is equivalent to finding values $\hat{\beta}_0$ and $\hat{\beta}_{1}$ so that

$$
\hat{f}(x) = \hat{\beta}_{0} + \hat{\beta}_{1} x \approx f(x) = \beta_{0} + \beta_{1} x
$$
Thus, this would be a **parametric** model since any candidate approximating function is uniquely specified by specifying the values for the parameters $\hat{\beta}_0$ and $\hat{\beta}_{1}$. 

@fig-slr shows the plot of data that has been generated by a relationship of the form $y = \beta_{0} + \beta_{1} x + \epsilon$. You should examine the code used to create or simulate the data in this example and see how it relates to the expression $y = \beta_{0} + \beta_{1} x + \epsilon$.

```{r}
#| label: fig-slr
#| fig-cap: A data set with two numerical variables $x$ and $y$ generated by an underlying linear function so that $y = \beta_{0} + \beta_{1}x + \epsilon$.

set.seed(1287)
N <- 25
x <- rnorm(N,mean=72,sd=12)
y <- 1.2 + 0.75 * x + rnorm(N,sd=2)
xy_data <- tibble(x=x,y=y)

xy_data %>%
  ggplot(aes(x=x,y=y)) + 
  geom_point()
```

From a (supervised) machine learning perspective, fitting a line to such data means "learning" the values of $\hat{\beta}_0$ and $\hat{\beta}_{1}$ from the data. How do we learn $\hat{\beta}_0$ and $\hat{\beta}_{1}$? @fig-a-resid shows the same data as in @fig-slr but where we have added a best fit line as well as a single residual value. 

```{r}
#| code-fold: true
#| message: false
#| label: fig-a-resid
#| fig-cap: The same data as shown in @fig-slr but with a best fit line as well as a single residual also shown.

fitted_linear_model <- lm(y ~ x, data=xy_data) %>%
  augment()

a_point <- fitted_linear_model[1,1:3] %>% as.numeric()

xy_data %>%
  ggplot(aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(method="lm",se = FALSE) + 
  geom_point(data=NULL,aes(x=a_point[2],y=a_point[3]),color="purple",size=3) + 
  geom_segment(aes(x = a_point[2], y = a_point[1], xend = a_point[2], yend = a_point[3]), 
               data = NULL,
               color="red",lwd=1)
```

@fig-resids shows the same data as in @fig-slr but where we have added a best fit line as well as all the residual values.  One way to learn the values for $\hat{\beta}_0$ and $\hat{\beta}_{1}$ is the minimize the squared error for the residuals. 

```{r}
#| code-fold: true
#| message: false
#| label: fig-resids

fitted_linear_model %>%
  ggplot(aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(method="lm",se = FALSE) +
  geom_segment(aes(x = x, y = y, xend = x, yend = .fitted), 
               color="red",lwd=1)
```


Notice that we can write the squared error for the residuals as a function of two variables $L(\beta_{0},\beta_{1})$ defined by

$$
L(\beta_{0},\beta_{1}) = \sum_{i=1}^{n}(y_{i} - \beta_{0} - \beta_{1}x_{i})^2
$$

Then, in order to minimize this function we need to find the critical values for the function $L$ by computing partial derivatives and solving

$$
\begin{align*}
\frac{\partial L}{\partial \beta_{0}} &= 0 \\
\frac{\partial L}{\partial \beta_{1}} &= 0
\end{align*}
$$

However, there is an alternative approach that uses tools from linear algebra such as matrices and we will examine this approach for a few reasons:

1. It motivates the use of linear algebra and matrices in machine learning. 

2. It helps provide a geometric perspective to machine learning. 

3. It generalizes well to the situation when we have more than one predictor variable. 

The next section treats the linear algebra tools we will use for linear regression and the section after that applies linear algebra to do linear regression. 

**Note:** Linear regression can and often is used even in situations where we do not know *a priori* that $f$ in the relation $y = f(x) + \epsilon$ is linear. 

**Question:** What do you think some of the pros and cons of using linear regression for supervised learning even if the function $f$ in the relationship $y = f(x) + \epsilon$ might not be linear?

## Overdetermined Linear Systems and $QR$ Factorization

Recall that a system of linear equations is an expression of the form

$$
\begin{align*}
a_{11}x_{1} + a_{12}x_{2} + \cdots + a_{1p} x_{p}  &= b_{1} \\
a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2p} x_{p} &= b_{2} \\
 &\vdots \\
a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{np} x_{p} &= b_{n} 
\end{align*}
$$

where there are $p$ unknowns $x_{j}$, $n \times p$ coefficients $a_{ij}$, and $n$ given values $b_{i}$. Such a system can be rewritten using matrix notation as

$$
\left[\begin{array}{cccc} a_{11} & a_{12} & \cdots & a_{1p} \\ a_{21} & a_{22} & \cdots & a_{2p} \\ \vdots & \ddots & \cdots & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{np} \end{array}\right] \left[\begin{array}{c} x_{1} \\ x_{2} \\ \vdots \\ x_{p} \end{array} \right] = \left[\begin{array}{c} b_{1} \\ b_{2} \\ \vdots \\ b_{n} \end{array} \right]
$$

or even more concisely as 

$$
A {\bf x} = {\bf b}
$$

and we say that $A$ is an $n \times p$ matrix. 

**Definition:** A linear system $A {\bf x} = {\bf b}$ is said to be **overdetermined** if there are more equations that unknowns. That is, if $n > p$. 

We will soon see that the problem of linear regression typically corresponds to "solving" an overdetermined linear system. There's a problem though, overdetermined linear systems do not usually have a solution. For example, consider the following linear system:

$$
\begin{align*}
x &= 1 \\
x &= 2
\end{align*}
$$
which is an overdetermined linear system since it has two equations in one unknown. This system clearly does not possess a solution. 

Given an $n \times p$ matrix $A$ and an  $n$-vector ${\bf b}$, for any vector $p$-vector ${\bf x}$ we can always form the residual ${\bf r}$ defined by 

$$
{\bf r} = {\bf b} - A{\bf x}
$$
Note that ${\bf x}$ is a solution to the linear system $A{\bf x} = {\bf b}$ is and only if the residual ${\bf r}$ is the zero vector.

Now, based on the last example we cannot generally make the the residual zero for an overdetermined system. However, we could instead search for a vector ${\bf x}$ that makes the residual as small as possible. In order to do so, we need a way to measure the size of a residual. Vector norms are a mathematical object that allow us to measure the size of residuals. 


### Norms: A Technical Tool

**Definition:** 

### $QR$ Factorization

$$
\left[\begin{array}{cc} 3 & 1 \\ 4 & 2 \end{array}\right] = \left[\begin{array}{cc} \frac{3}{5} & -\frac{4}{5} \\ \frac{4}{5} & \frac{3}{5} \end{array}\right]\left[\begin{array}{cc} 5 & \frac{11}{5} \\ 0 & \frac{2}{5} \end{array}\right]
$$



```{r}
A <- matrix(c(3,1,4,2),2,2,byrow = TRUE)
A_qr <- qr(A)
Q <- qr.Q(A_qr)
R <- qr.R(A_qr)
```



```{r}
#| code-fold: false

Q %*% R
```




```{r}
#| code-fold: false

round(Q %*% t(Q),2)
```




```{r}
#| code-fold: false

round(t(Q) %*% Q,2)
```

## Preparation for the next lesson

For the next lesson:

-   Read chapter 4 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 3 of *Statistical Learning with Math and R* [@suzuki2020statistical].

-   Watch the corresponding video lecture on classification. [View on YouTube](https://youtu.be/BMJQ3LQ_QKU?si=XvFgGO2jJ5OyR-v4).

```{r}
#| echo: false

vembedr::embed_youtube(id="BMJQ3LQ_QKU?si=XvFgGO2jJ5OyR-v4",height=450) %>%
  vembedr::use_align("center")
```

## References

::: {#refs}
:::

::: {.callout-tip collapse="true"}
## Expand for Session Info

```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```
:::

[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width="15%"}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)

