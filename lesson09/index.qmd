---
title: "Lesson 9"
subtitle: "Unsupervised Learning"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson09.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(ISLR2)
library(kableExtra)
library(ggthemes)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))


penguins <- penguins %>%
  drop_na()

penguins_X <- penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm)

```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the unsupervised learning methods of principal component analysis (PCA) and clustering.  

- Discuss the interpretations of PCA.

- Use `tidymodels` and `tidyclust` for PCA and clustering in practice. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 12 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapters 17, 20, 21, and 22 of [Hands-On Machine Learning with R*](https://bradleyboehmke.github.io/HOML/) [@boehmke2019hands].

- [View PCA video on YouTube](https://youtu.be/kpuQqOzQXfM?si=6TW_E3xckkiXU6qP).

```{r}
#| echo: false

vembedr::embed_youtube(id="kpuQqOzQXfM?si=6TW_E3xckkiXU6qP",height=450) %>%
  vembedr::use_align("center")
```

- [View k-means clustering video on YouTube](https://youtu.be/ded_NQqOe7I?si=7mi3VTkSLjZWgfCD).
    
```{r}
#| echo: false

vembedr::embed_youtube(id="ded_NQqOe7I?si=7mi3VTkSLjZWgfCD",height=450) %>%
  vembedr::use_align("center")
```


## Overview

Unsupervised learning focuses on finding patterns in data without a response variable.  This is in contrast to supervised learning where we have a response variable and are trying to predict it.  Unsupervised learning is often used for exploratory data analysis (EDA) and data reduction. Some common questions that can be addressed via unsupervised learning include:

1. Is there an informative way to visualize the data?

2. Are there groups of observations that are similar to each other?

3. Can we discover subgroups of observations that are distinct from each other?


Unsupervised learning can be much more challenging than supervised learning because there isn't necessarily a clear and simple goal such as prediction. Furthermore, there is a lack of established methods for evaluating the performance of an unsupervised learning method. That is, there isn't really a way for us to check our work in unsupervised learning. 


In this lesson, we will focus on two unsupervised learning methods: principal component analysis (PCA) and clustering.  PCA is a data reduction technique that finds a low-dimensional representation of the data that captures as much of the variation in the data as possible.  Clustering is a method that groups observations into clusters based on their similarity.  We will use the `tidymodels` and `tidyclust` packages for PCA and clustering in practice.


### Principal Component Analysis (PCA)

Suppose we have a large set of correlated variables. [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) allows us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. This is what we call dimension reduction. The principal component directions are the directions along which the original data vary the most. Often, the first few principal components are sufficient to summarize most of the variation in the data. This is particularly useful for data visualization. Another interpretation of principal components that is also very useful is: principal components provide low-dimensional linear surfaces that are *closest* to the observations.

We note that PCA is a matrix factorization of the data derived from a basic result in linear algebra known as the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD).  We will not go into the details of the SVD here, but we will discuss the results of the SVD and how they relate to PCA.



### Clustering

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. There are many different clustering methods, some common ones include:

- [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), and

- [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)

Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:

-  PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;

- Clustering looks to find homogeneous subgroups among the observations.


## Principal Component Analysis (PCA)

Before we explain what principal components are, let's consider an example data set. We will work with the four numerical columns from the `penguins` data set which we've stored in a data frame named `penguins_X`. The rows of `penguins_X` are shown below.

```{r}
#| code-fold: true

penguins_X %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```

We can compute the PCA for this data using the `prcomp` function as follows:

```{r}
penguins_pca <- prcomp(penguins_X, scale = TRUE)

```

Let's consider what is stored in the `penguins_pca` object. If we examine the object, we will see that it has to components: `rotation` and `x`. The rotation matrix will be of size $4 \times 4$ while the $x$ matrix will be the same size as the original data set which is $333 \times 4$.


First, we can look at the `rotation` component which contains the principal component directions. Each column of `rotation` contains a principal component direction. The `rotation` matrix is shown below.

```{r}
#| code-fold: true

penguins_pca$rotation %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```

We can also look at the `x` component which contains the principal component scores. Each column of `x` contains the principal component scores for a given principal component direction. The rows of `x` are shown below.

```{r}
#| code-fold: true

penguins_pca$x %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")


```


Let's see what happens when we multiply the $x$ matrix by the transpose of the `rotation` matrix. The rows of the resulting matrix are shown below.

```{r}
penguins_pca$x %*% t(penguins_pca$rotation) %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Now, we will see that the previous result is related to the original data set. The first rows of the normalized original data set are shown below.

```{r}
penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm) %>%
  scale() %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```


What we have just discovered, at least for this example is that the principal component analysis is a factorization of our data matrix into a product of two matrices. The first matrix is the $x$ matrix which contains what we call the principal components or scores. The second matrix is the transpose of the `rotation` matrix which contains the principal component directions or what are sometimes called the loadings. The product of these two matrices is equal to the normalized original data set. 

There is some additional information stored in the `penguins_pca` object. We can see the **proportion of variance explained** by each principal component direction using the `summary` function as follows:

```{r}

summary(penguins_pca)

```

The proportion of variance explained by each principal component direction provides information about how much of the information in the original data set is captured using each principal component direction. We see that for our data, a very large proportion of the variance is explained by the first two principal components. Let's add the principal components to the original data set and plot the first two principal components against each other.

```{r}
#| label: fig-penguins-pca-plot
#| fig-cap: Plot of the first two principal components for the penguins data set.

penguins_pca %>%
  augment(penguins) %>%
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, color = species)) +
  geom_point() + 
  scale_color_colorblind()

```

From @fig-penguins-pca-plot, we see that the first two principal components do a good job of separating the penguins by species.

### Singular Value Decomposition (SVD)


The [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) of a matrix is a very important result in linear algebra and is used in many different applications. We will not go into the details of the SVD here, but we will discuss the results of the SVD and how they relate to PCA. 

Let $A$ be an $m \times n$ matrix with entries from the real numbers. Further, suppose that $A$ has rank $r$. Then, there exists matrices $U$, $V$, and $\Sigma$ such that

$$
A = U \Sigma V^T
$$

where $U$ is an $m \times r$ orthogonal matrix, $V$ is an $n \times r$ orthogonal matrix, and $\Sigma$ is an $r \times r$ diagonal matrix. The diagonal entries of $\Sigma$ are non-negative numbers called the singular values of $A$ and are denoted by $\sigma_1, \sigma_2, \ldots, \sigma_r$. The singular values are ordered such that $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r \geq 0$. 

Let's explore in the context of our example the relationship between the SVD and PCA. The following shows matrix $V$ from the SVD of the normalized penguins data set.


```{r}
penguins_X %>%
  scale() %>%
  svd() %>%
  .$v %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Note that this is the same as the `rotation` matrix from the PCA. The following shows the matrix product $U\Sigma$ from the SVD of the normalized penguins data set.


```{r}
penguins_svd <- penguins_X %>%
  scale() %>%
  svd()

penguins_svd$u %*% diag(penguins_svd$d) %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Note that this is the same as the `x` matrix from the PCA. 

### How PCA Works

Suppose that we have a data matrix $X$. We will assume that the columns of $X$ have been centered so that the column means are zero. We now describe how to obtain the principal components of $X$, proceeding in sequential order. The first principal component of our features $X_{1}, X_{2}, \ldots , X_{p}$, that is, the columns of $X$ is the linear combination

$$
Z_{1} = \sigma_{11}X_{1} + \sigma_{21}X_{2} + \ldots + \sigma_{p1}X_{p}
$$

that has the largest variance and is normalized so that $\sum_{j=1}^{p}\sigma_{j1}^{2} = 1$. That is, the first principal component is the linear combination of the features that solves the optimization problem

$$
\text{maximize}_{\sigma_{11},\sigma_{21}, \ldots, \sigma_{p1}}\left\{\frac{1}{n}\sum_{i=1}^{n}\left(\sum_{j=1}^{p}\sigma_{j1}x_{ij} \right)^{2} \right\} \ \text{subject to } \sum_{j=1}^{p}\sigma_{j1}^{2} = 1
$$
and this optimization problem can be solved via linear algebra. After the first principal component $Z_{1}$ of $X$ has been determined, the second principal component of $Z_{2}$ is the linear combination of $X_{1},\ldots, X_{p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_{1}$. This happens exactly when the second load vector $\sigma_{2} = (\sigma_{12},\sigma_{22}, \ldots, \sigma_{p2})^{T}$ is orthogonal to the first one $\sigma_{1} = (\sigma_{11},\sigma_{21}, \ldots, \sigma_{p1})^{T}$. In other words, $Z_{2}$ solves the optimization problem

$$
\text{maximize}_{\sigma_{12},\sigma_{22}, \ldots, \sigma_{p2}}\left\{\frac{1}{n}\sum_{i=1}^{n}\left(\sum_{j=1}^{p}\sigma_{j2}x_{ij} \right)^{2} \right\} \ \text{subject to } \sum_{j=1}^{p}\sigma_{j2}^{2} = 1 \text{ and } \sum_{j=1}^{p}\sigma_{j1}\sigma_{j2} = 0
$$

This process continues until we have obtained $p$ principal components.

### Other Methods for Dimension Reduction

PCA works well when the features are approximately linearly related to each other. There are many other methods for dimension reduction that work well when the features are not linearly related to each other. For reference, here is a list of some of the more popular methods for dimension reduction:


#### Common Dimension Reduction Methods

1. [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)
   - R Package: `prcomp` (in base R)
   - Description: PCA is a linear dimension reduction method that identifies the most significant orthogonal components in the data, reducing the data's dimensionality while preserving the most important information.
   - Use Case: Use PCA to reduce the number of features while retaining most of the variance in the data, making it useful for data visualization, noise reduction, and feature selection.

2. [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
   - R Package: `Rtsne`
   - Description: t-SNE is a nonlinear dimension reduction technique that aims to preserve the pairwise similarities between data points in lower-dimensional space, making it effective for visualizing high-dimensional data clusters.
   - Use Case: Use t-SNE for visualizing high-dimensional data in 2D or 3D, often in applications like clustering analysis and visualization of complex datasets.

3. [Uniform Manifold Approximation and Projection (UMAP)](https://en.wikipedia.org/wiki/Uniform_manifold_approximation_and_projection)
   - R Package: `umap`
   - Description: UMAP is a dimension reduction method similar to t-SNE, but it uses a different mathematical framework. It's known for its speed and scalability while maintaining data structure.
   - Use Case: UMAP is suitable for visualizing and exploring high-dimensional data, often applied in clustering, visualization, and reducing dimensions for machine learning.

4. [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)
   - R Package: `keras`, `tfautodiff`, `h2o`
   - Description: Autoencoders are neural network architectures that aim to learn a compressed representation of the input data by training an encoder-decoder model.
   - Use Case: Autoencoders are versatile and can be used for feature extraction, denoising, anomaly detection, and generating new data samples.

5. [Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
   - R Package: `MASS`
   - Description: LDA is a supervised method that can also be used for dimension reduction. It finds linear combinations of features that maximize class separability.
   - Use Case: LDA is often used in classification problems, but it can also be used for dimension reduction when there's a clear class structure in the data.

6. [Multidimensional Scaling (MDS)](https://en.wikipedia.org/wiki/Multidimensional_scaling)
   - R Package: `MASS`, `cmdscale`
   - Description: MDS reduces dimensionality by finding a lower-dimensional representation that best preserves pairwise distances between data points.
   - Use Case: MDS is useful for visualizing dissimilarity or distance matrices, such as in psychology, geography, or genomics.

7. [Independent Component Analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis)
   - R Package: `fastICA`
   - Description: ICA is used to separate a multivariate signal into additive, independent components, which can be seen as a form of dimension reduction.
   - Use Case: ICA is applied in signal processing, blind source separation, and extracting underlying sources from mixed signals.



In our lab for this lesson, we will explore another dimension reduction technique known as [uniform manifold approximation and projection](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection)  (UMAP) that works well when the features are not linearly related to each other. UMAP operates by constructing a high-dimensional representation of the data, mapping it to a lower-dimensional space, and optimizing the embedding to maintain the density of data points in a manner that is both globally and locally consistent. It focuses on capturing the global structure of the data while also preserving fine-grained local relationships, making it well-suited for visualizing high-dimensional data and discovering clusters or patterns. UMAP's ability to balance global and local aspects of data structure distinguishes it as a valuable tool for exploratory data analysis, clustering, and manifold learning. The technical details of UMAP are beyond the scope of this course, but we will explore how to use it in the lab for this lesson.

## Clustering



### k-means Clustering


### Hierarchical Clustering



## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)