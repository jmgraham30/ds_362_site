---
title: "Lesson 9"
subtitle: "Unsupervised Learning"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson09.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(ISLR2)
library(kableExtra)
library(ggthemes)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))


penguins <- penguins %>%
  drop_na()

penguins_X <- penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm)

```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the unsupervised learning methods of principal component analysis (PCA) and clustering.  

- Discuss the interpretations of PCA.

- Use `tidymodels` and `tidyclust` for PCA and clustering in practice. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 12 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapters 17, 20, 21, and 22 of [Hands-On Machine Learning with R*](https://bradleyboehmke.github.io/HOML/) [@boehmke2019hands].

- [View PCA video on YouTube](https://youtu.be/kpuQqOzQXfM?si=6TW_E3xckkiXU6qP).

```{r}
#| echo: false

vembedr::embed_youtube(id="kpuQqOzQXfM?si=6TW_E3xckkiXU6qP",height=450) %>%
  vembedr::use_align("center")
```

- [View k-means clustering video on YouTube](https://youtu.be/ded_NQqOe7I?si=7mi3VTkSLjZWgfCD).
    
```{r}
#| echo: false

vembedr::embed_youtube(id="ded_NQqOe7I?si=7mi3VTkSLjZWgfCD",height=450) %>%
  vembedr::use_align("center")
```


## Overview

Unsupervised learning focuses on finding patterns in data without a response variable.  This is in contrast to supervised learning where we have a response variable and are trying to predict it.  Unsupervised learning is often used for exploratory data analysis (EDA) and data reduction. Some common questions that can be addressed via unsupervised learning include:

1. Is there an informative way to visualize the data?

2. Are there groups of observations that are similar to each other?

3. Can we discover subgroups of observations that are distinct from each other?


Unsupervised learning can be much more challenging than supervised learning because there isn't necessarily a clear and simple goal such as prediction. Furthermore, there is a lack of established methods for evaluating the performance of an unsupervised learning method. That is, there isn't really a way for us to check our work in unsupervised learning. 


In this lesson, we will focus on two unsupervised learning methods: principal component analysis (PCA) and clustering.  PCA is a data reduction technique that finds a low-dimensional representation of the data that captures as much of the variation in the data as possible.  Clustering is a method that groups observations into clusters based on their similarity.  We will use the `tidymodels` and `tidyclust` packages for PCA and clustering in practice.


### Principal Component Analysis (PCA)

Suppose we have a large set of correlated variables. [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) allows us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. This is what we call dimension reduction. The principal component directions are the directions along which the original data vary the most. Often, the first few principal components are sufficient to summarize most of the variation in the data. This is particularly useful for data visualization. Another interpretation of principal components that is also very useful is: principal components provide low-dimensional linear surfaces that are *closest* to the observations.

We note that PCA is a matrix factorization of the data derived from a basic result in linear algebra known as the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD).  We will not go into the details of the SVD here, but we will discuss the results of the SVD and how they relate to PCA.



### Clustering

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. There are many different clustering methods, some common ones include:

- [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), and

- [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)

Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:

-  PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;

- Clustering looks to find homogeneous subgroups among the observations.


## Principal Component Analysis (PCA)

Before we explain what principal components are, let's consider an example data set. We will work with the four numerical columns from the `penguins` data set which we've stored in a data frame named `penguins_X`. The first few rows of `penguins_X` are shown below.

```{r}
#| code-fold: true

penguins_X %>% 
  head() %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```

We can compute the PCA for this data using the `prcomp` function as follows:

```{r}
penguins_pca <- prcomp(penguins_X, scale = TRUE)

```

Let's consider what is stored in the `penguins_pca` object. If we examine the object, we will see that it has to components: `rotation` and `x`. The rotation matrix will be of size $4 \times 4$ while the $x$ matrix will be the same size as the original data set which is $333 \times 4$.


First, we can look at the `rotation` component which contains the principal component directions. Each column of `rotation` contains a principal component direction. The `rotation` matrix is shown below.

```{r}
#| code-fold: true

penguins_pca$rotation %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```

We can also look at the `x` component which contains the principal component scores. Each column of `x` contains the principal component scores for a given principal component direction. The first few rows of `x` are shown below.

```{r}
#| code-fold: true

penguins_pca$x %>% 
  head() %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")


```


Let's see what happens when we multiply the $x$ matrix by the transpose of the `rotation` matrix. The first few rows of the resulting matrix are shown below.

```{r}
penguins_pca$x %*% t(penguins_pca$rotation) %>%
  head() %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Now, we will see that the previous result is related to the original data set. The first few rows of the normalized original data set are shown below.

```{r}
penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm) %>%
  scale() %>%
  head() %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```


What we have just discovered, at least for this example is that the principal component analysis is a factorization of our data matrix into a product of two matrices. The first matrix is the $x$ matrix which contains what we call the principal components or scores. The second matrix is the transpose of the `rotation` matrix which contains the principal component directions or what are sometimes called the loadings. The product of these two matrices is equal to the normalized original data set. 

There is some additional information stored in the `penguins_pca` object. We can see the **proportion of variance explained** by each principal component direction using the `summary` function as follows:

```{r}

summary(penguins_pca)

```

The proportion of variance explained by each principal component direction provides information about how much of the information in the original data set is captured using each principal component direction. We see that for our data, a very large proportion of the variance is explained by the first two principal components. Let's add the principal components to the original data set and plot the first two principal components against each other.

```{r}
#| label: fig-penguins-pca-plot
#| fig-cap: Plot of the first two principal components for the penguins data set.

penguins_pca %>%
  augment(penguins) %>%
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, color = species)) +
  geom_point() + 
  scale_color_colorblind()

```

From @fig-penguins-pca-plot, we see that the first two principal components do a good job of separating the penguins by species.

### Singular Value Decomposition (SVD)


The [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) of a matrix is a very important result in linear algebra and is used in many different applications. We will not go into the details of the SVD here, but we will discuss the results of the SVD and how they relate to PCA. 

```{r}
penguins_X %>%
  scale() %>%
  svd() %>%
  .$v %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

### How PCA Works

## Clustering



### k-means Clustering


### Hierarchical Clustering



## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)