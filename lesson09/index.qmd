---
title: "Lesson 9"
subtitle: "Unsupervised Learning"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson09.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(tidyclust)
library(ISLR2)
library(kableExtra)
library(ggthemes)
library(factoextra)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))


penguins <- penguins %>%
  drop_na()

penguins_X <- penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm)

```

## Learning Objectives

After this lesson, students will be able to: 

- Describe the unsupervised learning methods of principal component analysis (PCA) and clustering.  

- Discuss the interpretations of PCA.

- Use `tidymodels` and `tidyclust` for PCA and clustering in practice. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 12 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapters 17, 20, 21, and 22 of [Hands-On Machine Learning with R*](https://bradleyboehmke.github.io/HOML/) [@boehmke2019hands].

- [View PCA video on YouTube](https://youtu.be/kpuQqOzQXfM?si=6TW_E3xckkiXU6qP).

```{r}
#| echo: false

vembedr::embed_youtube(id="kpuQqOzQXfM?si=6TW_E3xckkiXU6qP",height=450) %>%
  vembedr::use_align("center")
```

- [View k-means clustering video on YouTube](https://youtu.be/ded_NQqOe7I?si=7mi3VTkSLjZWgfCD).
    
```{r}
#| echo: false

vembedr::embed_youtube(id="ded_NQqOe7I?si=7mi3VTkSLjZWgfCD",height=450) %>%
  vembedr::use_align("center")
```


## Overview

Unsupervised learning focuses on finding patterns in data without a response variable.  This is in contrast to supervised learning where we have a response variable and are trying to predict it.  Unsupervised learning is often used for exploratory data analysis (EDA) and data reduction. Some common questions that can be addressed via unsupervised learning include:

1. Is there an informative way to visualize the data?

2. Are there groups of observations that are similar to each other?

3. Can we discover subgroups of observations that are distinct from each other?


Unsupervised learning can be much more challenging than supervised learning because there isn't necessarily a clear and simple goal such as prediction. Furthermore, there is a lack of established methods for evaluating the performance of an unsupervised learning method. That is, there isn't really a way for us to check our work in unsupervised learning. 


In this lesson, we will focus on two unsupervised learning methods: principal component analysis (PCA) and clustering.  PCA is a data reduction technique that finds a low-dimensional representation of the data that captures as much of the variation in the data as possible.  Clustering is a method that groups observations into clusters based on their similarity.  We will use the `tidymodels` and `tidyclust` packages for PCA and clustering in practice.


### Principal Component Analysis (PCA)

Suppose we have a large set of correlated variables. [Principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) allows us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. This is what we call dimension reduction. The principal component directions are the directions along which the original data vary the most. Often, the first few principal components are sufficient to summarize most of the variation in the data. This is particularly useful for data visualization. Another interpretation of principal components that is also very useful is: principal components provide low-dimensional linear surfaces that are *closest* to the observations.

We note that PCA is a matrix factorization of the data derived from a basic result in linear algebra known as the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD).  We will not go into the details of the SVD here, but we will discuss the results of the SVD and how they relate to PCA.



### Clustering

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. There are many different clustering methods, some common ones include:

- [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), and

- [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)

Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:

-  PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;

- Clustering looks to find homogeneous subgroups among the observations.


## Principal Component Analysis (PCA)

Before we explain what principal components are, let's consider an example data set. We will work with the four numerical columns from the `penguins` data set which we've stored in a data frame named `penguins_X`. The rows of `penguins_X` are shown below.

```{r}
#| code-fold: true

penguins_X %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```

We can compute the PCA for this data using the `prcomp` function as follows:

```{r}
penguins_pca <- prcomp(penguins_X, scale = TRUE)

```

Let's consider what is stored in the `penguins_pca` object. If we examine the object, we will see that it has two components: `rotation` and `x`. The rotation matrix will be of size $4 \times 4$ while the $x$ matrix will be the same size as the original data set which is $333 \times 4$.


First, we can look at the `rotation` component which contains the principal component directions. Each column of `rotation` contains a principal component direction. The `rotation` matrix is shown below.

```{r}
#| code-fold: true

penguins_pca$rotation %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```

We can also look at the `x` component which contains the principal component scores. Each column of `x` contains the principal component scores for a given principal component direction. The rows of `x` are shown below.

```{r}
#| code-fold: true

penguins_pca$x %>% 
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")


```


Let's see what happens when we multiply the $x$ matrix by the transpose of the `rotation` matrix. The rows of the resulting matrix are shown below.

```{r}
#| code-fold: true


penguins_pca$x %*% t(penguins_pca$rotation) %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Now, we will see that the previous result is related to the original data set. The first rows of the normalized original data set are shown below.

```{r}
#| code-fold: true

penguins %>%
  select(flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm) %>%
  scale() %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

```


What we have just discovered, at least for this example is that the principal component analysis is a factorization of our data matrix into a product of two matrices. The first matrix is the $x$ matrix which contains what we call the principal components or scores. The second matrix is the transpose of the `rotation` matrix which contains the principal component directions or what are sometimes called the loadings. The product of these two matrices is equal to the normalized original data set. 

There is some additional information stored in the `penguins_pca` object. We can see the **proportion of variance explained** by each principal component direction using the `summary` function as follows:

```{r}

summary(penguins_pca)

```

The proportion of variance explained by each principal component direction provides information about how much of the information in the original data set is captured using each principal component direction. We see that for our data, a very large proportion of the variance is explained by the first two principal components. Let's add the principal components to the original data set and plot the first two principal components against each other.

```{r}
#| label: fig-penguins-pca-plot
#| fig-cap: Plot of the first two principal components for the penguins data set.

penguins_pca %>%
  augment(penguins) %>%
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, color = species)) +
  geom_point() + 
  scale_color_colorblind()

```

From @fig-penguins-pca-plot, we see that the first two principal components do a good job of separating the penguins by species.

Now you should have a well-developed intuition for what principal components are. We will now discuss some of the mathematics behind principal components.

### Singular Value Decomposition (SVD)


The [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) of a matrix is a very important result in linear algebra and is used in many different applications. We will not go into the details of the SVD here, but we will discuss the results of the SVD and how they relate to PCA. 

Let $A$ be an $m \times n$ matrix with entries from the real numbers. Further, suppose that $A$ has rank $r$. Then, there exists matrices $U$, $V$, and $\Sigma$ such that

$$
A = U \Sigma V^T
$$

where $U$ is an $m \times r$ orthogonal matrix, $V$ is an $n \times r$ orthogonal matrix, and $\Sigma$ is an $r \times r$ diagonal matrix. The diagonal entries of $\Sigma$ are non-negative numbers called the singular values of $A$ and are denoted by $\sigma_1, \sigma_2, \ldots, \sigma_r$. The singular values are ordered such that $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r \geq 0$. 

Let's explore in the context of our example the relationship between the SVD and PCA. The following shows matrix $V$ from the SVD of the normalized penguins data set.


```{r}
penguins_X %>%
  scale() %>%
  svd() %>%
  .$v %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Note that this is the same as the `rotation` matrix from the PCA. The following shows the matrix product $U\Sigma$ from the SVD of the normalized penguins data set.


```{r}
penguins_svd <- penguins_X %>%
  scale() %>%
  svd()

penguins_svd$u %*% diag(penguins_svd$d) %>%
  kable() %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

Note that this is the same as the `x` matrix from the PCA. 

### How PCA Works

Suppose that we have a data matrix $X$. We will assume that the columns of $X$ have been centered so that the column means are zero. We now describe how to obtain the principal components of $X$, proceeding in sequential order. The first principal component of our features $X_{1}, X_{2}, \ldots , X_{p}$, that is, the columns of $X$ is the linear combination

$$
Z_{1} = \sigma_{11}X_{1} + \sigma_{21}X_{2} + \ldots + \sigma_{p1}X_{p}
$$

that has the largest variance and is normalized so that $\sum_{j=1}^{p}\sigma_{j1}^{2} = 1$. That is, the first principal component is the linear combination of the features that solves the optimization problem

$$
\text{maximize}_{\sigma_{11},\sigma_{21}, \ldots, \sigma_{p1}}\left\{\frac{1}{n}\sum_{i=1}^{n}\left(\sum_{j=1}^{p}\sigma_{j1}x_{ij} \right)^{2} \right\} \ \text{subject to } \sum_{j=1}^{p}\sigma_{j1}^{2} = 1
$$
and this optimization problem can be solved via linear algebra. After the first principal component $Z_{1}$ of $X$ has been determined, the second principal component of $Z_{2}$ is the linear combination of $X_{1},\ldots, X_{p}$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_{1}$. This happens exactly when the second load vector $\sigma_{2} = (\sigma_{12},\sigma_{22}, \ldots, \sigma_{p2})^{T}$ is orthogonal to the first one $\sigma_{1} = (\sigma_{11},\sigma_{21}, \ldots, \sigma_{p1})^{T}$. In other words, $Z_{2}$ solves the optimization problem

$$
\text{maximize}_{\sigma_{12},\sigma_{22}, \ldots, \sigma_{p2}}\left\{\frac{1}{n}\sum_{i=1}^{n}\left(\sum_{j=1}^{p}\sigma_{j2}x_{ij} \right)^{2} \right\} \ \text{subject to } \sum_{j=1}^{p}\sigma_{j2}^{2} = 1 \text{ and } \sum_{j=1}^{p}\sigma_{j1}\sigma_{j2} = 0
$$

This process continues until we have obtained $p$ principal components.

### Other Methods for Dimension Reduction

PCA works well when the features are approximately linearly related to each other. There are many other methods for dimension reduction that work well when the features are not linearly related to each other. For reference, here is a list of some of the more popular methods for dimension reduction:


#### Common Dimension Reduction Methods

1. [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)
   - R Package: `prcomp` (in base R)
   - Description: PCA is a linear dimension reduction method that identifies the most significant orthogonal components in the data, reducing the data's dimensionality while preserving the most important information.
   - Use Case: Use PCA to reduce the number of features while retaining most of the variance in the data, making it useful for data visualization, noise reduction, and feature selection.

2. [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
   - R Package: `Rtsne`
   - Description: t-SNE is a nonlinear dimension reduction technique that aims to preserve the pairwise similarities between data points in lower-dimensional space, making it effective for visualizing high-dimensional data clusters.
   - Use Case: Use t-SNE for visualizing high-dimensional data in 2D or 3D, often in applications like clustering analysis and visualization of complex datasets.

3. [Uniform Manifold Approximation and Projection (UMAP)](https://en.wikipedia.org/wiki/Uniform_manifold_approximation_and_projection)
   - R Package: `umap`
   - Description: UMAP is a dimension reduction method similar to t-SNE, but it uses a different mathematical framework. It's known for its speed and scalability while maintaining data structure.
   - Use Case: UMAP is suitable for visualizing and exploring high-dimensional data, often applied in clustering, visualization, and reducing dimensions for machine learning.

4. [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)
   - R Package: `keras`, `tfautodiff`, `h2o`
   - Description: Autoencoders are neural network architectures that aim to learn a compressed representation of the input data by training an encoder-decoder model.
   - Use Case: Autoencoders are versatile and can be used for feature extraction, denoising, anomaly detection, and generating new data samples.

5. [Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
   - R Package: `MASS`
   - Description: LDA is a supervised method that can also be used for dimension reduction. It finds linear combinations of features that maximize class separability.
   - Use Case: LDA is often used in classification problems, but it can also be used for dimension reduction when there's a clear class structure in the data.

6. [Multidimensional Scaling (MDS)](https://en.wikipedia.org/wiki/Multidimensional_scaling)
   - R Package: `MASS`, `cmdscale`
   - Description: MDS reduces dimensionality by finding a lower-dimensional representation that best preserves pairwise distances between data points.
   - Use Case: MDS is useful for visualizing dissimilarity or distance matrices, such as in psychology, geography, or genomics.

7. [Independent Component Analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis)
   - R Package: `fastICA`
   - Description: ICA is used to separate a multivariate signal into additive, independent components, which can be seen as a form of dimension reduction.
   - Use Case: ICA is applied in signal processing, blind source separation, and extracting underlying sources from mixed signals.



In our lab for this lesson, we will explore another dimension reduction technique known as [uniform manifold approximation and projection](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Uniform_manifold_approximation_and_projection)  (UMAP) that works well when the features are not linearly related to each other. UMAP operates by constructing a high-dimensional representation of the data, mapping it to a lower-dimensional space, and optimizing the embedding to maintain the density of data points in a manner that is both globally and locally consistent. It focuses on capturing the global structure of the data while also preserving fine-grained local relationships, making it well-suited for visualizing high-dimensional data and discovering clusters or patterns. UMAP's ability to balance global and local aspects of data structure distinguishes it as a valuable tool for exploratory data analysis, clustering, and manifold learning. The technical details of UMAP are beyond the scope of this course, but we will explore how to use it in the lab for this lesson.

## Clustering

In this section, we will explore the concept of clustering and how it can be used to discover patterns in data. Clustering is an unsupervised learning technique that aims to group similar data points together in a way that is meaningful or useful. It is often used as an exploratory data analysis technique to discover patterns in unlabeled data, such as in market segmentation, social network analysis, and image segmentation. Clustering is also used as a preprocessing step for other machine learning algorithms, such as anomaly detection and dimensionality reduction. In this lesson, we will explore two of the most popular clustering algorithms: k-means clustering and hierarchical clustering.

### k-means Clustering

[k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering) is a simple and popular clustering algorithm that aims to partition $n$ observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. The number of clusters $k$ is specified by the user, and the algorithm assigns each observation to exactly one cluster. The algorithm works as follows:

1. Randomly assign a number, from 1 to $k$, to each of the observations. These serve as initial cluster assignments for the observations.

2. Iterate until the clusters stop changing:
     
     a. For each cluster, compute the cluster centroid by taking the mean vector of points in the cluster.
   
     b. Assign each data point to the cluster for which the centroid is the closest.
     
  
@fig-k-means shows an example of the k-means clustering algorithm steps with $k=3$.  
  
![The progress of the k-means algorithm. ](https://www.dropbox.com/scl/fi/buy1bg61fcfv3lmg8dkm4/12_8.png?rlkey=c2uu761sns0yyl3neq3uoxof2&raw=1){#fig-k-means}

The algorithm is guaranteed to converge to a result, but the result may be a local optimum rather than a global optimum. Therefore, it is common to run the algorithm multiple times from different random initializations. The final result is chosen to be the one with the lowest within-cluster sum of squares, which is a measure of the compactness of the clusters. The within-cluster sum of squares is defined as the sum of the squared Euclidean distances between each point in the cluster and the cluster centroid. The algorithm is sensitive to the initial cluster assignments, so it is common to run the algorithm multiple times from different random initializations. The final result is chosen to be the one with the lowest within-cluster sum of squares, which is a measure of the compactness of the clusters. The within-cluster sum of squares is defined as the sum of the squared Euclidean distances between each point in the cluster and the cluster centroid.


### Hierarchical Clustering

[Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is another popular clustering algorithm that aims to build a hierarchy of clusters. The algorithm starts by treating each observation as its own cluster. Then, it repeatedly merges the two closest clusters until only one cluster remains. The result is a tree-based representation of the observations, called a *dendrogram*, which allows us to view the results of hierarchical clustering at different levels of granularity. The algorithm starts with the smallest units (individual observations) and merges them together to form larger clusters. We will focus on bottom-up or *agglomerative*  hierarchical clustering. The algorithm can be summarized as follows:

1. Begin with $n$ observations and a measure (such as Euclidean distance) of all the $n(n-1)/2$ pairwise dissimilarities. Treat each observation as its own cluster.

2. For $i = n, n-1, \ldots, 2$:

     a. Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Merge these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion line should be placed.

     b. Compute the new pairwise inter-cluster dissimilarities among the $i-1$ remaining clusters.
     
Let's describe how to interpret a dendrogram. @fig-dendrogram shows a denrogram obtained through hierarchical clustering of the four physical measurements in the `penguins` data set. Note that the colors are added to the plot after the fact and are not a result of the clustering algorithm.  

```{r}
#| code-fold: true
#| message: false
#| warning: false
#| label: fig-dendrogram
#| fig-cap: A dendrogram showing the hierarchical clustering of the observations for the four physical measurement variables from the `penguins` data set. The hieght along the vertical axis indicates how different observations are. The height of the horizontal dashed line shows the distance at which the algorithm would cut the dendrogram to form three distinct clusters. The colors are added to the plot after the fact and are not a result of the clustering algorithm.

res_hclust_complete <- hier_clust(linkage_method = "complete") %>%
  fit(~., data = penguins_X)

# Plot the dendrogram
res_hclust_complete %>%
  extract_fit_engine() %>%
  fviz_dend(main = "complete",k=3,
            palette=c("#E69F00", "#56B4E9", "#009E73")) + 
  geom_hline(yintercept = 2000,linetype="dashed",color="darkgray")
```


The leaves at the very bottom of the dendrogram are each of the individual observations in the data. As one moves up the tree, leaves and branches fuse with earlier fusions indicating greater similarity between observations. Note that for any two observations, we can look for the point in the dendrogram where the two branches that contain them fuse. The height of the fusion line at that point indicates how different the two observations are. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.


@fig-dendrogram-interpretation shows an example of how to interpret a dendrogram with nine observations in two-dimensional space. One can see that observations 5 and 7 are quite similar to each other, since they fuse at the lowest point on the dendrogram. Obser- vations 1 and 6 are also quite similar to each other. However, it is tempting but incorrect to conclude from the figure that observations 9 and 2 are quite similar to each other on the basis that they are located near each other on the dendrogram. In fact, based on the information contained in the dendrogram, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7. 

![An illustration of how to properly interpret a dendrogram with nine observations in two-dimensional space. (Left) a dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar to each other, as are observations 1 and 6. However, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7, even though observations 9 and 2 are close together in terms of horizontal distance. This is because observations 2,8,5, and 7 all fuse with observation 9 at the same height, approximately 1.8. (Right) the raw data used to generate the dendrogram can be used to confirm that indeed, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7.
](https://www.dropbox.com/scl/fi/zrpir4mf6uwg7pwl9mhtf/12_12.png?rlkey=9cy27dlr3r4ixzw7ak073qfmw&raw=1){#fig-dendrogram-interpretation}

We will explore both k-means and hierarchical clustering further in the lab for this lesson.

## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)