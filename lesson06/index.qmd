---
title: "Lesson 6"
subtitle: "Tree-based Models"
author: "JMG"
format:
  html:
    echo: true
    code-fold: false
    code-summary: "Show the code"
    toc: true
    toc-location: left
bibliography: lesson06.bib
license: "CC BY-NC-SA 4.0"
---

```{r}
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(ISLR2)
library(vip)
library(parttree)

tidymodels_prefer()

theme_set(theme_minimal(base_size = 12))
```

## Learning Objectives

After this lesson, students will be able to: 

- Define decision tree and understand the splitting algorithm for tree-based regression and classification. 

- Define the concepts of bagging, boosting, and random forests.

- Use the `tidymodels` workflow to fit and tune various tree-based models. 


## Readings, etc.

For this lesson, refer to the following readings, etc.:

- Read chapter 8 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 8 of *Statistical Learning with Math and R* [@suzuki2020statistical].

-   Watch the corresponding video lecture on decision trees. [View on YouTube](https://youtu.be/QNnayf--_yk?si=x-oVRRqQE1cK-oZJ).

```{r}
#| echo: false

vembedr::embed_youtube(id="QNnayf--_yk?si=x-oVRRqQE1cK-oZJ",height=450) %>%
  vembedr::use_align("center")
```


- Go through the following two blog posts by Julia Silge:

    - [Predicting injuries for Chicago traffic crashes](https://juliasilge.com/blog/chicago-traffic-model/)
    
    - [Predict availability in water sources with random forest models](https://juliasilge.com/blog/water-sources/)


## Overview

Tree-based models are a class of *nonparametric* algorithms that work by partitioning the feature space, that is, the predictors into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. Predictions are obtained by averaging response values in each region. Such divide-and-conquer methods can produce simple rules that are easy to interpret and visualize. However, simple decision trees typically lack in predictive performance compared to more complex algorithms. In order to address this, techniques such as bagging, boosting, and random forests have been developed. Each of these approaches involve fitting multiple trees which are combined in some way that results in improved accuracy but at a cost of some loss in interpretability.  


## Basic Decision Trees

### Motivating Example

To motivate decision trees, let's consider a regression problem. The `Hitters` data from the `ISLR2` package contains data on major league baseball players. We would like to use this data to predict player salaries based on information about the players and their performance. Let's start by getting a glimpse of the data.

```{r}
#| echo: false


glimpse(Hitters)
```

We see that there are some missing values for the `Salary` response variable so let's remove the rows with the missing observations.

```{r}
Hitters <- Hitters %>%
  filter(!is.na(Salary))
```

Let's also look at the distribution for `Salary`.

```{r}
#| code-fold: true
#| message: false
#| label: fig-player-salaries
#| fig-cap: | 
#|  Histrogram of salaries for players recorded in the `Hitters` data set from the `ISLR2` package. 


Hitters %>%
  ggplot(aes(x=Salary)) + 
  geom_histogram(color="white")
```

Since the data is skewed, we should $\log$ transform the response variable. 

```{r}
#| code-fold: true
#| message: false
#| label: fig-player-salaries-log
#| fig-cap: | 
#|  Histrogram ($\log$ scale) of salaries for players recorded in the `Hitters` data set from 
#|  the `ISLR2` package. 

Hitters <- Hitters %>%
  mutate(Salary_log = log(Salary))

Hitters %>%
  ggplot(aes(x=Salary_log)) + 
  geom_histogram(color="white") + 
  labs(x = "Salary (log)")
```

For illustrative purposes, let's suppose we want to model the $\log$ scaled salary as a response to the predictor variables `Hits` which records the number of by the player in their most recent year of play and `Years` which is the numbe of years the player has played. We first examine a plot of the relationship between these variables.

```{r}
#| code-fold: true
#| message: false
#| label: fig-player-salaries-hits
#| fig-cap: | 
#|  Scatter plot of ($\log$ scale) of salaries versus hits for players recorded 
#|  in the `Hitters` data set from the `ISLR2` package. 


Hitters %>%
  ggplot(aes(x = Years, y = Hits, color=Salary_log)) + 
  geom_point() + 
  labs(color = "Salary (log)")
```

For this problem, the basic decision tree algorithm for regression will separate the `Years-Hits` predictor or feature space (in this case the plane) into some number of distinct regions and then make predictions by averaging the $\log$ scaled salary values in each of the regions. 

The question is, how to we divide up the predictor space?  

### Basic Decison Tree Regression Algorithm

To build a regression tree, there are two basic steps:

1. Divide the predictor space. Suppose there are $p$ predictors so that ${\bf x} = (x_{1}, x_{2}, \ldots , x_{p})$. For the set of all possible predictor values, divide this up into $J$ non-overlapping regions, $R_{1}, R_{2}, \ldots ,R_{J}$. 

2. For every observation that falls into region $R_{j}$ we make the same prediction, which is simply the mean of the response values for the training observations in $R_{j}$.


The question is, how do we construct the regions $R_{1}, R_{2}, \ldots ,R_{J}$? A typical approach is to divide the predictor space into *boxes*, that is, each $R_{j}$ will be a set of the form $(a_{1j}, b_{1j}) \times (a_{2j},b_{2j}) \times \cdots \times (a_{pj},b_{pj})$. Then the goal is to find boxes that minimize

$$
\text{RSS} = \sum_{j=1}^{J}\sum_{i \in R_{j}}(y_{i} - \bar{y}_{R_{j}})^2
$$

There is a major challenge to minimizing the RSS in this setting:

> It is infeasible to consider every possible partition of the space of predictors into $J$ boxes. 

Instead of an exhaustive search, the typical approach is what is known as **recursive binary splitting**. Here is how it works:

1. Select the predictor $x_{j}$ and a cutpoint $s$ such that splitting the predictor space into the regions $\{{\bf x} | x_{j} < s\}$ and $\{{\bf x} | x_{j} \geq s\}$ produces the greatest possible reduction in RSS.

2. Repeat the process, but this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. This will result in three regions and we attempt to split one of those, etc. The process continues until a stopping criterion is reached. This algorithm is an example of a *greedy* algorithm because at each step of the tree-building process, the best split is made at that step.

@fig-three-split shows an example of splitting applied to the   `Hitters` data set from the `ISLR2` package. 

![A three-region partition for the `Hitters` data set from the `ISLR2` package.](https://www.dropbox.com/scl/fi/in7v3m89cpfm3y1llmaa9/8_2.jpg?rlkey=b70m3u4v1zpmrlcmhvrkh4s8s&raw=1){#fig-three-split}

We can represent this result by the tree diagram shown in @fig-three-split-tree.

![The tree diagram for the split shown in @fig-three-split.](https://www.dropbox.com/scl/fi/j6ar9pe82uhx3ea95mahe/8_1.jpg?rlkey=d78x4028k37sdzl3qu6d5bdvk&raw=1){#fig-three-split-tree width=50% height=50%}

Once the regions $R_{1}, R_{2}, \ldots, R_{J}$ have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.  

Here is an example in R using the `rpart` package that fits and plots a similar decision tree model:

```{r}
hitters_dt <- rpart::rpart(Salary_log ~ Hits + Years,
                           data=Hitters,
                           method="anova",
                           control=list(cp = 0.1,maxdepth=2,
                                        minsplit=20,xval = 0))

rpart.plot::rpart.plot(hitters_dt)
```


@fig-predictions shows the regions and predictions from the decision tree obtained with the `rpart` function. This figure is obtained by using functions from the [`parttree`](https://github.com/grantmcdermott/parttree) package.

```{r}
#| code-fold: true
#| label: fig-predictions
#| fig-cap: The `Hitters` data from @fig-three-split shows with the regions and predictions from the decision tree obtained with the `rpart` function. 


Hitters %>%
  ggplot(aes(x=Years,y=Hits)) + 
  geom_parttree(data=hitters_dt,aes(fill = Salary_log),alpha=0.3) + 
  geom_point(aes(color=Salary_log)) +
  scale_colour_viridis_c(aesthetics = c("color", "fill"))
```


The `vip` package allows us to visualize the variable importance scores for the predictors in a decision tree model. That is, a measure of the decrease in RSS due to splits over a given predictor. @fig-vip shows the variable importance scores for the predictors in the `Hitters` data based on the decision tree obtained with the `rpart` function.

```{r}
#| code-fold: true
#| label: fig-vip
#| fig-cap: The variable importance scores for the predictors in the `Hitters` data based on the decision tree obtained with the `rpart` function. 


vip(hitters_dt,aesthetics = list(fill = "midnightblue", alpha = 0.8))
```

### Pruning

It is easy to overfit a decision tree to training data. An approach to dealing with this issue is to use a technique known as *pruning*. The idea is to build an initially large tree $T_{0}$, and then prune it back to a subtree that leads to the lowest test error rate. In order to do so, we will need to incorporate some way to estimate test error. 

The approach we take, known as **cost complexity pruning** is to consider a sequence of subtrees indexed by a non-negative parameter $\alpha$. For each value of $\alpha$, there corresponds a subtree $T \subset T_{0}$ that minimizes the following expression: 

$$
\sum_{m=1}^{|T|} \sum_{i:\ x_{i} \in R_{m}} (y_{i} - \bar{y}_{R_{m}})^2 + \alpha |T|
$$

Here, $|T|$ is the number of terminal nodes of the subtree $T$, $R_{m}$ is the box corresponding to the $m$th terminal node, and $\bar{y}_{R_{m}}$ is the predicted response associated with $R_{m}$. The tuning parameter $\alpha$ is a penalty that controls a trade-off between the subtree's complexity and its fit to the training data. When $\alpha = 0$, then the subtree will be $T_{0}$. As $\alpha$ increases, the cost of having more terminal nodes is higher so the higher $\alpha$, the smaller $T$ will tend to be.   

@tbl-reg-tree-alg summarizes our algorithm for building regression tree by pruning. 

|    Algorithm: Building a Regression Tree       |
|:----------------------------------------------|
| 1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.       |
| 2. Apply cost complexity pruning to the large tree in order to obtain a sequence of  best subtrees, as a function of $\alpha$.  |
| 3. Use $V$-fold cross-validation to choose $\alpha$. That is, divide the training observations into $V$ folds. For each $k=1,\ldots , V$: |
|    (a) Repeat Steps 1 and 2 on all but the $v$th fold of the training data.|
|    (b) Evaluate the mean squared prediction error on the data in the left-out $v$th fold, as a function of $\alpha$.|
|Average the results for each value of $\alpha$, and pick $\alpha$ to minimize the average error.|
| 4. Return the subtree from Step 2 that corresponds to the chosen value of $\alpha$. |

: Regression tree algorithm that uses pruning. {#tbl-reg-tree-alg}

In the function `rpart` briefly introduced earlier, one can control the complexity parameter through setting a value named `cp` and the maximum depth with `maxdepth`. Let's see what happens if we decrease `cp` and increase `maxdepth` in comparison with our earlier tree for the `Hitters` data.

```{r}
hitters_dt_2 <- rpart::rpart(Salary_log ~ Hits + Years,
                           data=Hitters,
                           method="anova",
                           control=list(cp = 0.01,maxdepth=20,
                                        minsplit=10,xval = 10))

rpart.plot::rpart.plot(hitters_dt_2)
```

We can also plot the results of the cross-validation:

```{r}
#| label: fig-cp-cv
#| fig-cap: The results of cross-validation to choose a value for the complexity parameter $\alpha$.


rpart::plotcp(hitters_dt_2)
```

Notice from @fig-cp-cv we see that there is little gain in decreasing the complexity parameter, that is, the value we denoted by $\alpha$ beyond a certain point. Clearly, pruning has occurred because we could have grown a tree with up to 20 splits. In fact, let's examine the larger tree:

```{r}
hitters_dt_3 <- rpart::rpart(Salary_log ~ Hits + Years,
                           data=Hitters,
                           method="anova",
                           control=list(cp = 0.0,maxdepth=20,
                                        minsplit=5,xval = 0))

rpart.plot::rpart.plot(hitters_dt_3)
```


### Classification Trees

Classification trees work very similarly to regression trees. The main difference is that RSS cannot be used in making splits since the response is categorical. There are three alternatives used in place of RSS to fit classification trees:

1. Classification error rate: $E = 1 - \max_{k}(\hat{p}_{mk})$,

2. The Gini index: $G = \sum_{k=1}^{K}\hat{p}_{mk}(1 - \hat{p}_{mk})$,

3. Entropy: $D = -\sum_{k=1}^{K}\hat{p}_{mk}\log(\hat{p}_{mk})$,

where $\hat{p}_{mk}$ is the proportion of training observations in the $m$th region that are from the $k$th class. 

Both the Gini index and entropy provide a measure of node purity, a value that measures the extent to which a node in a tree contains observations from a single class. A very small value for the node purity would correspond to a situation where a node contains predominantly observations from a single class. 

Let's examine a small case study for classification trees. Consider the data `Heart` which we must read in from a `.csv` file. These data contain a binary outcome `AHD` for 303 patients who presented with chest pain. An outcome value of `Yes` indicates the presence of heart disease based on an angiographic test, while `No` means no heart disease. There are 13 predictors including `Age`, `Sex`, `Chol` (a cholesterol measurement), and other heart and lung function measurements. Let's import and glimpse the data.

```{r}
#| message: false
#| warning: false


Heart <- read_csv("https://www.statlearning.com/s/Heart.csv")

glimpse(Heart)
```

Let's fit a large classification tree without pruning.

```{r}
heart_dt <- rpart::rpart(AHD ~ .,
                           data=Heart,
                           method="class",
                           control=list(cp = 0.0,xval=0))

rpart.plot::rpart.plot(heart_dt)
```


Now let's include some pruning.

```{r}
heart_dt_2 <- rpart::rpart(AHD ~ .,
                           data=Heart,
                           method="class",
                           control=list(cp = 0.032,xval=10))

rpart.plot::rpart.plot(heart_dt_2)
```

We can see the cross-validation results.

```{r}
rpart::plotcp(heart_dt_2)
```

### Advantages and Disadvantages of Trees

@tbl-tree-pro-con lists some of the advantages and disadvantages associated with the use of decision trees.

| Description |  Advantage or Disadvantage  |
|:------------|:----------------------------|
|Easy to explain| Advantage|
|Mirror decision-making|  Advantage|
|Graphical display| Advantage|
|Easily handle categorical predictors| Advantage|
|Lacking in predictive accuracy|Disadvantage|
|Non-robust|Disadvantage|

: Advantages and disadvantages of decision trees. {#tbl-tree-pro-con}


## Bagging

Decision trees tend to have low bias but high variance. There is a fairly general trick that can be used to reduce variance, that is, by averaging. Here's an illustration of the principle. Suppose we have an i.i.d. sequence $Z_{1}, Z_{2}, \ldots , Z_{n}$ each with variance $\sigma^{2}$. Then, the variance of the mean $\bar{Z} = \frac{1}{n}\sum_{i=1}^{n}Z_{n}$ is $\text{Var}[\bar{Z}] = \frac{\sigma^2}{n}$ and $\frac{\sigma^2}{n} < \sigma^{2}$. Thus, averaging tends to reduce variance. 

In the context of machine learning, we can in principle fit a model on $B$ separate training sets to get $\hat{f}_{1}({\bf x}), \hat{f}_{2}({\bf x}), \ldots , \hat{f}_{B}({\bf x})$ and the make a prediction

$$
\hat{f}_{\text{avg}}({\bf x}) = \frac{1}{B}\sum_{i=1}^{B}\hat{f}_{i}({\bf x})
$$
which we expect to reduce the variance in predictions. However, this is impractical because it requires access to many training sets. Instead, we can bootstrap. We generate $B$ bootstrap resamples, fit models $\hat{f}_{1}^{\ast}({\bf x}), \hat{f}_{2}^{\ast}({\bf x}), \ldots , \hat{f}_{B}^{\ast}({\bf x})$  on each of the bootstrap resamples and then compute

$$
\hat{f}_{\text{bag}}({\bf x}) = \frac{1}{B}\sum_{i=1}^{B}\hat{f}_{i}^{\ast}({\bf x})
$$
This is know as bootstrap aggregation or **bagging**. Bagging is especially useful in the setting of decision trees although it can be applied to pretty much any type of model.  

As described so far, bagging has been presented in the context of regression but it works in the context of classification too. The main difference is that rather than averaging, we take a majority vote to make predictions. 

**Note:** A single bootstrap resample does not typically make use of all the observations in the training set. Those observations in a bootstrap resample that are not included are called the **out-of-bag** (OOB) observations. This is useful because the OOB sets can be used to estimate test error without the need for cross-validation. 


We will explore bagging in applications together in an RStudio project. 


## Random Forests

Random forests are a modification of bagged decision trees that build a large collection of de-correlated trees to further improve predictive performance. They have become a very popular "out-of-the-box" or "off-the-shelf" learning algorithm that enjoys good predictive performance with relatively little hyperparameter tuning.

Random forests are built using the same fundamental principles as decision trees and bagging. Bagging trees introduces a random component into the tree building process by building many trees on bootstrapped copies of the training data. Bagging then aggregates the predictions across all the trees; this aggregation reduces the variance of the overall procedure and results in improved predictive performance. However, simply bagging trees results in tree correlation that limits the effect of variance reduction.

Random forests help to reduce tree correlation by injecting more randomness into the tree-growing process. More specifically, while growing a decision tree during the bagging process, random forests perform split-variable randomization where each time a split is to be performed, the search for the split variable is limited to a random subset of $m_{\text{try}}$ of the original $p$ features. Typical default values are $m_{\text{try}} = \frac{p}{3}$ for regression and $m_{\text{try}} = \sqrt{p}$ for classification but ideally $m_{\text{try}}$ should be considered a hyperparameter whose value needs to be tuned using cross-validation. 


We will explore random forests in applications together in an RStudio project.

## Boosting 

Boosting works in a similar way to bagging, except that the trees are grown sequentially. That is, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set. Boosting is described in @tbl-boosting. 


|    Algorithm: Boosting for Regression Trees       |
|:----------------------------------------------|
| 1. Set $\hat{f}({\bf x}) = 0$ and $r_{i}=y_{i}$ for all $i$ in the training set.       |
| 2. For $b=1,2,\ldots, B$, repeat:  |
|    (a) For a tree $\hat{f}_{b}$ with $d$ splits ($d+1$ terminal nodes) to the training data $(X,r)$.|
|    (b) Update $\hat{f}$ by adding in a shrunken version of the new tree: $\hat{f}({\bf x}) + \lambda \hat{f}_{b} \rightarrow \hat{f}({\bf x})$.|
| (c) Update the residuals $r_{i} - \lambda \hat{f}_{b} \rightarrow r_{i}$.|
| 3. Output the boosted model, $\hat{f}({\bf x}) = \sum_{b=1}^{B}\hat{f}_{b}({\bf x})$. |

: The booting algorithm described for regression trees. {#tbl-boosting}

## Summary

In this lesson, we have covered the various tree-based methods:

1. Basic decision trees.

2. Bootstrap aggregated (bagging) tree.

3. Random forests.

4. Boosting. 

Tree-based methods and random forests and boosting in particular are among the most popular and commonly used supervised machine learning methods in use today. It is important to note that fitting tree-based models involves the tuning of hyperparameters, some of which we have not touched on in the examples given in lecture.  To get a better feel for hyperparameter tuning for tree-base models, it is highly recommended that you work through one or more of the following blog posts by Julia Silge:

* [Hyperparameter tuning and food consumption](https://juliasilge.com/blog/food-hyperparameter-tune/)

* [Tuning random forest hyperparameters with trees data](https://juliasilge.com/blog/sf-trees-random-tuning/)

* [Tune XGBoost with tidymodels and beach volleyball](https://juliasilge.com/blog/xgboost-tune-volleyball/)

* [Bagging with tidymodels and astronaut missions](https://juliasilge.com/blog/astronaut-missions-bagging/)

* [Tune random forests for IKEA prices](https://juliasilge.com/blog/ikea-prices/)

* [Tune and interpret decision trees for wind turbines](https://juliasilge.com/blog/wind-turbine/)

* [Use racing methods to tune xgboost models and predict home runs](https://juliasilge.com/blog/baseball-racing/)

* [Tune xgboost models with early stopping to predict shelter animal status](https://juliasilge.com/blog/shelter-animals/)

* [Tune an xgboost model with early stopping and childcare costs](https://juliasilge.com/blog/childcare-costs/)


## Preparation for the next lesson

To prepare for the next lesson, please read:

- Read chapter 9 from of *An Introduction to Statistical Learning* [@tibshirani2017introduction]. You may also want to read chapter 9 of *Statistical Learning with Math and R* [@suzuki2020statistical].

Watch the following video lectures on support vector machines: 

* [View Supoprt Vector Classifier video on YouTube](https://youtu.be/pjvnCEfAswc?si=5I-xmhB3Y006RP0Y).

```{r}
#| echo: false

vembedr::embed_youtube(id="pjvnCEfAswc?si=5I-xmhB3Y006RP0Y",height=450) %>%
  vembedr::use_align("center")
```

* [View Supoprt Vector Classifiers in R video on YouTube](https://youtu.be/WCRwbrNWrpw?si=rkEz30HgpeBK9VTD).
    
```{r}
#| echo: false

vembedr::embed_youtube(id="WCRwbrNWrpw?si=rkEz30HgpeBK9VTD",height=450) %>%
  vembedr::use_align("center")
```

## References

::: {#refs}
:::


:::{.callout-tip collapse="true"}
## Expand for Session Info
```{r}
#| echo: false


library(sessioninfo)
# save the session info as an object
pkg_sesh <- session_info(pkgs = "attached")

# get the quarto version
quarto_version <- system("quarto --version", intern = TRUE)

# inject the quarto info
pkg_sesh$platform$quarto <- paste(
  system("quarto --version", intern = TRUE), 
  "@", 
  quarto::quarto_path()
  )

# print it out
pkg_sesh
```

:::


[![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png?raw=1){fig-align="left" width=15%}](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode)